{
    "docs": [
        {
            "location": "/", 
            "text": "Chip-seq practical session\n\n\nRunning all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance-clusters (HPC).\n\n\nlog in \ngaia\n\n\ngaia\n is one of the \nHPC of the UNI\n.\n\n\nconnect to the frontend\n\n\nTo connect to it, you need an account and an authorized ssh key. After the setting up of your account, the following should work fi you are using mac or GNU/Lunix:\n\n\nssh gaia-cluster\n\n\n\n\nOtherwise on Windows, right-click on \npageant\n in system tray and load a saved session \ngaia\n. Log as you username.\n\n\nYou should the prompt of the gaia frontend:\n\n\n===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $\n\n\n\n\nNote that your are on the \naccess\n frontend.\n\n\nThe frontend is meant for browsing / transfer your files only and you \nMUST\n connect to a node for any computational work using the utility \noarsub\n described \nhere\n. This program managed the queuing system and dispatch jobs among the resources according to the demands.\n\n\nSoftware are organized into modules that provide you with the binaries but also all the environment required for their running processes.\n\n\nconnect to a node\n\n\nConnecting to a computing node is anyway required to use modules.\n\n\nFor Thursday:\n\n\noarsub -I -t inner=3505160 -l nodes=1,walltime=10\n\n\nFor Friday:\n\n\noarsub -I -t inner=3505161 -l nodes=1,walltime=10\n\n\nWithout entering into the details of \nsubmitting a job\n, here is the explanation for the above command:\n\n\n\n\n-I\n is for interactive, default is passive\n\n\n-t inner=xxx\n, connect to a \ncontainer\n, specific for today because we booked resources for the course\n\n\n-l\n define the resources you need. The less you ask for, the more high up you are in the queue. A \nnode\n is usually composed of 12 \ncores\n, so 12 tasks could be run in parallel. \nwalltime\n define for how long in hours your job will last.\n\n\n\n\nOnce logged in, the prompt changes for:\n\n\n09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326-\n717]\n\n\n\n\nwhere you see the node you are logged to (here \ngaia-66\n), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).\n\n\nmonitoring your the resources used\n\n\nOn a shared cluster, you have to take of \nthree\n things:\n1. memory usage\n2. cores used\n3. disk space\n\n\nmemory\n\n\nEach node has\nOn a interactive session, use the command \nhtop\n to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled.\n\n\nFor passive sessions, you can use \nganglia\n to check out the nodes you are using.\n\n\ncores\n\n\neven if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly.\n\nOn a interactive session, use the command \nhtop\n to see if a process is correctly using close to 100% of a core.\n\n\ndisk space\n\n\nLike on your local machine, you need to check how much data you used.\nUsing a command line, you could use\n\n\ndu -sh ~\n\n\n\n\nto display your disk usage (\ndu\n) for your home folder (\n~\n).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#chip-seq-practical-session", 
            "text": "Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance-clusters (HPC).", 
            "title": "Chip-seq practical session"
        }, 
        {
            "location": "/#log-in-gaia", 
            "text": "gaia  is one of the  HPC of the UNI .  connect to the frontend  To connect to it, you need an account and an authorized ssh key. After the setting up of your account, the following should work fi you are using mac or GNU/Lunix:  ssh gaia-cluster  Otherwise on Windows, right-click on  pageant  in system tray and load a saved session  gaia . Log as you username.  You should the prompt of the gaia frontend:  ===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $  Note that your are on the  access  frontend.  The frontend is meant for browsing / transfer your files only and you  MUST  connect to a node for any computational work using the utility  oarsub  described  here . This program managed the queuing system and dispatch jobs among the resources according to the demands.  Software are organized into modules that provide you with the binaries but also all the environment required for their running processes.  connect to a node  Connecting to a computing node is anyway required to use modules.  For Thursday:  oarsub -I -t inner=3505160 -l nodes=1,walltime=10  For Friday:  oarsub -I -t inner=3505161 -l nodes=1,walltime=10  Without entering into the details of  submitting a job , here is the explanation for the above command:   -I  is for interactive, default is passive  -t inner=xxx , connect to a  container , specific for today because we booked resources for the course  -l  define the resources you need. The less you ask for, the more high up you are in the queue. A  node  is usually composed of 12  cores , so 12 tasks could be run in parallel.  walltime  define for how long in hours your job will last.   Once logged in, the prompt changes for:  09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326- 717]  where you see the node you are logged to (here  gaia-66 ), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).", 
            "title": "log in gaia"
        }, 
        {
            "location": "/#monitoring-your-the-resources-used", 
            "text": "On a shared cluster, you have to take of  three  things:\n1. memory usage\n2. cores used\n3. disk space  memory  Each node has\nOn a interactive session, use the command  htop  to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled.  For passive sessions, you can use  ganglia  to check out the nodes you are using.  cores  even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly. \nOn a interactive session, use the command  htop  to see if a process is correctly using close to 100% of a core.  disk space  Like on your local machine, you need to check how much data you used.\nUsing a command line, you could use  du -sh ~  to display your disk usage ( du ) for your home folder ( ~ ).", 
            "title": "monitoring your the resources used"
        }, 
        {
            "location": "/cli/", 
            "text": "command line\n\n\nThe programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac.\n\n\nYou need to know these commands:\n\n\npwd more less cp mv mkdir ls cd chmod find\n\n\nTwo useful tips:\n\n\n\n\nuse \nTAB\n on your keyboard for command and name completion.\n\n\nthe \nup\n arrow allows to browse your history (also available with \nhistory\n)\n\n\n\n\nExercise 1\n\n\ngo to your home folder\n\n\ncd\n\ncreate a fake file by using\n\n\necho \"hello world\" \n filetest\n\nsee if this file is present\n\nls -l\n\nread it\n\n\nmore fileTest\n\nOf note, \nless\n is an alternative to \nmore\n\nrename it\n\n\nmv fileTest test\n\ncheck\n\n\nls -l\n\ncreate a folder\n\n\nmkdir TEST\n\nOf note, all commands are case-sensitive\n\nll\n\n\nll\n is a classic alias for \nls -l\n\nmove the file in this folder\n\n\nmv test TEST\n\ncheck, and\nsee if present in the folder\n\n\nll TEST\n\ncopy it in the current folder\n\n\ncp TEST/test .\n\n\n.\n is the current folder, \n..\n is the folder one level close to the root \n/\n\nnow we have the same file, with same name, one in the TEST folder, one in the current.\nuse the up arrow, you should see 'cp TEST/test .'\nand change it for\n\n\ncp TEST/test test2\n\nthe first and last field of \nls -l\n should provide  \n\n\ndrwxr-xr-x TEST\n-rw-r--r-- test\n-rw-r--r-- test2\n\n\n\n\ntrash test\n\n\nrm test\n\nif this command doesn't ask for confirmation, let me know we may change this behavior.\n\n\nchmod\n allows to change permissions\n\n\ntry to read the file \ntest\n after\n\n\nchmod 222 test\n\n\nr\n stands for read, \nw\n for write and \nx\n for execution for files and browsing for folders.\n\nthe first pattern is the owner\n\nthe second pattern is for the group\n\nthe third pattern is for everyone else\n\n\ntext editor\n\n\nLet's have a look at a text editor, there is plenty of them, the one I use is \nvim\n, why?\n\nBecause\n\n\n\n\nit's commonly installed on servers\n\n\nextremely powerful\n\n\n\n\nenter the editor\n\n\nvim test2\n\nyou have two modes\n\n\n\n\ncommand\n\n\ninsert\n\n\n\n\nBy default you are in the command mode, let's enter in the editor mode with either \ni\n or \ninsert\n on your keyboard.\nYou should see\n--INSERT--\n at the bottom.\nNow you can edit you file.\nWhen its finished, press \nECHAP\n to return in the command mode. You must enter \n:\n for each command.\nThe useful ones  \n\n\n\n\nw\n save changes to the file\n\n\n:q!\n quit without saving changes\n\n\n:wq\n write and quit\n\n\n\n\nExercise 2\n\n\nfind\nis great but not at all user-friendly.\nTry to find all your files which are bigger than 1 Go.\nthen which are older than 1 year.\nImagine doing this with windows...\n\n\nExercise 3, using a FASTA file\n\n\nGet all sequences in genbank with the keyword trnl\n\n\nhttp://www.ncbi.nlm.nih.gov/sites/entrez?db=nuccore\ncmd=search\nterm=trnl\n\n\nbut download only sequences from one class like \nmammals\n as a FASTA file.\nYou should obtain a 1.1 Mo file. Otherwise, you can use \n/home/users/aginolhac/trnl_mammals.fasta\n\n\n\n\n\n\nhow to obtain the first 500 lines of a file?\n\nsee the command \nhead\n and its manual \nman head\n. You can redirect the output to a file with 'command \n file_first500.fasta' for example.\n\n\n\n\n\n\nhow can you obtain from line 400 to 560?\n\nThink of piping \nhead\n and \ntail\n\n\n\n\n\n\ncount how many lines in this file. See \nwc\n\n\n\n\n\n\ncount how many sequences you have in the FASTA file. look at \ngrep\n\n\n\n\n\n\nyou should have obtained the 500 first lines in a file and the 500 last in a second file. How can you merge these two files? look at \ncat\n\n\n\n\n\n\nExtra questions\n\n\n\n\n\n\nthere is a empty file after each sequence. Try to remove them (my favorite is \nsed\n, but check the \n-v\n option of grep)\n\n\n\n\n\n\nExtract all headers (start with \n) then the \ngi number\n (see \ncut\n). Redirect to a file.\n\n\n\n\n\n\nLook if there some double gi, look at \nsort\n and \nuniq\n.", 
            "title": "command line, basics"
        }, 
        {
            "location": "/cli/#command-line", 
            "text": "The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac.  You need to know these commands:  pwd more less cp mv mkdir ls cd chmod find  Two useful tips:   use  TAB  on your keyboard for command and name completion.  the  up  arrow allows to browse your history (also available with  history )   Exercise 1  go to your home folder  cd \ncreate a fake file by using  echo \"hello world\"   filetest \nsee if this file is present ls -l \nread it  more fileTest \nOf note,  less  is an alternative to  more \nrename it  mv fileTest test \ncheck  ls -l \ncreate a folder  mkdir TEST \nOf note, all commands are case-sensitive ll  ll  is a classic alias for  ls -l \nmove the file in this folder  mv test TEST \ncheck, and\nsee if present in the folder  ll TEST \ncopy it in the current folder  cp TEST/test .  .  is the current folder,  ..  is the folder one level close to the root  / \nnow we have the same file, with same name, one in the TEST folder, one in the current.\nuse the up arrow, you should see 'cp TEST/test .'\nand change it for  cp TEST/test test2 \nthe first and last field of  ls -l  should provide    drwxr-xr-x TEST\n-rw-r--r-- test\n-rw-r--r-- test2  trash test  rm test \nif this command doesn't ask for confirmation, let me know we may change this behavior.  chmod  allows to change permissions  try to read the file  test  after  chmod 222 test  r  stands for read,  w  for write and  x  for execution for files and browsing for folders. \nthe first pattern is the owner \nthe second pattern is for the group \nthe third pattern is for everyone else  text editor  Let's have a look at a text editor, there is plenty of them, the one I use is  vim , why? \nBecause   it's commonly installed on servers  extremely powerful   enter the editor  vim test2 \nyou have two modes   command  insert   By default you are in the command mode, let's enter in the editor mode with either  i  or  insert  on your keyboard.\nYou should see --INSERT--  at the bottom.\nNow you can edit you file.\nWhen its finished, press  ECHAP  to return in the command mode. You must enter  :  for each command.\nThe useful ones     w  save changes to the file  :q!  quit without saving changes  :wq  write and quit   Exercise 2  find is great but not at all user-friendly.\nTry to find all your files which are bigger than 1 Go.\nthen which are older than 1 year.\nImagine doing this with windows...  Exercise 3, using a FASTA file  Get all sequences in genbank with the keyword trnl  http://www.ncbi.nlm.nih.gov/sites/entrez?db=nuccore cmd=search term=trnl  but download only sequences from one class like  mammals  as a FASTA file.\nYou should obtain a 1.1 Mo file. Otherwise, you can use  /home/users/aginolhac/trnl_mammals.fasta    how to obtain the first 500 lines of a file? \nsee the command  head  and its manual  man head . You can redirect the output to a file with 'command   file_first500.fasta' for example.    how can you obtain from line 400 to 560? \nThink of piping  head  and  tail    count how many lines in this file. See  wc    count how many sequences you have in the FASTA file. look at  grep    you should have obtained the 500 first lines in a file and the 500 last in a second file. How can you merge these two files? look at  cat    Extra questions    there is a empty file after each sequence. Try to remove them (my favorite is  sed , but check the  -v  option of grep)    Extract all headers (start with  ) then the  gi number  (see  cut ). Redirect to a file.    Look if there some double gi, look at  sort  and  uniq .", 
            "title": "command line"
        }, 
        {
            "location": "/install/", 
            "text": "load necessary software as modules\n\n\nAdd location of these modules\n\n\nmodule use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/\n\n\n\n\nLoad the modules\n\n\nmodule load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/pysam\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\nmodule load bio/MACS2\n\n\n\n\nTweak for the \npicard-tools\n\n\nTo get all jars available\n\n\nmkdir -p ~/install/jar_root/\ncp /opt/apps/sources/p/picard/picard-tools-1.100.zip ~/install/jar_root/\ncd ~/install/jar_root/\nunzip picard-tools-1.100.zip\nmv  picard-tools-1.100/*.jar .\ncd\n\n\n\n\nyou need to see \nyes\n to overwrite one file.\n\n\nFinal tweak for \nGatk\n\n\ncp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/\n\n\n\n\nprepare your working environment\n\n\ngo to your home directory:\n\ncd\n\ncreate a new folder to work in:\n\nmkdir chip-seq\n\ngo inside:\n\ncd chip-seq\n\ncreate and go in a sub-folder:\n\nmkdir raw ; cd raw\n\nsymbolic link the fastq files:\n\nln -s /work/users/aginolhac/chip-seq/raw/C* .\n\n\ncheck integrity of files\n\n\nJust as a side note, such large files are usually a pain to download. Since they are the very raw files after the sequencer (despite basecalling) checking their integrity is worth doing. Computing the \nmd5sum\n ensure you have the same file as your sequence provider. Then \npaleomix\n will check the FASTQ are correct, \ni. e\n have 4 lines in a correct format.\n\n\nmd5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5", 
            "title": "Setup"
        }, 
        {
            "location": "/install/#load-necessary-software-as-modules", 
            "text": "Add location of these modules  module use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/  Load the modules  module load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/pysam\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\nmodule load bio/MACS2  Tweak for the  picard-tools  To get all jars available  mkdir -p ~/install/jar_root/\ncp /opt/apps/sources/p/picard/picard-tools-1.100.zip ~/install/jar_root/\ncd ~/install/jar_root/\nunzip picard-tools-1.100.zip\nmv  picard-tools-1.100/*.jar .\ncd  you need to see  yes  to overwrite one file.  Final tweak for  Gatk  cp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/", 
            "title": "load necessary software as modules"
        }, 
        {
            "location": "/install/#prepare-your-working-environment", 
            "text": "go to your home directory: cd \ncreate a new folder to work in: mkdir chip-seq \ngo inside: cd chip-seq \ncreate and go in a sub-folder: mkdir raw ; cd raw \nsymbolic link the fastq files: ln -s /work/users/aginolhac/chip-seq/raw/C* .", 
            "title": "prepare your working environment"
        }, 
        {
            "location": "/install/#check-integrity-of-files", 
            "text": "Just as a side note, such large files are usually a pain to download. Since they are the very raw files after the sequencer (despite basecalling) checking their integrity is worth doing. Computing the  md5sum  ensure you have the same file as your sequence provider. Then  paleomix  will check the FASTQ are correct,  i. e  have 4 lines in a correct format.  md5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5", 
            "title": "check integrity of files"
        }, 
        {
            "location": "/fastqc/", 
            "text": "FASTQ Quality controls\n\n\nUsing \nFastQC\n you can perform the necessary controls over fastq files.\n\n\nfastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\n\n\n\nrunning serial\n\n\nA tidy bit of \nbash\n programming to do it for all files\n\n\nfor f in *.gz\n  do fastqc $f\ndone\n\n\n\n\nrunning in parallel\n\n\nIf you have booked \n2\n nodes, otherwise update the \n-j\n option:\n\n\nparallel -j 2 \nfastqc {}\n ::: *.gz\n\n\n\n\nthe \n{}\n instruction will be replaced by all occurrences of the pattern \n*.gz\n, everything that ends by \n.gz\n. \nparallel\n takes care of submitting a new job so the number of parallel remains the same.\n\n\nvisualize the results\n\n\ncollect the \nhtml\n files using either \nrsync\n, \nscp\n for command lines or \nFileZilla\n for  GUI tool.\n\n\nYou should observe some issues that needs to be solve.", 
            "title": "sequence QC"
        }, 
        {
            "location": "/fastqc/#fastq-quality-controls", 
            "text": "Using  FastQC  you can perform the necessary controls over fastq files.  fastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz  running serial  A tidy bit of  bash  programming to do it for all files  for f in *.gz\n  do fastqc $f\ndone  running in parallel  If you have booked  2  nodes, otherwise update the  -j  option:  parallel -j 2  fastqc {}  ::: *.gz  the  {}  instruction will be replaced by all occurrences of the pattern  *.gz , everything that ends by  .gz .  parallel  takes care of submitting a new job so the number of parallel remains the same.  visualize the results  collect the  html  files using either  rsync ,  scp  for command lines or  FileZilla  for  GUI tool.  You should observe some issues that needs to be solve.", 
            "title": "FASTQ Quality controls"
        }, 
        {
            "location": "/mapping/", 
            "text": "paleomix, Next-Generation Sequencing wrapper\n\n\nthis framework is open-source and available at \nGitHub\n and wrap all steps from \nfastq\n to \nbam\n files. Actually, this tool can do much more but the rest is out of scope. Its major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default.\n\n\ncheck if paleomix is available\n\n\npaleomix -h\n\n\n\n\ntest your install\n\n\nfetch the example, reference is the human mitochondrial genome\n\n\nmkdir -p ~/install/paleomix/example\ncp -r /work/users/aginolhac/chip-seq/paleomix/examples/bam_pipeline/00* ~/install/paleomix/example\ncd ~/install/paleomix/example\n\n\n\n\nrun the example, start by a \ndry-run\n, adjust the number of threads accordingly.\n\n\npaleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml\n\n\n\n\nIf all fine, re-rerun the command without the \n--dry-run\n option\n\n\nGenerate a makefile\n\n\nTrimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folder, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes through a \nYAML\n makefile. The syntax is pretty forward.\n\n\nCreate a generic makefile\n\n\ncd ~/chip-seq\npaleomix bam_pipeline mkfile \n mouse.makefile\n\n\n\n\nEdit the makefile\n\n\nusing your favorite editor, edit the \nmouse.makefile\n. For example \nvim mouse.makefile\n or \nkate\n or \nnano\n.\n\n\nOptions\n\n\nFor duplicates, change the default behaviour from \nfilter\n to \nmark\n  \n\n\n  PCRDuplicates: mark\n\n\n\n\nFeatures\n\n\nUnder the \nFeatures\n section, comment with a \n#\n the part that should be run to fit the following\n\n\nFeatures:\n  - Raw BAM        # Generate BAM from the raw libraries (no indel realignment)\n                   #   Location: {Destination}/{Target}.{Genome}.bam\n#    - Realigned BAM  # Generate indel-realigned BAM using the GATK Indel realigner\n                   #   Location: {Destination}/{Target}.{Genome}.realigned.bam\n#    - mapDamage      # Generate mapDamage plot for each (unrealigned) library\n                   #   Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/\n  - Coverage       # Generate coverage information for the raw BAM (wo/ indel realignment)\n                   #   Location: {Destination}/{Target}.{Genome}.coverage\n#    - Depths         # Generate histogram of number of sites with a given read-depth\n                   #   Location: {Destination}/{Target}.{Genome}.depths\n  - Summary        # Generate target summary (uses statistics from raw BAM)\n                   #   Location: {Destination}/{Target}.summary\n\n\n\n\nPrefixes\n\n\nThese are the references to align read to.\n\n\nPrefixes:\n  # Name of the prefix; is used as part of the output filenames\n  mouse_19:\n    # Path to .fasta file containg a set of reference sequences.\n    Path: /work/users/aginolhac/chip-seq/references/chr19.fasta\n\n\n\n\nSamples\n\n\nenter at the end of the makefile, the following lines, according to your login.\nDo use \nspaces\n and not tabs for the indentation.\n\n\nTC1-I-A-D3:\n  TC1-I-A-D3:\n    TC1-I-A-D3:\n      \n14s006680-1-1\n:\n        /home/users/student01/chip-seq/raw/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz\n\nTC1-H3K4-A-D3:\n  TC1-H3K4-A-D3:\n    TC1-H3K4-A-D3:\n      \n14s006647-1-1\n:\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\nTC1-I-ST2-D0:\n  TC1-I-ST2-D0:\n    TC1-I-ST2-D0:\n      \n14s006677-1-1\n:\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz\n\nTC1-H3K4-ST2-D0:\n  TC1-H3K4-ST2-D0:\n    TC1-H3K4-ST2-D0:\n      \n14s006644\n:\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz\n\n\n\n\nPerform the trimming / mapping\n\n\npaleomix bam_pipeline run --bwa-max-threads=2 --max-threads=12 --dry-run mouse.makefile\n\n\n\n\ncheck trimming\n\n\nFirst of all, check using \nfastqc\n that the trimming did remove the adapters that were contaminated the reads.\n\n\nfind . -name \nreads.truncated.bz2\n | parallel \nfastqc {}\n \n\n\n\n\n\nusing the character \n tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using \nhtop\n.\n\n\ncheck especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters?\n\n\nfilter for unique reads\n\n\nUniqueness\n of reads refers to mappability. The less locations a read has in a genome, the higher is mappability will be.\nA common filter is to use \n30\n as a threshold for filtering reads:\n\n\nsamtools view -b -q 30 file.bam \n file.q30.bam\n\n\n\n\nFilter in parallel\n\n\nparallel \nsamtools view -b -q 30 {} \n {.}.q30.bam\n ::: *.bam\n\n\n\n\nSince we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?\n\n\nfilter for duplicates?\n\n\nA duplicate is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high coverages. Duplicates have a unclear definition in a mapped file. Usually, single-end reads that are mapped at the same 5' end are considered as duplicates. External coordinate are used for paired-end reads.\n\nFor regular NGS, filtering for duplicates is mandatory. However, for chip-seq since the reads are by nature clustered location this is not recommended. If duplication is observed at the reads level, such as in \nfastqc\n output, then filtering may be necessary. Marking duplicates allows to keep track of them without losing them.", 
            "title": "Mapping"
        }, 
        {
            "location": "/mapping/#paleomix-next-generation-sequencing-wrapper", 
            "text": "this framework is open-source and available at  GitHub  and wrap all steps from  fastq  to  bam  files. Actually, this tool can do much more but the rest is out of scope. Its major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default.  check if paleomix is available  paleomix -h  test your install  fetch the example, reference is the human mitochondrial genome  mkdir -p ~/install/paleomix/example\ncp -r /work/users/aginolhac/chip-seq/paleomix/examples/bam_pipeline/00* ~/install/paleomix/example\ncd ~/install/paleomix/example  run the example, start by a  dry-run , adjust the number of threads accordingly.  paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml  If all fine, re-rerun the command without the  --dry-run  option  Generate a makefile  Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folder, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes through a  YAML  makefile. The syntax is pretty forward.  Create a generic makefile  cd ~/chip-seq\npaleomix bam_pipeline mkfile   mouse.makefile  Edit the makefile  using your favorite editor, edit the  mouse.makefile . For example  vim mouse.makefile  or  kate  or  nano .  Options  For duplicates, change the default behaviour from  filter  to  mark       PCRDuplicates: mark  Features  Under the  Features  section, comment with a  #  the part that should be run to fit the following  Features:\n  - Raw BAM        # Generate BAM from the raw libraries (no indel realignment)\n                   #   Location: {Destination}/{Target}.{Genome}.bam\n#    - Realigned BAM  # Generate indel-realigned BAM using the GATK Indel realigner\n                   #   Location: {Destination}/{Target}.{Genome}.realigned.bam\n#    - mapDamage      # Generate mapDamage plot for each (unrealigned) library\n                   #   Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/\n  - Coverage       # Generate coverage information for the raw BAM (wo/ indel realignment)\n                   #   Location: {Destination}/{Target}.{Genome}.coverage\n#    - Depths         # Generate histogram of number of sites with a given read-depth\n                   #   Location: {Destination}/{Target}.{Genome}.depths\n  - Summary        # Generate target summary (uses statistics from raw BAM)\n                   #   Location: {Destination}/{Target}.summary  Prefixes  These are the references to align read to.  Prefixes:\n  # Name of the prefix; is used as part of the output filenames\n  mouse_19:\n    # Path to .fasta file containg a set of reference sequences.\n    Path: /work/users/aginolhac/chip-seq/references/chr19.fasta  Samples  enter at the end of the makefile, the following lines, according to your login.\nDo use  spaces  and not tabs for the indentation.  TC1-I-A-D3:\n  TC1-I-A-D3:\n    TC1-I-A-D3:\n       14s006680-1-1 :\n        /home/users/student01/chip-seq/raw/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz\n\nTC1-H3K4-A-D3:\n  TC1-H3K4-A-D3:\n    TC1-H3K4-A-D3:\n       14s006647-1-1 :\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\nTC1-I-ST2-D0:\n  TC1-I-ST2-D0:\n    TC1-I-ST2-D0:\n       14s006677-1-1 :\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz\n\nTC1-H3K4-ST2-D0:\n  TC1-H3K4-ST2-D0:\n    TC1-H3K4-ST2-D0:\n       14s006644 :\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz  Perform the trimming / mapping  paleomix bam_pipeline run --bwa-max-threads=2 --max-threads=12 --dry-run mouse.makefile", 
            "title": "paleomix, Next-Generation Sequencing wrapper"
        }, 
        {
            "location": "/mapping/#check-trimming", 
            "text": "First of all, check using  fastqc  that the trimming did remove the adapters that were contaminated the reads.  find . -name  reads.truncated.bz2  | parallel  fastqc {}     using the character   tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using  htop .  check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters?", 
            "title": "check trimming"
        }, 
        {
            "location": "/mapping/#filter-for-unique-reads", 
            "text": "Uniqueness  of reads refers to mappability. The less locations a read has in a genome, the higher is mappability will be.\nA common filter is to use  30  as a threshold for filtering reads:  samtools view -b -q 30 file.bam   file.q30.bam  Filter in parallel  parallel  samtools view -b -q 30 {}   {.}.q30.bam  ::: *.bam  Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?", 
            "title": "filter for unique reads"
        }, 
        {
            "location": "/mapping/#filter-for-duplicates", 
            "text": "A duplicate is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high coverages. Duplicates have a unclear definition in a mapped file. Usually, single-end reads that are mapped at the same 5' end are considered as duplicates. External coordinate are used for paired-end reads. \nFor regular NGS, filtering for duplicates is mandatory. However, for chip-seq since the reads are by nature clustered location this is not recommended. If duplication is observed at the reads level, such as in  fastqc  output, then filtering may be necessary. Marking duplicates allows to keep track of them without losing them.", 
            "title": "filter for duplicates?"
        }, 
        {
            "location": "/peak/", 
            "text": "Peak calling\n\n\nUsing \nMACS2\n\n\nFor both the day 0 and day 3 of differentiation into adipocytes, two files are available\n\n\n\n\ninput, as control\n\n\nhistone modification H3K4\n\n\n\n\nMACS2\n is going to use both files to normalize the read counts and perform the peak calling.\n\n\nRetrieve the BAM files with all chromosomes\n\n\ncd ~/chip-seq\nmkdir bams\ncd bams\nln -s /work/users/aginolhac/chip-seq/data/*.bam .\n\n\n\n\nPerform peak calling\n\n\nmacs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 \n\nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3\n\n\n\n\ncheck model inferred by MACS2\n\n\nfirst load R as a module and execute R script.\n\n\nmodule load lang/R\nRscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r\nRscript TC1-ST2-H3K4-D0/TC1_ST2-D0_H3K4_model.r\n\n\n\n\nfetch the pdf produced.\n\n\nsort per chromosomes and coordinates\n\n\nfind TC* -name '*.bdg' | parallel \nsort -k1,1 -k2,2n {} \n {.}.sort.bdg\n\n\n\n\n\nconvert to bigwig\n\n\nin order to get smaller files\n\n\nfind TC* -name '*sort.bdg' | parallel -j 1 \n/work/users/aginolhac/chip-seq/bedGraphToBigWig {} /work/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\n\n\n\n\n\nFetch the files and display them in IGV\n\n\nGREAT analysis\n\n\nThe website \nGREAT\n allows to paste bed regions of enriched regions.\n\n\npredict functions of cis-regulatory regions\n\n\nUsing the \nTC1-A-H3K4_peaks.narrowPeak\n file produced by MACS2.\n\n\nThis file has the different fields:\n\n\n\n\nchromosome\n\n\nstart\n\n\nend\n\n\npeak name\n\n\ninteger score for display\n\n\nstrand\n\n\nfold-change\n\n\n-log10pvalue\n\n\n-log10qvalue\n\n\nrelative summit position to peak start\n\n\n\n\nLet's format the file as a 3 fields BED file and focus on more significant peaks filtering on \nq-values\n.\n\n\nawk '$9\n40' TC1-A-H3K4_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' \n TC1-A-H3K4_peaks.bed\n\n\n\n\nthen  \n\n\n\n\nload the BED in \nGREAT\n  \n\n\nfor the relevant genome, \nmm10\n  \n\n\nassociation rule: single nearest gene", 
            "title": "Peak calling"
        }, 
        {
            "location": "/peak/#peak-calling", 
            "text": "Using  MACS2  For both the day 0 and day 3 of differentiation into adipocytes, two files are available   input, as control  histone modification H3K4   MACS2  is going to use both files to normalize the read counts and perform the peak calling.  Retrieve the BAM files with all chromosomes  cd ~/chip-seq\nmkdir bams\ncd bams\nln -s /work/users/aginolhac/chip-seq/data/*.bam .  Perform peak calling  macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0  \nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3  check model inferred by MACS2  first load R as a module and execute R script.  module load lang/R\nRscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r\nRscript TC1-ST2-H3K4-D0/TC1_ST2-D0_H3K4_model.r  fetch the pdf produced.  sort per chromosomes and coordinates  find TC* -name '*.bdg' | parallel  sort -k1,1 -k2,2n {}   {.}.sort.bdg   convert to bigwig  in order to get smaller files  find TC* -name '*sort.bdg' | parallel -j 1  /work/users/aginolhac/chip-seq/bedGraphToBigWig {} /work/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig   Fetch the files and display them in IGV", 
            "title": "Peak calling"
        }, 
        {
            "location": "/peak/#great-analysis", 
            "text": "The website  GREAT  allows to paste bed regions of enriched regions.  predict functions of cis-regulatory regions  Using the  TC1-A-H3K4_peaks.narrowPeak  file produced by MACS2.  This file has the different fields:   chromosome  start  end  peak name  integer score for display  strand  fold-change  -log10pvalue  -log10qvalue  relative summit position to peak start   Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on  q-values .  awk '$9 40' TC1-A-H3K4_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/'   TC1-A-H3K4_peaks.bed  then     load the BED in  GREAT     for the relevant genome,  mm10     association rule: single nearest gene", 
            "title": "GREAT analysis"
        }, 
        {
            "location": "/contact/", 
            "text": "Aur\u00e9lien Ginolhac\n\n\naurelien.ginolhac@uni.lu\n\n\n--\n\nUniversity of Luxembourg\n\nFaculty of Science, Technology and Communication\n\nLife Science Research Unit\n\nCampus Limpertsberg, BRB 1.10b\n\n162a, avenue de la Fa\u00efencerie\n\nL-1511 Luxembourg\n\nPhone: (+352) 46 66 44 6560\n\nFax: (+352) 46 66 44 36560", 
            "title": "Contact"
        }
    ]
}