{
    "docs": [
        {
            "location": "/", 
            "text": "Chip-seq practical session\n\n\nRunning all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance-clusters (HPC).\n\n\nlog in \ngaia\n\n\ngaia\n is one of the \nHPC of the UNI\n.\n\n\nconnect to the frontend\n\n\nTo connect to it, you need an account and an authorized ssh key. After the setting up of your account, the following should work:\n\n\nssh gaia-cluster\n\n\n\n\nYou should the prompt of the gaia frontend:\n\n\n===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $\n\n\n\n\nNote that your are on the \naccess\n frontend.\n\n\nThe frontend is meant for browsing / transfer your files only and you \nMUST\n connect to a node for any computational work using the utility \noarsub\n described \nhere\n. This program managed the queuing system and dispatch jobs among the resources according to the demands.\n\n\nSoftware are organized into modules that provide you with the binaries but also all the environment required for their running processes.\n\n\nconnect to a node\n\n\nConnecting to a computing node is anyway required to use modules.\n\n\nFor Thursday:\n\n\noarsub -I -t inner=3505160 -l nodes=1,walltime=10\n\n\nFor Friday:\n\n\noarsub -I -t inner=3505161 -l nodes=1,walltime=10\n\n\nWithout entering into the details of \nsubmitting a job\n, here is the explanation for the above command:\n\n\n\n\n-I\n is for interactive, default is passive\n\n\n-t inner=xxx\n, connect to a \ncontainer\n, specific for today because we booked resources for the course\n\n\n-l\n define the resources you need. The less you ask for, the more high up you are in the queue. A \nnode\n is composed of 12 \ncores\n, so 12 tasks could be run in parallel. \nwalltime\n define for how long in hours your job will last.\n\n\n\n\nOnce logged in, the prompt changes for:\n\n\n09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326-\n717]\n\n\n\n\nwhere you see the node you are logged to (here \ngaia-66\n), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).\n\n\nmonitoring your the resources used\n\n\nOn a shared cluster, you have to take of \nthree\n things:\n1. memory usage\n2. cores used\n3. disk space\n\n\nmemory\n\n\nEach node has\nOn a interactive session, use the command \nhtop\n to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled.\n\n\nFor passive sessions, you can use \nganglia\n to check out the nodes you are using.\n\n\ncores\n\n\neven if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly.\n\nOn a interactive session, use the command \nhtop\n to see if a process is correctly using close to 100% of a core.\n\n\ndisk space\n\n\nload necessary software as modules\n\n\n\n\nAdd location of these modules\n\n\n\n\nmodule use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/\n\n\n\n\n\n\nLoad the modules\n\n\n\n\nmodule load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/pysam\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\n\n\n\n\n\n\nTweak for the \npicard-tools\n\nTo get all jars available\n\n\n\n\nmkdir -p ~/install/jar_root/\ncp /opt/apps/sources/p/picard/picard-tools-1.100.zip ~/install/jar_root/\ncd ~/install/jar_root/\nunzip picard-tools-1.100.zip\nmv  picard-tools-1.100/*.jar .\ncd\n\n\n\n\nyou need to see \nyes\n to overwrite one file.\n4. Final tweak for \nGatk\n\n\ncp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/\n\n\n\n\nprepare your working environment\n\n\ngo to your home directory:\n\n\ncd\n\ncreate a new folder to work in:\n\n\nmkdir chip-seq\n\ngo inside:\n\n\ncd chip-seq\n\ncreate and go in a sub-folder:\n\nmkdir raw ; cd raw\n\nsymbolic link:\n\nln -s /work/users/aginolhac/chip-seq/raw/C* .\n\n\ncheck integrity of files\n\n\nJust as a side note, such large files are usually a pain to download. Since they are the very raw files after the sequencer (despite basecalling) checking their integrity is worth doing. Computing the \nmd5um\n ensure you have the same file as your sequence provider. Then \npaleomix\n will check the FASTQ is correct, \ni. e\n has 4 lines in a correct format.\n\n\nmd5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5\n\n\nFASTQ Quality controls\n\n\nUsing \nFastQC\n you can perform the necessary controls over fastq files.\n\n\nfastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\n\n\n\nrunning serial\n\n\nA tidy bit of \nbash\n programming to do it for all files\n\n\nfor f in *.gz\n  do fastqc $f\ndone\n\n\n\n\nrunning in parallel\n\n\nIf you have booked \n2\n nodes, otherwise updae the \n-j\n option:\n\n\nparallel -j 2 \nfastqc {}\n ::: *.gz\n\n\n\n\nthe \n{}\n instruction will be replaced by all occurences of the pattern \n*.gz\n, everything that ends by \n.gz\n. \nparallel\n takes care of submitting a new job so the number of parallel remains the same.\n\n\nvisualize the results\n\n\ncollect the \nhtml\n files using either \nrsync\n, \nscp\n for command lines or \nFileZilla\n for  GUI tool.\n\n\nYou should observe some issues that needs to be solve.\n\n\npaleomix\n\n\nthis framework is open-source and available at \nGitHub\n and wrap all steps from \nfastq\n to \nbam\n files. Actually, this tol can do much more but the rest is out of scope.\n\n\ncheck if paleomix is available\n\n\npaleomix -h\n\n\n\n\ntest your install\n\n\nclone Mikkel's repository\n\n\ngit clone https://github.com/MikkelSchubert/paleomix.git ~/install/paleomix\ncd ~/install/paleomix/examples/bam_pipeline\n\n\n\n\nrun the example, start by a \ndry-run\n\n\npaleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml\n\n\n\n\nGenerate a makefile\n\n\nTrimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folder, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes thourgh a \nYAML\n makefile. The syntax is pretty forward.\n\n\nCreate a generic makefile\n\n\ncd ..\npaleomix bam_pipeline mkfile \n mouse.makefile", 
            "title": "Home"
        }, 
        {
            "location": "/#chip-seq-practical-session", 
            "text": "Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance-clusters (HPC).  log in  gaia  gaia  is one of the  HPC of the UNI .  connect to the frontend  To connect to it, you need an account and an authorized ssh key. After the setting up of your account, the following should work:  ssh gaia-cluster  You should the prompt of the gaia frontend:  ===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $  Note that your are on the  access  frontend.  The frontend is meant for browsing / transfer your files only and you  MUST  connect to a node for any computational work using the utility  oarsub  described  here . This program managed the queuing system and dispatch jobs among the resources according to the demands.  Software are organized into modules that provide you with the binaries but also all the environment required for their running processes.  connect to a node  Connecting to a computing node is anyway required to use modules.  For Thursday:  oarsub -I -t inner=3505160 -l nodes=1,walltime=10  For Friday:  oarsub -I -t inner=3505161 -l nodes=1,walltime=10  Without entering into the details of  submitting a job , here is the explanation for the above command:   -I  is for interactive, default is passive  -t inner=xxx , connect to a  container , specific for today because we booked resources for the course  -l  define the resources you need. The less you ask for, the more high up you are in the queue. A  node  is composed of 12  cores , so 12 tasks could be run in parallel.  walltime  define for how long in hours your job will last.   Once logged in, the prompt changes for:  09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326- 717]  where you see the node you are logged to (here  gaia-66 ), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).  monitoring your the resources used  On a shared cluster, you have to take of  three  things:\n1. memory usage\n2. cores used\n3. disk space  memory  Each node has\nOn a interactive session, use the command  htop  to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled.  For passive sessions, you can use  ganglia  to check out the nodes you are using.  cores  even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly. \nOn a interactive session, use the command  htop  to see if a process is correctly using close to 100% of a core.  disk space  load necessary software as modules   Add location of these modules   module use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/   Load the modules   module load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/pysam\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage   Tweak for the  picard-tools \nTo get all jars available   mkdir -p ~/install/jar_root/\ncp /opt/apps/sources/p/picard/picard-tools-1.100.zip ~/install/jar_root/\ncd ~/install/jar_root/\nunzip picard-tools-1.100.zip\nmv  picard-tools-1.100/*.jar .\ncd  you need to see  yes  to overwrite one file.\n4. Final tweak for  Gatk  cp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/  prepare your working environment  go to your home directory:  cd \ncreate a new folder to work in:  mkdir chip-seq \ngo inside:  cd chip-seq \ncreate and go in a sub-folder: mkdir raw ; cd raw \nsymbolic link: ln -s /work/users/aginolhac/chip-seq/raw/C* .  check integrity of files  Just as a side note, such large files are usually a pain to download. Since they are the very raw files after the sequencer (despite basecalling) checking their integrity is worth doing. Computing the  md5um  ensure you have the same file as your sequence provider. Then  paleomix  will check the FASTQ is correct,  i. e  has 4 lines in a correct format.  md5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5  FASTQ Quality controls  Using  FastQC  you can perform the necessary controls over fastq files.  fastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz  running serial  A tidy bit of  bash  programming to do it for all files  for f in *.gz\n  do fastqc $f\ndone  running in parallel  If you have booked  2  nodes, otherwise updae the  -j  option:  parallel -j 2  fastqc {}  ::: *.gz  the  {}  instruction will be replaced by all occurences of the pattern  *.gz , everything that ends by  .gz .  parallel  takes care of submitting a new job so the number of parallel remains the same.  visualize the results  collect the  html  files using either  rsync ,  scp  for command lines or  FileZilla  for  GUI tool.  You should observe some issues that needs to be solve.  paleomix  this framework is open-source and available at  GitHub  and wrap all steps from  fastq  to  bam  files. Actually, this tol can do much more but the rest is out of scope.  check if paleomix is available  paleomix -h  test your install  clone Mikkel's repository  git clone https://github.com/MikkelSchubert/paleomix.git ~/install/paleomix\ncd ~/install/paleomix/examples/bam_pipeline  run the example, start by a  dry-run  paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml  Generate a makefile  Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folder, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes thourgh a  YAML  makefile. The syntax is pretty forward.  Create a generic makefile  cd ..\npaleomix bam_pipeline mkfile   mouse.makefile", 
            "title": "Chip-seq practical session"
        }
    ]
}