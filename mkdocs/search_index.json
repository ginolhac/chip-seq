{
    "docs": [
        {
            "location": "/", 
            "text": "ChIP-seq practical session\n\n\nRunning all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance-clusters (HPC).\n\n\nlog in \ngaia\n\n\ngaia\n is one of the \nHPC of the UNI\n.\n\n\nconnect to the frontend\n\n\nTo connect to it, you need an account and an authorized ssh key. \nAfter the setting up of your account, the following should work if you are using mac or GNU/Linux:\n\n\nssh gaia-cluster\n\n\n\n\nOtherwise on Windows, right-click on \npageant\n in the system tray and load a saved session \ngaia\n. In the terminal, log as you username, such as \nstudent01\n.\n\n\nYou should the prompt of the gaia frontend:\n\n\n===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $\n\n\n\n\nNote that your are on the \naccess\n frontend.\n\n\nThe frontend is meant for browsing / transfer your files only and you \nMUST\n connect to a node for any computational work \nusing the utility \noarsub\n described \nhere\n. This program managed the queuing system and dispatch jobs among the resources according to the demands.\n\n\nSoftware are organized into modules that provide you with the binaries but also all the environment required for their running processes.\n\n\nconnect to a node\n\n\nConnecting to a computing node is anyway required to use modules.\n\n\nFor Thursday:\n\n\noarsub -I -t inner=3505160 -l nodes=1,walltime=10\n\n\nFor Friday:\n\n\noarsub -I -t inner=3505161 -l nodes=1,walltime=10\n\n\nWithout entering into the details of \nsubmitting a job\n,\nhere is the explanation for the above command:\n\n\n\n\n-I\n is for interactive, default is passive\n\n\n-t inner=xxx\n, connect to a \ncontainer\n, specific for today because we booked resources for the course\n\n\n-l\n define the resources you need. The less you ask for, the more high up you are in the queue. A \nnode\n is usually composed of 12 \ncores\n, \nso 12 tasks could be run in parallel. \nwalltime\n define for how long in hours your job will last.\n\n\n\n\nOnce logged in, the prompt changes for:\n\n\n09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326-\n717]\n\n\n\n\nwhere you see the node you are logged to (here \ngaia-66\n), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).\n\n\nmonitoring your the resources used\n\n\nOn a shared cluster, you have to take of \nthree\n things:\n\n\n\n\nmemory usage\n\n\ncores used\n\n\ndisk space\n\n\n\n\nmemory\n\n\nEach node has\nOn a interactive session, use the command \nhtop\n to see if the memory is not full. If the system is swapping (using hard drives for memory storage)\nit becomes super slow and eventually stalled.\n\n\nFor passive sessions, you can use \nganglia\n to check out the nodes you are using.\n\n\ncores\n\n\neven if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources.\nIn this example, each task will use 1/10th of a core, then runs very slowly.\n\nOn a interactive session, use the command \nhtop\n to see if a process is correctly using close to 100% of a core.\n\n\ndisk space\n\n\nLike on your local machine, you need to check how much data you used.\nUsing a command line, you could use\n\n\ndu -sh ~\n\n\n\n\nto display your disk usage (\ndu\n) for your home folder (\n~\n).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#chip-seq-practical-session", 
            "text": "Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance-clusters (HPC).", 
            "title": "ChIP-seq practical session"
        }, 
        {
            "location": "/#log-in-gaia", 
            "text": "gaia  is one of the  HPC of the UNI .  connect to the frontend  To connect to it, you need an account and an authorized ssh key. \nAfter the setting up of your account, the following should work if you are using mac or GNU/Linux:  ssh gaia-cluster  Otherwise on Windows, right-click on  pageant  in the system tray and load a saved session  gaia . In the terminal, log as you username, such as  student01 .  You should the prompt of the gaia frontend:  ===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $  Note that your are on the  access  frontend.  The frontend is meant for browsing / transfer your files only and you  MUST  connect to a node for any computational work \nusing the utility  oarsub  described  here . This program managed the queuing system and dispatch jobs among the resources according to the demands.  Software are organized into modules that provide you with the binaries but also all the environment required for their running processes.  connect to a node  Connecting to a computing node is anyway required to use modules.  For Thursday:  oarsub -I -t inner=3505160 -l nodes=1,walltime=10  For Friday:  oarsub -I -t inner=3505161 -l nodes=1,walltime=10  Without entering into the details of  submitting a job ,\nhere is the explanation for the above command:   -I  is for interactive, default is passive  -t inner=xxx , connect to a  container , specific for today because we booked resources for the course  -l  define the resources you need. The less you ask for, the more high up you are in the queue. A  node  is usually composed of 12  cores , \nso 12 tasks could be run in parallel.  walltime  define for how long in hours your job will last.   Once logged in, the prompt changes for:  09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326- 717]  where you see the node you are logged to (here  gaia-66 ), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).", 
            "title": "log in gaia"
        }, 
        {
            "location": "/#monitoring-your-the-resources-used", 
            "text": "On a shared cluster, you have to take of  three  things:   memory usage  cores used  disk space   memory  Each node has\nOn a interactive session, use the command  htop  to see if the memory is not full. If the system is swapping (using hard drives for memory storage)\nit becomes super slow and eventually stalled.  For passive sessions, you can use  ganglia  to check out the nodes you are using.  cores  even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources.\nIn this example, each task will use 1/10th of a core, then runs very slowly. \nOn a interactive session, use the command  htop  to see if a process is correctly using close to 100% of a core.  disk space  Like on your local machine, you need to check how much data you used.\nUsing a command line, you could use  du -sh ~  to display your disk usage ( du ) for your home folder ( ~ ).", 
            "title": "monitoring your the resources used"
        }, 
        {
            "location": "/cli/", 
            "text": "command line\n\n\nThe programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac.\n\n\nYou need to know these commands: \n\n\npwd\nmore\nless\ncp\nmv\nmkdir\nls\ncd\nchmod\nrm\nfind\n\n\n\n\nTwo useful tips:\n\n\n\n\nuse \nTAB\n on your keyboard for command and name completion.\n\n\nthe \nup\n arrow allows to browse your history (also available with \nhistory\n)\n\n\n\n\nExercise 1\n\n\ngo to your home folder\n\n\ncd\n\ncreate a fake file by using\n\n\necho \"hello world\" \n filetest\n\nsee if this file is present\n\nls -l\n\nread it\n\n\nmore fileTest\n\nOf note, \nless\n is an alternative to \nmore\n\nrename it\n\n\nmv fileTest test\n\ncheck\n\n\nls -l\n\ncreate a folder\n\n\nmkdir TEST\n\nOf note, all commands are case-sensitive\n\nll\n\n\nll\n is a classic alias for \nls -l\n\nmove the file in this folder\n\n\nmv test TEST\n\ncheck, and\nsee if present in the folder\n\n\nll TEST\n\ncopy it in the current folder\n\n\ncp TEST/test .\n\n\n.\n is the current folder, \n..\n is the folder one level close to the root \n/\n\nnow we have the same file, with same name, one in the TEST folder, one in the current.\nuse the up arrow, you should see 'cp TEST/test .'\nand change it for\n\n\ncp TEST/test test2\n\nthe first and last field of \nls -l\n should provide\n\n\ndrwxr-xr-x TEST\n-rw-r--r-- test\n-rw-r--r-- test2\n\n\n\n\ntrash test\n\n\nrm test\n\nif this command doesn't ask for confirmation, let me know we may change this behavior.\n\n\nchmod\n allows to change permissions\n\n\ntry to read the file \ntest\n after\n\n\nchmod 222 test\n\n\nr\n stands for read, \nw\n for write and \nx\n for execution for files and browsing for folders.\n\nthe first pattern is the owner\n\nthe second pattern is for the group\n\nthe third pattern is for everyone else\n\n\ntext editor\n\n\nLet's have a look at a text editor, there is plenty of them, the one I use is \nvim\n, why?\n\nBecause\n\n\n\n\nit's commonly installed on servers\n\n\nextremely powerful\n\n\n\n\nenter the editor\n\n\nvim test2\n\nyou have two modes\n\n\n\n\ncommand\n\n\ninsert\n\n\n\n\nBy default you are in the command mode, let's enter in the editor mode with either \ni\n or \ninsert\n on your keyboard.\nYou should see\n--INSERT--\n at the bottom.\nNow you can edit you file.\nWhen its finished, press \nECHAP\n to return in the command mode. You must enter \n:\n for each command.\nThe useful ones  \n\n\n\n\nw\n save changes to the file\n\n\n:q!\n quit without saving changes\n\n\n:wq\n write and quit\n\n\n\n\nExercise 2\n\n\nfind\nis great but not at all user-friendly.\nTry to find all your files which are bigger than 1 Go.\nthen which are older than 1 year.\nImagine doing this with windows...\n\n\nExercise 3, using a FASTA file\n\n\nGet all sequences in genbank with the keyword trnl\n\n\nhttp://www.ncbi.nlm.nih.gov/sites/entrez?db=nuccore\ncmd=search\nterm=trnl\n\n\nbut download only sequences from one class like \nmammals\n as a FASTA file.\nYou should obtain a 1.1 Mo file. Otherwise, you can use \n/home/users/aginolhac/trnl_mammals.fasta\n\n\n\n\n\n\nhow to obtain the first 500 lines of a file?\n\nsee the command \nhead\n and its manual \nman head\n. You can redirect the output to a file with 'command \n file_first500.fasta' for example.\n\n\n\n\n\n\nhow can you obtain from line 400 to 560?\n\nThink of piping \nhead\n and \ntail\n\n\n\n\n\n\ncount how many lines in this file. See \nwc\n\n\n\n\n\n\ncount how many sequences you have in the FASTA file. look at \ngrep\n\n\n\n\n\n\nyou should have obtained the 500 first lines in a file and the 500 last in a second file. How can you merge these two files? look at \ncat\n\n\n\n\n\n\nExtra questions\n\n\n\n\n\n\nthere is a empty file after each sequence. Try to remove them (my favorite is \nsed\n, but check the \n-v\n option of grep)\n\n\n\n\n\n\nExtract all headers (start with \n) then the \ngi number\n (see \ncut\n). Redirect to a file.\n\n\n\n\n\n\nLook if there some double gi, look at \nsort\n and \nuniq\n.", 
            "title": "command line, basics"
        }, 
        {
            "location": "/cli/#command-line", 
            "text": "The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac.  You need to know these commands:   pwd\nmore\nless\ncp\nmv\nmkdir\nls\ncd\nchmod\nrm\nfind  Two useful tips:   use  TAB  on your keyboard for command and name completion.  the  up  arrow allows to browse your history (also available with  history )   Exercise 1  go to your home folder  cd \ncreate a fake file by using  echo \"hello world\"   filetest \nsee if this file is present ls -l \nread it  more fileTest \nOf note,  less  is an alternative to  more \nrename it  mv fileTest test \ncheck  ls -l \ncreate a folder  mkdir TEST \nOf note, all commands are case-sensitive ll  ll  is a classic alias for  ls -l \nmove the file in this folder  mv test TEST \ncheck, and\nsee if present in the folder  ll TEST \ncopy it in the current folder  cp TEST/test .  .  is the current folder,  ..  is the folder one level close to the root  / \nnow we have the same file, with same name, one in the TEST folder, one in the current.\nuse the up arrow, you should see 'cp TEST/test .'\nand change it for  cp TEST/test test2 \nthe first and last field of  ls -l  should provide  drwxr-xr-x TEST\n-rw-r--r-- test\n-rw-r--r-- test2  trash test  rm test \nif this command doesn't ask for confirmation, let me know we may change this behavior.  chmod  allows to change permissions  try to read the file  test  after  chmod 222 test  r  stands for read,  w  for write and  x  for execution for files and browsing for folders. \nthe first pattern is the owner \nthe second pattern is for the group \nthe third pattern is for everyone else  text editor  Let's have a look at a text editor, there is plenty of them, the one I use is  vim , why? \nBecause   it's commonly installed on servers  extremely powerful   enter the editor  vim test2 \nyou have two modes   command  insert   By default you are in the command mode, let's enter in the editor mode with either  i  or  insert  on your keyboard.\nYou should see --INSERT--  at the bottom.\nNow you can edit you file.\nWhen its finished, press  ECHAP  to return in the command mode. You must enter  :  for each command.\nThe useful ones     w  save changes to the file  :q!  quit without saving changes  :wq  write and quit   Exercise 2  find is great but not at all user-friendly.\nTry to find all your files which are bigger than 1 Go.\nthen which are older than 1 year.\nImagine doing this with windows...  Exercise 3, using a FASTA file  Get all sequences in genbank with the keyword trnl  http://www.ncbi.nlm.nih.gov/sites/entrez?db=nuccore cmd=search term=trnl  but download only sequences from one class like  mammals  as a FASTA file.\nYou should obtain a 1.1 Mo file. Otherwise, you can use  /home/users/aginolhac/trnl_mammals.fasta    how to obtain the first 500 lines of a file? \nsee the command  head  and its manual  man head . You can redirect the output to a file with 'command   file_first500.fasta' for example.    how can you obtain from line 400 to 560? \nThink of piping  head  and  tail    count how many lines in this file. See  wc    count how many sequences you have in the FASTA file. look at  grep    you should have obtained the 500 first lines in a file and the 500 last in a second file. How can you merge these two files? look at  cat    Extra questions    there is a empty file after each sequence. Try to remove them (my favorite is  sed , but check the  -v  option of grep)    Extract all headers (start with  ) then the  gi number  (see  cut ). Redirect to a file.    Look if there some double gi, look at  sort  and  uniq .", 
            "title": "command line"
        }, 
        {
            "location": "/install/", 
            "text": "load necessary software as modules\n\n\nAdd location of these modules\n\n\nmodule use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/\n\n\n\n\nLoad the modules\n\n\nmodule load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\nmodule load bio/MACS2\nmodule load lang/Java\nmodule load lang/R/3.3.0-ictce-7.3.5-bare\n\n\n\n\nTweak for the \npicard\n\n\nmkdir -p ~/install/jar_root/\ncp /home/users/aginolhac/install/jar_root/picard.jar ~/install/jar_root/\n\n\n\n\nFinal tweak for \nGatk\n\n\ncp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/\n\n\n\n\nprepare your working environment\n\n\ngo to your home directory:\n\ncd\n\ncreate a new folder to work in:\n\nmkdir chip-seq\n\ngo inside:\n\ncd chip-seq\n\ncreate and go in a sub-folder:\n\nmkdir raw\n\ngo inside:\n\ncd raw\n\nsymbolic link the fastq files:\n\nln -s /work/users/aginolhac/chip-seq/doctoral_school/raw/C* .\n\n\ncheck integrity of files\n\n\nJust as a side note, such large files are usually a pain to download. Since they are the very raw files\nafter the sequencer (despite basecalling) checking their integrity is worth doing.\nComputing the \nmd5sum\n ensure you have the same file as your sequence provider.\nThen \npaleomix\n will check the FASTQ are correct, \ni. e\n have 4 lines in a correct format.\n\n\nmd5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5", 
            "title": "Setup"
        }, 
        {
            "location": "/install/#load-necessary-software-as-modules", 
            "text": "Add location of these modules  module use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/  Load the modules  module load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\nmodule load bio/MACS2\nmodule load lang/Java\nmodule load lang/R/3.3.0-ictce-7.3.5-bare  Tweak for the  picard  mkdir -p ~/install/jar_root/\ncp /home/users/aginolhac/install/jar_root/picard.jar ~/install/jar_root/  Final tweak for  Gatk  cp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/", 
            "title": "load necessary software as modules"
        }, 
        {
            "location": "/install/#prepare-your-working-environment", 
            "text": "go to your home directory: cd \ncreate a new folder to work in: mkdir chip-seq \ngo inside: cd chip-seq \ncreate and go in a sub-folder: mkdir raw \ngo inside: cd raw \nsymbolic link the fastq files: ln -s /work/users/aginolhac/chip-seq/doctoral_school/raw/C* .", 
            "title": "prepare your working environment"
        }, 
        {
            "location": "/install/#check-integrity-of-files", 
            "text": "Just as a side note, such large files are usually a pain to download. Since they are the very raw files\nafter the sequencer (despite basecalling) checking their integrity is worth doing.\nComputing the  md5sum  ensure you have the same file as your sequence provider.\nThen  paleomix  will check the FASTQ are correct,  i. e  have 4 lines in a correct format.  md5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5", 
            "title": "check integrity of files"
        }, 
        {
            "location": "/fastqc/", 
            "text": "FASTQ Quality controls\n\n\nUsing \nFastQC\n you can perform the necessary controls over fastq files.\n\n\nfastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\n\n\n\nrunning serial\n\n\nA tidy bit of \nbash\n programming to do it for all files\n\n\nfor f in *.gz\n  do fastqc $f\ndone\n\n\n\n\nrunning in parallel\n\n\nIf you have booked \n2\n nodes, otherwise update the \n-j\n option:\n\n\nparallel -j 2 \nfastqc {}\n ::: *.gz\n\n\n\n\nthe \n{}\n instruction will be replaced by all occurrences of the pattern \n*.gz\n, everything that ends by \n.gz\n. \nparallel\n takes care of submitting a new job so the number of parallel remains the same.\n\n\nvisualize the results\n\n\ncollect the \nhtml\n files using either \nrsync\n, \nscp\n for command lines or \nFileZilla\n for  GUI tool.\n\n\nYou should observe some issues that needs to be solve.", 
            "title": "sequence QC"
        }, 
        {
            "location": "/fastqc/#fastq-quality-controls", 
            "text": "Using  FastQC  you can perform the necessary controls over fastq files.  fastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz  running serial  A tidy bit of  bash  programming to do it for all files  for f in *.gz\n  do fastqc $f\ndone  running in parallel  If you have booked  2  nodes, otherwise update the  -j  option:  parallel -j 2  fastqc {}  ::: *.gz  the  {}  instruction will be replaced by all occurrences of the pattern  *.gz , everything that ends by  .gz .  parallel  takes care of submitting a new job so the number of parallel remains the same.  visualize the results  collect the  html  files using either  rsync ,  scp  for command lines or  FileZilla  for  GUI tool.  You should observe some issues that needs to be solve.", 
            "title": "FASTQ Quality controls"
        }, 
        {
            "location": "/mapping/", 
            "text": "paleomix, Next-Generation Sequencing wrapper\n\n\nthis framework is open-source and available at \nGitHub\n and \nwrap all steps from \nfastq\n to \nbam\n files. \nActually, this tool can do much more but the rest is out of scope. \nIts major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default.\nFull documentation available \nhere\n\n\ncheck if paleomix is available\n\n\npaleomix\n\n\n\n\ntest your install\n\n\nfetch the example, reference is the human mitochondrial genome\n\n\nmkdir -p ~/install/paleomix/example\ncp -r /work/users/aginolhac/chip-seq/paleomix/examples/bam_pipeline/00* ~/install/paleomix/example\ncd ~/install/paleomix/example\n\n\n\n\nrun the example, start by a \ndry-run\n, adjust the number of threads accordingly.\n\n\npaleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml\n\n\n\n\nIf all fine, re-rerun the command without the \n--dry-run\n option\n\n\nOf note, calling \nmapDamage\n was disabled to limit the computation time (~ 35 min when included).\nAnyway, this tool is not used for ChIP-seq analysis.\n\n\nGenerate a makefile\n\n\nTrimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. \nPaleomix works in temporary folder, check the data produced and then copy back files that are complete. \nPlus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. \nThis goes through a \nYAML\n makefile. The syntax is pretty straight-forward.\nWhat matters is, that you use \nSPACES\n and not TABS.\n\n\nCreate a generic makefile (extension, \nyml\n or \nyaml\n to get syntax highlights)\n\n\ncd ~/chip-seq\npaleomix bam_pipeline mkfile \n mouse.yaml\n\n\n\n\nEdit the makefile\n\n\nusing your favorite text editor, edit the \nmouse.makefile\n. For example \nvim mouse.makefile\n or \nkate\n or \nnano\n.\n\n\nOptions\n\n\nFor duplicates, change the default behaviour from \nfilter\n to \nmark\n  \n\n\n  PCRDuplicates: mark\n\n\n\n\nFeatures\n\n\nUnder the \nFeatures\n section, update the feature that need to be performed.\nChange \nyes/no\n to match the following:\n\n\n  Features:\n    RawBAM: yes         # Generate BAM from the raw libraries (no indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.bam\n    RealignedBAM: no    # Generate indel-realigned BAM using the GATK Indel realigner\n                        #   Location: {Destination}/{Target}.{Genome}.realigned.bam\n    mapDamage: no       # Generate mapDamage plot for each (unrealigned) library\n                        #   Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/\n    Coverage: yes       # Generate coverage information for the raw BAM (wo/ indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.coverage\n    Depths: no          # Generate histogram of number of sites with a given read-depth\n                        #   Location: {Destination}/{Target}.{Genome}.depths\n    Summary: yes        # Generate summary table for each target\n                        #   Location: {Destination}/{Target}.summary\n    DuplicateHist: no   # Generate histogram of PCR duplicates, for use with PreSeq\n                        #   Location: {Destination}/{Target}.{Genome}.duphist/{Library}/\n\n\n\n\nIn detail, the \nRealignedBAM\n are important for calling variants, we only need to \nRawBAM\n.\nMoreover, the \nDepths\n also help to define which upper limit could be used for variant calling.\nThis is not in the scope of ChIP-seq analysis. Same for mapDamage, only relevant for ancient DNA.\n\n\nPrefixes\n\n\nThese are the references to align read to. You could notice that we are going to use only one chromosome\nto save computational time.\n\n\nPrefixes:\n  mouse_19:\n    Path: /work/users/aginolhac/chip-seq/doctoral_school/references/chr19.fasta\n\n\n\n\nSamples\n\n\nenter at the end of the makefile, the following lines, according to your login.\nAgain, do use \nspaces\n and not tabs for the indentation. For those who are lazy and use copy/paste in \nvim\n\n use the trick to \n:set paste\n to avoid extra spaces, comment hashes etc to be automatically add.\n\n\nBe careful to \nreplace\n \nstudent01\n by the relevant username\n\n\nTC1-I-A-D3:\n  TC1-I-A-D3:\n    TC1-I-A-D3:\n      \n14s006680-1-1\n:\n        /home/users/student01/chip-seq/raw/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz\n\nTC1-H3K4-A-D3:\n  TC1-H3K4-A-D3:\n    TC1-H3K4-A-D3:\n      \n14s006647-1-1\n:\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\nTC1-I-ST2-D0:\n  TC1-I-ST2-D0:\n    TC1-I-ST2-D0:\n      \n14s006677-1-1\n:\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz\n\nTC1-H3K4-ST2-D0:\n  TC1-H3K4-ST2-D0:\n    TC1-H3K4-ST2-D0:\n      \n14s006644\n:\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz\n\n\n\n\nPerform the trimming / mapping\n\n\nFirst use the option \n--dry-run\n to spot mistakes.\n\n\nPlease \nadapt\n the \n--max-threads\n option to the #cpus actually booked\n\n\npaleomix bam_pipeline run --bwa-max-threads=2 --max-threads=12 --dry-run mouse.yaml\n\n\n\n\nwhen all green lights are on, remove the \ndry-run\n and perform the mapping.\n\n\ncheck trimming\n\n\nFirst of all, check using \nfastqc\n that the trimming did remove the adapters that were contaminated the reads.\n\n\nAgain, with \nparallel\n specify the \nmax\n number of jobs with the option \n-j\n to fit the #cpus booked\n\n\nfind . -name \nreads.truncated.bz2\n | parallel -j 12 \nfastqc {}\n \n\n\n\n\n\nusing the character \n tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using \nhtop\n.\n\n\ncheck especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters?\n\n\nfilter for unique reads\n\n\nUniqueness\n of reads refers to mappability. The less locations a read has in a genome, the higher is mappability will be.\nA common filter is to use \n30\n as a threshold for filtering reads:\n\n\nsamtools view -b -q 30 file.bam \n file.q30.bam\n\n\n\n\nFilter in parallel\n\n\nparallel \nsamtools view -b -q 30 {} \n {.}.q30.bam\n ::: *.bam\n\n\n\n\nSince we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?\n\n\nfilter for duplicates?\n\n\nDuplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage.\nDuplicates have a unclear definition in a mapped file. Usually, single-end reads that are mapped\nat the same 5' end are considered as duplicates. External coordinate are used for paired-end reads.\n\nFor regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are,\nby nature, clustered at one location this is not recommended. If duplication is observed at the reads level, \nsuch as in \nfastqc\n output, then filtering may be necessary. Marking duplicates allows to keep track of them without losing them.", 
            "title": "Mapping"
        }, 
        {
            "location": "/mapping/#paleomix-next-generation-sequencing-wrapper", 
            "text": "this framework is open-source and available at  GitHub  and \nwrap all steps from  fastq  to  bam  files. \nActually, this tool can do much more but the rest is out of scope. \nIts major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default.\nFull documentation available  here  check if paleomix is available  paleomix  test your install  fetch the example, reference is the human mitochondrial genome  mkdir -p ~/install/paleomix/example\ncp -r /work/users/aginolhac/chip-seq/paleomix/examples/bam_pipeline/00* ~/install/paleomix/example\ncd ~/install/paleomix/example  run the example, start by a  dry-run , adjust the number of threads accordingly.  paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml  If all fine, re-rerun the command without the  --dry-run  option  Of note, calling  mapDamage  was disabled to limit the computation time (~ 35 min when included).\nAnyway, this tool is not used for ChIP-seq analysis.  Generate a makefile  Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. \nPaleomix works in temporary folder, check the data produced and then copy back files that are complete. \nPlus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. \nThis goes through a  YAML  makefile. The syntax is pretty straight-forward.\nWhat matters is, that you use  SPACES  and not TABS.  Create a generic makefile (extension,  yml  or  yaml  to get syntax highlights)  cd ~/chip-seq\npaleomix bam_pipeline mkfile   mouse.yaml  Edit the makefile  using your favorite text editor, edit the  mouse.makefile . For example  vim mouse.makefile  or  kate  or  nano .  Options  For duplicates, change the default behaviour from  filter  to  mark       PCRDuplicates: mark  Features  Under the  Features  section, update the feature that need to be performed.\nChange  yes/no  to match the following:    Features:\n    RawBAM: yes         # Generate BAM from the raw libraries (no indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.bam\n    RealignedBAM: no    # Generate indel-realigned BAM using the GATK Indel realigner\n                        #   Location: {Destination}/{Target}.{Genome}.realigned.bam\n    mapDamage: no       # Generate mapDamage plot for each (unrealigned) library\n                        #   Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/\n    Coverage: yes       # Generate coverage information for the raw BAM (wo/ indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.coverage\n    Depths: no          # Generate histogram of number of sites with a given read-depth\n                        #   Location: {Destination}/{Target}.{Genome}.depths\n    Summary: yes        # Generate summary table for each target\n                        #   Location: {Destination}/{Target}.summary\n    DuplicateHist: no   # Generate histogram of PCR duplicates, for use with PreSeq\n                        #   Location: {Destination}/{Target}.{Genome}.duphist/{Library}/  In detail, the  RealignedBAM  are important for calling variants, we only need to  RawBAM .\nMoreover, the  Depths  also help to define which upper limit could be used for variant calling.\nThis is not in the scope of ChIP-seq analysis. Same for mapDamage, only relevant for ancient DNA.  Prefixes  These are the references to align read to. You could notice that we are going to use only one chromosome\nto save computational time.  Prefixes:\n  mouse_19:\n    Path: /work/users/aginolhac/chip-seq/doctoral_school/references/chr19.fasta  Samples  enter at the end of the makefile, the following lines, according to your login.\nAgain, do use  spaces  and not tabs for the indentation. For those who are lazy and use copy/paste in  vim \n use the trick to  :set paste  to avoid extra spaces, comment hashes etc to be automatically add.  Be careful to  replace   student01  by the relevant username  TC1-I-A-D3:\n  TC1-I-A-D3:\n    TC1-I-A-D3:\n       14s006680-1-1 :\n        /home/users/student01/chip-seq/raw/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz\n\nTC1-H3K4-A-D3:\n  TC1-H3K4-A-D3:\n    TC1-H3K4-A-D3:\n       14s006647-1-1 :\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\nTC1-I-ST2-D0:\n  TC1-I-ST2-D0:\n    TC1-I-ST2-D0:\n       14s006677-1-1 :\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz\n\nTC1-H3K4-ST2-D0:\n  TC1-H3K4-ST2-D0:\n    TC1-H3K4-ST2-D0:\n       14s006644 :\n        /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz  Perform the trimming / mapping  First use the option  --dry-run  to spot mistakes.  Please  adapt  the  --max-threads  option to the #cpus actually booked  paleomix bam_pipeline run --bwa-max-threads=2 --max-threads=12 --dry-run mouse.yaml  when all green lights are on, remove the  dry-run  and perform the mapping.", 
            "title": "paleomix, Next-Generation Sequencing wrapper"
        }, 
        {
            "location": "/mapping/#check-trimming", 
            "text": "First of all, check using  fastqc  that the trimming did remove the adapters that were contaminated the reads.  Again, with  parallel  specify the  max  number of jobs with the option  -j  to fit the #cpus booked  find . -name  reads.truncated.bz2  | parallel -j 12  fastqc {}     using the character   tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using  htop .  check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters?", 
            "title": "check trimming"
        }, 
        {
            "location": "/mapping/#filter-for-unique-reads", 
            "text": "Uniqueness  of reads refers to mappability. The less locations a read has in a genome, the higher is mappability will be.\nA common filter is to use  30  as a threshold for filtering reads:  samtools view -b -q 30 file.bam   file.q30.bam  Filter in parallel  parallel  samtools view -b -q 30 {}   {.}.q30.bam  ::: *.bam  Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?", 
            "title": "filter for unique reads"
        }, 
        {
            "location": "/mapping/#filter-for-duplicates", 
            "text": "Duplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage.\nDuplicates have a unclear definition in a mapped file. Usually, single-end reads that are mapped\nat the same 5' end are considered as duplicates. External coordinate are used for paired-end reads. \nFor regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are,\nby nature, clustered at one location this is not recommended. If duplication is observed at the reads level, \nsuch as in  fastqc  output, then filtering may be necessary. Marking duplicates allows to keep track of them without losing them.", 
            "title": "filter for duplicates?"
        }, 
        {
            "location": "/peak/", 
            "text": "Peak calling\n\n\nUsing \nMACS2\n\n\nFor both the day 0 and day 3 of differentiation into adipocytes, two files are available\n\n\n\n\ninput, as control\n\n\nhistone modification H3K4\n\n\n\n\nMACS2\n is going to use both files to normalize the read counts and perform the peak calling.\n\n\nRetrieve the BAM files with all chromosomes\n\n\ncd ~/chip-seq\nmkdir bams\ncd bams\nln -s /work/users/aginolhac/chip-seq/data/*.bam .\n\n\n\n\nPerform peak calling\n\n\nmacs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 \n\nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3\n\n\n\n\ncheck model inferred by MACS2\n\n\nfirst load R as a module and execute R script.\n\n\nmodule load lang/R\nRscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r\nRscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r\n\n\n\n\nfetch the pdf produced.\n\n\nsort per chromosomes and coordinates\n\n\nfind TC* -name '*.bdg' | parallel \nsort -k1,1 -k2,2n {} \n {.}.sort.bdg\n\n\n\n\n\nconvert to bigwig\n\n\nin order to get smaller files\n\n\nfind TC* -name '*sort.bdg' | parallel -j 1 \n/work/users/aginolhac/chip-seq/bedGraphToBigWig {} /work/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\n\n\n\n\n\nFetch the files and display them in IGV\n\n\nPerform peak calling with broad option\n\n\nmacs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-ST2-H3K4-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K4-D0-broad \n\nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-A-H3K4-D3-broad -B -q 0.01 --outdir TC1-A-H3K4-D3-broad\n\n\n\n\nGREAT analysis\n\n\nThe website \nGREAT\n allows to paste bed regions of enriched regions.\n\n\npredict functions of cis-regulatory regions\n\n\nUsing the \nTC1-A-H3K4_peaks.narrowPeak\n file produced by MACS2.\n\n\nThis file has the different fields:\n\n\n\n\nchromosome\n\n\nstart\n\n\nend\n\n\npeak name\n\n\ninteger score for display\n\n\nstrand\n\n\nfold-change\n\n\n-log10pvalue\n\n\n-log10qvalue\n\n\nrelative summit position to peak start\n\n\n\n\nLet's format the file as a 3 fields BED file and focus on more significant peaks filtering on \nq-values\n.\n\n\nawk '$9\n40' TC1-A-H3K4_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' \n TC1-A-H3K4_peaks.bed\n\n\n\n\nthen  \n\n\n\n\nload the BED in \nGREAT\n  \n\n\nfor the relevant genome, \nmm10\n  \n\n\nassociation rule: single nearest genome\n\n\n\n\nDifferential peak calling\n\n\nODIN\n allows to compare two conditions associated with their own controls.\n\n\nA command line looks like\n\n\nrgt-ODIN  --input-1=../TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n          --input-2=../TC1-I-A-D3.GRCm38.p3.q30.bam \\\n          -m -n TC1-I-A-D0vsD15 -v \\\n          TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n          ../references/GRCm38.p3.fasta ../references/GRCm38.p3.chom.sizes", 
            "title": "Peak calling"
        }, 
        {
            "location": "/peak/#peak-calling", 
            "text": "Using  MACS2  For both the day 0 and day 3 of differentiation into adipocytes, two files are available   input, as control  histone modification H3K4   MACS2  is going to use both files to normalize the read counts and perform the peak calling.  Retrieve the BAM files with all chromosomes  cd ~/chip-seq\nmkdir bams\ncd bams\nln -s /work/users/aginolhac/chip-seq/data/*.bam .  Perform peak calling  macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0  \nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3  check model inferred by MACS2  first load R as a module and execute R script.  module load lang/R\nRscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r\nRscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r  fetch the pdf produced.  sort per chromosomes and coordinates  find TC* -name '*.bdg' | parallel  sort -k1,1 -k2,2n {}   {.}.sort.bdg   convert to bigwig  in order to get smaller files  find TC* -name '*sort.bdg' | parallel -j 1  /work/users/aginolhac/chip-seq/bedGraphToBigWig {} /work/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig   Fetch the files and display them in IGV  Perform peak calling with broad option  macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-ST2-H3K4-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K4-D0-broad  \nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-A-H3K4-D3-broad -B -q 0.01 --outdir TC1-A-H3K4-D3-broad", 
            "title": "Peak calling"
        }, 
        {
            "location": "/peak/#great-analysis", 
            "text": "The website  GREAT  allows to paste bed regions of enriched regions.  predict functions of cis-regulatory regions  Using the  TC1-A-H3K4_peaks.narrowPeak  file produced by MACS2.  This file has the different fields:   chromosome  start  end  peak name  integer score for display  strand  fold-change  -log10pvalue  -log10qvalue  relative summit position to peak start   Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on  q-values .  awk '$9 40' TC1-A-H3K4_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/'   TC1-A-H3K4_peaks.bed  then     load the BED in  GREAT     for the relevant genome,  mm10     association rule: single nearest genome   Differential peak calling  ODIN  allows to compare two conditions associated with their own controls.  A command line looks like  rgt-ODIN  --input-1=../TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n          --input-2=../TC1-I-A-D3.GRCm38.p3.q30.bam \\\n          -m -n TC1-I-A-D0vsD15 -v \\\n          TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n          ../references/GRCm38.p3.fasta ../references/GRCm38.p3.chom.sizes", 
            "title": "GREAT analysis"
        }, 
        {
            "location": "/contact/", 
            "text": "Aurelien Ginolhac\n\n\naurelien.ginolhac@uni.lu\n\n\n--\n\nUniversity of Luxembourg\n\nFaculty of Science, Technology and Communication\n\nLife Science Research Unit\n\nCampus Belval, Biotech II - Office 4.22\n\n6, Avenue du Swing\n\nL-4367 Belvaux \n\nPhone: (+352) 46 66 44 6560\n\nFax: (+352) 46 66 44 36560", 
            "title": "Contact"
        }
    ]
}