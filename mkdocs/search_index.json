{
    "docs": [
        {
            "location": "/", 
            "text": "ChIP-seq practical session\n\n\nRunning all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance clusters (HPC).\n\n\nlog in \ngaia\n\n\ngaia\n is one of the \nHPC of the UNI\n.\n\n\nconnect to the frontend\n\n\nTo connect to it, you need an account and an authorized ssh key. Actually, a pair of keys, one public and one private.\nThe public key is sent over when connecting to the remote and compared to the authorized private key.\nA match allows the sender to log in. No password required.\n\n\nAfter the setting up of your account, the following should work if you are using mac or GNU/Linux:\n\n\nssh gaia-cluster\n\n\n\n\nOtherwise, on Windows, right-click on \npageant\n in the system tray and load a saved session \ngaia\n. In the terminal, log as your username, such as \nstudent01\n.\n\n\nYou should see the following prompt of the gaia frontend:\n\n\n===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $\n\n\n\n\nNote that you are on the \naccess\n frontend.\n\n\nThe frontend is meant for browsing / transferring your files only and you \nMUST\n connect to a node for any computational work \nusing the utility \noarsub\n described \nhere\n. This program managed the queuing system and dispatch jobs among the resources according to the demands.\n\n\nSoftware are organized into \nmodules\n that provide you with the binaries but also all the environment required for their running processes.\n\n\nTMUX\n\n\nlog in to a remote computer is great, all computation, heat generation is happening elsewhere but this comes with a price: disconnection.\n\nThis happens all the time. The way to get around it, is to have a \nscreen\n system that store your terminal, commands, environment in which \nyou can easily detach and re-attach.\n\n\nTwo systems exist, \nscreen\n and \ntmux\n. Both work well, but \ntmux\n has a nicer interface IMHO.\n\n\na short tutorial is \naccessible here.\n\n\nBriefly, on the \naccess\n frontend, start a \ntmux\n instance with\n\n\ntmux\n\n\n\n\nto detach (press CTRL and B together, release then use the next key):\n\n\nCTRL + B, then D\n\n\n\n\nto re-attach:\n\n\ntmux attach\n\n\n\n\nor the alias\n\n\ntmux at\n\n\n\n\nsome useful commands\n\n\nonce in an instance,\n\n\n\n\ncreate\n a new tab\n\n\n\n\nCTRL + B, then C\n\n\n\n\n\n\nmove to the \nnext\n tab\n\n\n\n\nCTRL + B, then N\n\n\n\n\n\n\nmove to the \nprevious\n tab\n\n\n\n\nCTRL + B, then P\n\n\n\n\n\n\nrename\n to the current tab\n\n\n\n\nCTRL + B, then ,\n\n\n\n\nthen type the new name\n\n\nQuit\n\n\nexit\n\n\n\n\nin all tabs kills the \ntmux\n session\n\n\nOf note, \ntmux\n instances live until the frontend is rebooted.\n\n\nconnect to a node\n\n\nConnecting to a computing node is anyway required to use modules.\n\n\nYou need to book ressources by specifying how many cores, optionaly if they are a same node, and a walltime clock. A job can never get extended.\n\n\nFor Thursday:\n\n\noarsub -I -t inner=3950979 -l nodes=1,walltime=9\noarsub -I -t inner=3950994 -l nodes=1,walltime=9\noarsub -I -t inner=3950995 -l nodes=1,walltime=9\n\n\n\n\nFor Friday:\n\n\noarsub -I -t inner=3950983 -l nodes=1,walltime=9\noarsub -I -t inner=3950992 -l nodes=1,walltime=9\noarsub -I -t inner=3950993 -l nodes=1,walltime=9\n\n\n\n\nWithout entering into the details of \nsubmitting a job\n,\nhere is the explanation for the above command:\n\n\n\n\n-I\n is for interactive, default is passive\n\n\n-t inner=xxx\n, connect to a \ncontainer\n, specific for today because we booked resources for the course\n\n\n-l\n define the resources you need. The less you ask for, the more high up you are in the queue. A \nnode\n is usually composed of 12 or 24 \ncores\n, \nso 12 tasks could be run in parallel. \nwalltime\n define for how long in hours your job will last.\n\n\n\n\nOnce logged in, the prompt changes for:\n\n\n09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326-\n717]\n\n\n\n\nwhere you see the node you are logged to (here \ngaia-66\n), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).\n\n\nmonitoring the resources used\n\n\nOn a shared cluster, you have to take of \nthree\n things:\n\n\n\n\nmemory usage\n\n\ncores used\n\n\ndisk space\n\n\n\n\nmemory\n\n\nEach node has\nOn an interactive session, use the command \nhtop\n to see if the memory is not full. If the system is swapping (using hard drives for memory storage)\nit becomes super slow and eventually stalled.\n\n\nFor passive sessions, you can use \nganglia\n to check out the nodes you are using.\n\n\ncores\n\n\neven if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources.\nIn this example, each task will use 1/10th of a core, then runs very slowly.\n\nOn an interactive session, use the command \nhtop\n to see if a process is correctly using close to 100% of a core.\n\n\ndisk space\n\n\nLike on your local machine, you need to check how much data you used.\nUsing a command line, you could use\n\n\ndisk usage\n\n\ndu -sh ~\n\n\n\n\nto display your disk usage (\ndu\n) for your home folder (\n~\n). In a form readable by human (\n-h\n)\n\n\ndisk free\n\n\ndf -h\n\n\n\n\ndisk free scans all disks mounted. Could takes time to display the global usage. \nPlease worry if only few Mb are available on the disk you are planning to write to.\n\n\nclosing connection\n\n\nWhen you are done, you can kill yourself your job by either doing \nCTRL + D\n or typing \nexit\n.\nThat will free your booked ressources for others. Once done, you will still logged on the frontend and normally inside a \ntmux\n.\nThe best is to detach from the \ntmux\n instance and log off from the gaia frontend using  \nCTRL + D\n or typing \nexit\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#chip-seq-practical-session", 
            "text": "Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance clusters (HPC).", 
            "title": "ChIP-seq practical session"
        }, 
        {
            "location": "/#log-in-gaia", 
            "text": "gaia  is one of the  HPC of the UNI .  connect to the frontend  To connect to it, you need an account and an authorized ssh key. Actually, a pair of keys, one public and one private.\nThe public key is sent over when connecting to the remote and compared to the authorized private key.\nA match allows the sender to log in. No password required.  After the setting up of your account, the following should work if you are using mac or GNU/Linux:  ssh gaia-cluster  Otherwise, on Windows, right-click on  pageant  in the system tray and load a saved session  gaia . In the terminal, log as your username, such as  student01 .  You should see the following prompt of the gaia frontend:  ===============================================================================\n /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !\n     First reserve your nodes (using oarsub(1))\nLinux access.gaia-cluster.uni.lux 3.2.0-4-amd64 unknown\n 16:45:49 up 126 days,  1:28, 34 users,  load average: 0.96, 1.58, 1.73\n0 16:45:49 your_login@access(gaia-cluster) ~ $  Note that you are on the  access  frontend.  The frontend is meant for browsing / transferring your files only and you  MUST  connect to a node for any computational work \nusing the utility  oarsub  described  here . This program managed the queuing system and dispatch jobs among the resources according to the demands.  Software are organized into  modules  that provide you with the binaries but also all the environment required for their running processes.  TMUX  log in to a remote computer is great, all computation, heat generation is happening elsewhere but this comes with a price: disconnection. \nThis happens all the time. The way to get around it, is to have a  screen  system that store your terminal, commands, environment in which \nyou can easily detach and re-attach.  Two systems exist,  screen  and  tmux . Both work well, but  tmux  has a nicer interface IMHO.  a short tutorial is  accessible here.  Briefly, on the  access  frontend, start a  tmux  instance with  tmux  to detach (press CTRL and B together, release then use the next key):  CTRL + B, then D  to re-attach:  tmux attach  or the alias  tmux at  some useful commands  once in an instance,   create  a new tab   CTRL + B, then C   move to the  next  tab   CTRL + B, then N   move to the  previous  tab   CTRL + B, then P   rename  to the current tab   CTRL + B, then ,  then type the new name  Quit  exit  in all tabs kills the  tmux  session  Of note,  tmux  instances live until the frontend is rebooted.  connect to a node  Connecting to a computing node is anyway required to use modules.  You need to book ressources by specifying how many cores, optionaly if they are a same node, and a walltime clock. A job can never get extended.  For Thursday:  oarsub -I -t inner=3950979 -l nodes=1,walltime=9\noarsub -I -t inner=3950994 -l nodes=1,walltime=9\noarsub -I -t inner=3950995 -l nodes=1,walltime=9  For Friday:  oarsub -I -t inner=3950983 -l nodes=1,walltime=9\noarsub -I -t inner=3950992 -l nodes=1,walltime=9\noarsub -I -t inner=3950993 -l nodes=1,walltime=9  Without entering into the details of  submitting a job ,\nhere is the explanation for the above command:   -I  is for interactive, default is passive  -t inner=xxx , connect to a  container , specific for today because we booked resources for the course  -l  define the resources you need. The less you ask for, the more high up you are in the queue. A  node  is usually composed of 12 or 24  cores , \nso 12 tasks could be run in parallel.  walltime  define for how long in hours your job will last.   Once logged in, the prompt changes for:  09:14:48 your_login@gaia-66(gaia-cluster)[OAR3511326- 717]  where you see the node you are logged to (here  gaia-66 ), the job ID (3511326) and the time in minutes before your job will be killed (717 minutes).", 
            "title": "log in gaia"
        }, 
        {
            "location": "/#monitoring-the-resources-used", 
            "text": "On a shared cluster, you have to take of  three  things:   memory usage  cores used  disk space   memory  Each node has\nOn an interactive session, use the command  htop  to see if the memory is not full. If the system is swapping (using hard drives for memory storage)\nit becomes super slow and eventually stalled.  For passive sessions, you can use  ganglia  to check out the nodes you are using.  cores  even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources.\nIn this example, each task will use 1/10th of a core, then runs very slowly. \nOn an interactive session, use the command  htop  to see if a process is correctly using close to 100% of a core.  disk space  Like on your local machine, you need to check how much data you used.\nUsing a command line, you could use  disk usage  du -sh ~  to display your disk usage ( du ) for your home folder ( ~ ). In a form readable by human ( -h )  disk free  df -h  disk free scans all disks mounted. Could takes time to display the global usage. \nPlease worry if only few Mb are available on the disk you are planning to write to.  closing connection  When you are done, you can kill yourself your job by either doing  CTRL + D  or typing  exit .\nThat will free your booked ressources for others. Once done, you will still logged on the frontend and normally inside a  tmux .\nThe best is to detach from the  tmux  instance and log off from the gaia frontend using   CTRL + D  or typing  exit .", 
            "title": "monitoring the resources used"
        }, 
        {
            "location": "/cli/", 
            "text": "command line\n\n\nThe programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac.\n\n\nYou need to know these commands:\n\n\npwd\nmore\nless\ncp\nmv\nmkdir\nls\ncd\nchmod\nrm\nfind\n\n\n\n\nTODO introduce \ntmux\n\n\nTwo useful tips:\n\n\n\n\nuse \nTAB\n on your keyboard for command and name completion.\n\n\nthe \nup\n arrow allows to browse your history (also available with \nhistory\n)\n\n\n\n\nExercise 1\n\n\ngo to your home folder\n\n\ncd\n\ncreate a fake file by using\n\n\necho \"hello world\" \n filetest\n\nsee if this file is present\n\nls -l\n\nread it\n\n\nmore fileTest\n\nOf note, \nless\n is an alternative to \nmore\n\nrename it\n\n\nmv fileTest test\n\ncheck\n\n\nls -l\n\ncreate a folder\n\n\nmkdir TEST\n\nOf note, all commands are case-sensitive\n\nll\n\n\nll\n is a classic alias for \nls -l\n\nmove the file in this folder\n\n\nmv test TEST\n\ncheck, and\nsee if present in the folder\n\n\nll TEST\n\ncopy it in the current folder\n\n\ncp TEST/test .\n\n\n.\n is the current folder, \n..\n is the folder one level close to the root \n/\n\nnow we have the same file, with the same name, one in the TEST folder, one in the current.\nuse the up arrow, you should see 'cp TEST/test .'\nand change it for\n\n\ncp TEST/test test2\n\nthe first and last field of \nls -l\n should provide\n\n\ndrwxr-xr-x TEST\n-rw-r--r-- test\n-rw-r--r-- test2\n\n\n\n\ntrash test\n\n\nrm test\n\nif this command doesn't ask for confirmation, let me know we may change this behavior.\n\n\nchmod\n allows changing permissions\n\n\ntry to read the file \ntest\n after\n\n\nchmod 222 test\n\n\nr\n stands for read, \nw\n for write and \nx\n for execution for files and browsing for folders.\n\nthe first pattern is the owner\n\nthe second pattern is for the group\n\nthe third pattern is for everyone else\n\n\nTODO more details needed\n\n\ntext editor\n\n\nLet's have a look at a text editor, there is plenty of them, the one I use is \nvim\n, why?\n\nBecause\n\n\n\n\nit's commonly installed on servers\n\n\nextremely powerful\n\n\n\n\nenter the editor\n\n\nvim test2\n\nyou have two modes\n\n\n\n\ncommand\n\n\ninsert\n\n\n\n\nBy default you are in the command mode, let's enter in the editor mode with either \ni\n or \ninsert\n on your keyboard.\nYou should see\n--INSERT--\n at the bottom.\nNow you can edit your file.\nWhen its finished, press \nECHAP\n to return in the command mode. You must enter \n:\n for each command.\nThe useful ones  \n\n\n\n\nw\n save changes to the file\n\n\n:q!\n quit without saving changes\n\n\n:wq\n write and quit\n\n\n\n\nExercise 2\n\n\nfind\n is great but not at all user-friendly.\nTry to find all your files which are bigger than 1 Go.\nthen which are older than 1 year.\nImagine doing this with windows...\n\n\nExercise 3, using a FASTA file\n\n\nGet all sequences in genbank with the keyword trnl\n\n\nhttp://www.ncbi.nlm.nih.gov/sites/entrez?db=nuccore\ncmd=search\nterm=trnl\n\n\nbut download only sequences from one class like \nmammals\n as a FASTA file.\nYou should obtain a 1.3 Mo file. Otherwise, you can use \n/home/users/aginolhac/trnl_mammals.fasta\n\n\n\n\n\n\nhow to obtain the first 500 lines of a file?\n\nsee the command \nhead\n and its manual \nman head\n. You can redirect the output to a file with 'command \n file_first500.fasta' for example.\n\n\n\n\n\n\nhow can you obtain from line 400 to 560?\n\nThink of piping \nhead\n and \ntail\n\n\n\n\n\n\ncount how many lines in this file. See \nwc\n\n\n\n\n\n\ncount how many sequences you have in the FASTA file. look at \ngrep\n\n\n\n\n\n\nyou should have obtained the 500 first lines in a file and the 500 last in a second file. How can you merge these two files? look at \ncat\n\n\n\n\n\n\nTODO: Introduce pipe\n\n\nExtra questions\n\n\n\n\n\n\nthere is an empty file after each sequence. Try to remove them (my favorite is \nsed\n, but check the \n-v\n option of grep)\n\n\n\n\n\n\nExtract all headers (start with \n) then the \ngi number\n (see \ncut\n). Redirect to a file.\n\n\n\n\n\n\nLook if there some double gi, look at \nsort\n and \nuniq\n.", 
            "title": "Command line, basics"
        }, 
        {
            "location": "/cli/#command-line", 
            "text": "The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac.  You need to know these commands:  pwd\nmore\nless\ncp\nmv\nmkdir\nls\ncd\nchmod\nrm\nfind  TODO introduce  tmux  Two useful tips:   use  TAB  on your keyboard for command and name completion.  the  up  arrow allows to browse your history (also available with  history )   Exercise 1  go to your home folder  cd \ncreate a fake file by using  echo \"hello world\"   filetest \nsee if this file is present ls -l \nread it  more fileTest \nOf note,  less  is an alternative to  more \nrename it  mv fileTest test \ncheck  ls -l \ncreate a folder  mkdir TEST \nOf note, all commands are case-sensitive ll  ll  is a classic alias for  ls -l \nmove the file in this folder  mv test TEST \ncheck, and\nsee if present in the folder  ll TEST \ncopy it in the current folder  cp TEST/test .  .  is the current folder,  ..  is the folder one level close to the root  / \nnow we have the same file, with the same name, one in the TEST folder, one in the current.\nuse the up arrow, you should see 'cp TEST/test .'\nand change it for  cp TEST/test test2 \nthe first and last field of  ls -l  should provide  drwxr-xr-x TEST\n-rw-r--r-- test\n-rw-r--r-- test2  trash test  rm test \nif this command doesn't ask for confirmation, let me know we may change this behavior.  chmod  allows changing permissions  try to read the file  test  after  chmod 222 test  r  stands for read,  w  for write and  x  for execution for files and browsing for folders. \nthe first pattern is the owner \nthe second pattern is for the group \nthe third pattern is for everyone else  TODO more details needed  text editor  Let's have a look at a text editor, there is plenty of them, the one I use is  vim , why? \nBecause   it's commonly installed on servers  extremely powerful   enter the editor  vim test2 \nyou have two modes   command  insert   By default you are in the command mode, let's enter in the editor mode with either  i  or  insert  on your keyboard.\nYou should see --INSERT--  at the bottom.\nNow you can edit your file.\nWhen its finished, press  ECHAP  to return in the command mode. You must enter  :  for each command.\nThe useful ones     w  save changes to the file  :q!  quit without saving changes  :wq  write and quit   Exercise 2  find  is great but not at all user-friendly.\nTry to find all your files which are bigger than 1 Go.\nthen which are older than 1 year.\nImagine doing this with windows...  Exercise 3, using a FASTA file  Get all sequences in genbank with the keyword trnl  http://www.ncbi.nlm.nih.gov/sites/entrez?db=nuccore cmd=search term=trnl  but download only sequences from one class like  mammals  as a FASTA file.\nYou should obtain a 1.3 Mo file. Otherwise, you can use  /home/users/aginolhac/trnl_mammals.fasta    how to obtain the first 500 lines of a file? \nsee the command  head  and its manual  man head . You can redirect the output to a file with 'command   file_first500.fasta' for example.    how can you obtain from line 400 to 560? \nThink of piping  head  and  tail    count how many lines in this file. See  wc    count how many sequences you have in the FASTA file. look at  grep    you should have obtained the 500 first lines in a file and the 500 last in a second file. How can you merge these two files? look at  cat    TODO: Introduce pipe  Extra questions    there is an empty file after each sequence. Try to remove them (my favorite is  sed , but check the  -v  option of grep)    Extract all headers (start with  ) then the  gi number  (see  cut ). Redirect to a file.    Look if there some double gi, look at  sort  and  uniq .", 
            "title": "command line"
        }, 
        {
            "location": "/install/", 
            "text": "load necessary software as modules\n\n\nAdd location of these modules\n\n\nmodule use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/\n\n\n\n\nLoad the modules\n\n\nmodule load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\nmodule load bio/MACS2\nmodule load lang/Java\nmodule load lang/R/3.3.0-ictce-7.3.5-bare\n\n\n\n\nTweak for the \npicard\n\n\nmkdir -p ~/install/jar_root/\ncp /home/users/aginolhac/install/jar_root/picard.jar ~/install/jar_root/\n\n\n\n\nFinal tweak for \nGatk\n\n\ncp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/\n\n\n\n\nprepare your working environment\n\n\ngo to your home directory:\n\ncd\n\ncreate a new folder to work in:\n\nmkdir chip-seq\n\ngo inside:\n\ncd chip-seq\n\ncreate and go in a sub-folder:\n\nmkdir raw\n\ngo inside:\n\ncd raw\n\nsymbolic link the fastq files:\n\nln -s /work/users/aginolhac/chip-seq/doctoral_school/raw/C* .\n\n\ncheck integrity of files\n\n\nJust as a side note, such large files are usually a pain to download. Since they are the very raw files\nafter the sequencer (despite basecalling) checking their integrity is worth doing.\nComputing the \nmd5sum\n ensure you have the same file as your sequence provider.\nThen \npaleomix\n will check the FASTQ are correct, \ni. e\n have 4 lines in a correct format.\n\n\nmd5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5", 
            "title": "Setup"
        }, 
        {
            "location": "/install/#load-necessary-software-as-modules", 
            "text": "Add location of these modules  module use $RESIF_ROOTINSTALL/lcsb/modules/all\nmodule use /home/users/aginolhac/.local/easybuild/modules/all/  Load the modules  module load bio/FastQC\nmodule load bio/AdapterRemoval\nmodule load bio/paleomix\nmodule load bio/SAMtools/0.1.19-goolf-1.4.10\nmodule load bio/BWA\nmodule load bio/mapDamage\nmodule load bio/MACS2\nmodule load lang/Java\nmodule load lang/R/3.3.0-ictce-7.3.5-bare  Tweak for the  picard  mkdir -p ~/install/jar_root/\ncp /home/users/aginolhac/install/jar_root/picard.jar ~/install/jar_root/  Final tweak for  Gatk  cp /home/users/aginolhac/install/jar_root/GenomeAnalysisTK.jar ~/install/jar_root/", 
            "title": "load necessary software as modules"
        }, 
        {
            "location": "/install/#prepare-your-working-environment", 
            "text": "go to your home directory: cd \ncreate a new folder to work in: mkdir chip-seq \ngo inside: cd chip-seq \ncreate and go in a sub-folder: mkdir raw \ngo inside: cd raw \nsymbolic link the fastq files: ln -s /work/users/aginolhac/chip-seq/doctoral_school/raw/C* .", 
            "title": "prepare your working environment"
        }, 
        {
            "location": "/install/#check-integrity-of-files", 
            "text": "Just as a side note, such large files are usually a pain to download. Since they are the very raw files\nafter the sequencer (despite basecalling) checking their integrity is worth doing.\nComputing the  md5sum  ensure you have the same file as your sequence provider.\nThen  paleomix  will check the FASTQ are correct,  i. e  have 4 lines in a correct format.  md5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5", 
            "title": "check integrity of files"
        }, 
        {
            "location": "/fastqc/", 
            "text": "FASTQ Quality controls\n\n\nUsing \nFastQC\n you can perform the necessary controls over fastq files.\n\n\nfastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\n\n\n\nrunning in parallel\n\n\nIf you have booked \n2\n cores, otherwise update the \n-j\n option:\n\n\nparallel -j 4 \nfastqc {}\n ::: *.gz\n\n\n\n\nthe \n{}\n instruction will be replaced by all occurrences of the pattern \n*.gz\n, everything that ends by \n.gz\n. \nparallel\n \ntakes care of submitting a new job so the number of parallel remains the same.\n\n\nrunning serial (for information)\n\n\nA tidy bit of \nbash\n programming to do it for all files \nNOT\n in parallel\n\n\nfor f in *.gz\n  do fastqc $f\ndone\n\n\n\n\nvisualize the results\n\n\ncollect the \nhtml\n files using either \nrsync\n, \nscp\n for command lines or \nFileZilla\n for  GUI tool.\n\n\nYou should observe some issues that needs to be solve.", 
            "title": "sequence QC"
        }, 
        {
            "location": "/fastqc/#fastq-quality-controls", 
            "text": "Using  FastQC  you can perform the necessary controls over fastq files.  fastqc C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz  running in parallel  If you have booked  2  cores, otherwise update the  -j  option:  parallel -j 4  fastqc {}  ::: *.gz  the  {}  instruction will be replaced by all occurrences of the pattern  *.gz , everything that ends by  .gz .  parallel  \ntakes care of submitting a new job so the number of parallel remains the same.  running serial (for information)  A tidy bit of  bash  programming to do it for all files  NOT  in parallel  for f in *.gz\n  do fastqc $f\ndone  visualize the results  collect the  html  files using either  rsync ,  scp  for command lines or  FileZilla  for  GUI tool.  You should observe some issues that needs to be solve.", 
            "title": "FASTQ Quality controls"
        }, 
        {
            "location": "/mapping/", 
            "text": "paleomix, Next-Generation Sequencing wrapper\n\n\nthis framework is open-source and available on \nGitHub\n and \nwraps all steps from \nfastq\n to \nbam\n files. \nActually, this tool can do much more but the rest is out of scope. \nIts major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default.\nActually, not entirely true since independent tasks can be spawn on a separate machine.\nFull documentation available \nhere\n\n\ncheck if paleomix is available\n\n\npaleomix\n\n\n\n\nIn case it is not, your are certainly missing the module, please re-run \nthe \nmodule use\n and \nmodule load\n in the \nset-up\n\n\ntest your install\n\n\nfetch the example, reference is the human mitochondrial genome\n\n\nmkdir -p ~/install/paleomix/example\ncp -r /work/users/aginolhac/chip-seq/paleomix/examples/bam_pipeline/00* ~/install/paleomix/example\ncd ~/install/paleomix/example\n\n\n\n\nrun the example, start by a \ndry-run\n, adjust the number of threads accordingly.\n\n\npaleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml\n\n\n\n\nIf all fine, re-rerun the command without the \n--dry-run\n option\n\n\nOf note, calling \nmapDamage\n was disabled to limit the computation time (~ 35 min when included).\nAnyway, this tool is not used for ChIP-seq analysis.\n\n\nGenerate a makefile\n\n\nTrimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. \nPaleomix works in temporary folders, check the data produced and then copy back files that are complete. \nPlus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. \nThis goes through a \nYAML\n makefile. The syntax is pretty straight-forward.\nWhat matters is, that you use \nSPACES\n and not TABS.\n\n\nCreate a generic makefile (extension, \nyml\n or \nyaml\n to get syntax highlights)\n\n\ncd ~/chip-seq\npaleomix bam_pipeline mkfile \n mouse.yaml\n\n\n\n\nEdit the makefile\n\n\nusing your favorite text editor, edit the \nmouse.yaml\n. For example \nvim mouse.yaml\n or \nkate\n or \nnano\n.\n\n\nOptions\n\n\nFor duplicates, change the default behavior from \nfilter\n to \nmark\n  \n\n\n  PCRDuplicates: mark\n\n\n\n\nFeatures\n\n\nUnder the \nFeatures\n section, update the featurse that need to be performed.\nChange \nyes/no\n to match the following:\n\n\n  Features:\n    RawBAM: yes         # Generate BAM from the raw libraries (no indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.bam\n    RealignedBAM: no    # Generate indel-realigned BAM using the GATK Indel realigner\n                        #   Location: {Destination}/{Target}.{Genome}.realigned.bam\n    mapDamage: no       # Generate mapDamage plot for each (unrealigned) library\n                        #   Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/\n    Coverage: yes       # Generate coverage information for the raw BAM (wo/ indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.coverage\n    Depths: no          # Generate histogram of number of sites with a given read-depth\n                        #   Location: {Destination}/{Target}.{Genome}.depths\n    Summary: yes        # Generate summary table for each target\n                        #   Location: {Destination}/{Target}.summary\n    DuplicateHist: no   # Generate histogram of PCR duplicates, for use with PreSeq\n                        #   Location: {Destination}/{Target}.{Genome}.duphist/{Library}/\n\n\n\n\nIn detail, the \nRealignedBAM\n are important for calling variants, we only need to \nRawBAM\n.\nMoreover, the \nDepths\n also help to define which upper limit could be used for variant calling.\nThis is not in the scope of ChIP-seq analysis. Same for mapDamage, only relevant for ancient DNA.\n\n\nPrefixes\n\n\nThese are the references to align read to. You could notice that we are going to use only one chromosome\nto save computational time.\n\n\nPrefixes:\n  mouse_19:\n    Path: /work/users/aginolhac/chip-seq/doctoral_school/references/chr19.fasta\n\n\n\n\nSamples\n\n\nenter at the end of the makefile, the following lines, according to your login.\nAgain, do use \nspaces\n and not tabs for the indentation. For those who are lazy and use copy/paste in \nvim\n\n use the trick to \n:set paste\n to avoid extra spaces, comment hashes etc to be automatically added.\n\n\nBe careful to \nreplace\n \nstudent01\n by the relevant username. Descriptions of the different hierachical names\ncan be read \nhere\n\n\nTC1-I-A-D3:\n  TC1-I-A-D3:\n    TC1-I-A-D3:\n      \n14s006680-1-1\n: /home/users/student01/chip-seq/raw/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz\n\nTC1-H3K4-A-D3:\n  TC1-H3K4-A-D3:\n    TC1-H3K4-A-D3:\n      \n14s006647-1-1\n: /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\nTC1-I-ST2-D0:\n  TC1-I-ST2-D0:\n    TC1-I-ST2-D0:\n      \n14s006677-1-1\n: /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz\n\nTC1-H3K4-ST2-D0:\n  TC1-H3K4-ST2-D0:\n    TC1-H3K4-ST2-D0:\n      \n14s006644\n: /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz\n\n\n\n\nPerform the trimming / mapping\n\n\nFirst use the option \n--dry-run\n to spot mistakes.\n\n\nPlease \nadapt\n the \n--max-threads\n option to the #cpus actually booked\n\n\npaleomix bam_pipeline run --bwa-max-threads=4 --max-threads=12 --dry-run mouse.yaml\n\n\n\n\nwhen all green lights are on, remove the \ndry-run\n and perform the mapping.\n\n\ncheck trimming\n\n\nFirst of all, check using \nfastqc\n that the trimming did remove the adapters that were contaminated the reads.\n\n\nAgain, with \nparallel\n specify the \nmax\n number of jobs with the option \n-j\n to fit the #cpus booked\n\n\nfind . -name \nreads.truncated.bz2\n | parallel -j 12 \nfastqc {}\n \n\n\n\n\n\nusing the character \n tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using \nhtop\n.\n\n\ncheck especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters?\n\n\nfilter for unique reads\n\n\nUniqueness\n of reads refers to mappability. The fewer locations a read has in a genome, the higher is mappability will be.\nA common filter is to use \n30\n as a threshold for filtering reads. Filter them in parallel\n\n\nparallel \nsamtools view -b -q 30 {} \n {.}.q30.bam\n ::: *.bam\n\n\n\n\nSince we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?\n\n\nfilter for duplicates?\n\n\nDuplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage.\nDuplicates have an unclear definition in a mapped file. Usually, single-end reads that are mapped\nat the same 5' end are considered as duplicates. External coordinates are used for paired-end reads.\n\nFor regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are,\nby nature, clustered at one location this is not recommended. If duplication is observed at the reads level, \nsuch as in \nfastqc\n output, then filtering may be necessary. Marking duplicates allow keeping track of them without losing them.", 
            "title": "Mapping"
        }, 
        {
            "location": "/mapping/#paleomix-next-generation-sequencing-wrapper", 
            "text": "this framework is open-source and available on  GitHub  and \nwraps all steps from  fastq  to  bam  files. \nActually, this tool can do much more but the rest is out of scope. \nIts major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default.\nActually, not entirely true since independent tasks can be spawn on a separate machine.\nFull documentation available  here  check if paleomix is available  paleomix  In case it is not, your are certainly missing the module, please re-run \nthe  module use  and  module load  in the  set-up  test your install  fetch the example, reference is the human mitochondrial genome  mkdir -p ~/install/paleomix/example\ncp -r /work/users/aginolhac/chip-seq/paleomix/examples/bam_pipeline/00* ~/install/paleomix/example\ncd ~/install/paleomix/example  run the example, start by a  dry-run , adjust the number of threads accordingly.  paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=2 --dry-run 000_makefile.yaml  If all fine, re-rerun the command without the  --dry-run  option  Of note, calling  mapDamage  was disabled to limit the computation time (~ 35 min when included).\nAnyway, this tool is not used for ChIP-seq analysis.  Generate a makefile  Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. \nPaleomix works in temporary folders, check the data produced and then copy back files that are complete. \nPlus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. \nThis goes through a  YAML  makefile. The syntax is pretty straight-forward.\nWhat matters is, that you use  SPACES  and not TABS.  Create a generic makefile (extension,  yml  or  yaml  to get syntax highlights)  cd ~/chip-seq\npaleomix bam_pipeline mkfile   mouse.yaml  Edit the makefile  using your favorite text editor, edit the  mouse.yaml . For example  vim mouse.yaml  or  kate  or  nano .  Options  For duplicates, change the default behavior from  filter  to  mark       PCRDuplicates: mark  Features  Under the  Features  section, update the featurse that need to be performed.\nChange  yes/no  to match the following:    Features:\n    RawBAM: yes         # Generate BAM from the raw libraries (no indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.bam\n    RealignedBAM: no    # Generate indel-realigned BAM using the GATK Indel realigner\n                        #   Location: {Destination}/{Target}.{Genome}.realigned.bam\n    mapDamage: no       # Generate mapDamage plot for each (unrealigned) library\n                        #   Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/\n    Coverage: yes       # Generate coverage information for the raw BAM (wo/ indel realignment)\n                        #   Location: {Destination}/{Target}.{Genome}.coverage\n    Depths: no          # Generate histogram of number of sites with a given read-depth\n                        #   Location: {Destination}/{Target}.{Genome}.depths\n    Summary: yes        # Generate summary table for each target\n                        #   Location: {Destination}/{Target}.summary\n    DuplicateHist: no   # Generate histogram of PCR duplicates, for use with PreSeq\n                        #   Location: {Destination}/{Target}.{Genome}.duphist/{Library}/  In detail, the  RealignedBAM  are important for calling variants, we only need to  RawBAM .\nMoreover, the  Depths  also help to define which upper limit could be used for variant calling.\nThis is not in the scope of ChIP-seq analysis. Same for mapDamage, only relevant for ancient DNA.  Prefixes  These are the references to align read to. You could notice that we are going to use only one chromosome\nto save computational time.  Prefixes:\n  mouse_19:\n    Path: /work/users/aginolhac/chip-seq/doctoral_school/references/chr19.fasta  Samples  enter at the end of the makefile, the following lines, according to your login.\nAgain, do use  spaces  and not tabs for the indentation. For those who are lazy and use copy/paste in  vim \n use the trick to  :set paste  to avoid extra spaces, comment hashes etc to be automatically added.  Be careful to  replace   student01  by the relevant username. Descriptions of the different hierachical names\ncan be read  here  TC1-I-A-D3:\n  TC1-I-A-D3:\n    TC1-I-A-D3:\n       14s006680-1-1 : /home/users/student01/chip-seq/raw/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz\n\nTC1-H3K4-A-D3:\n  TC1-H3K4-A-D3:\n    TC1-H3K4-A-D3:\n       14s006647-1-1 : /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz\n\nTC1-I-ST2-D0:\n  TC1-I-ST2-D0:\n    TC1-I-ST2-D0:\n       14s006677-1-1 : /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz\n\nTC1-H3K4-ST2-D0:\n  TC1-H3K4-ST2-D0:\n    TC1-H3K4-ST2-D0:\n       14s006644 : /home/users/student01/chip-seq/raw/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz  Perform the trimming / mapping  First use the option  --dry-run  to spot mistakes.  Please  adapt  the  --max-threads  option to the #cpus actually booked  paleomix bam_pipeline run --bwa-max-threads=4 --max-threads=12 --dry-run mouse.yaml  when all green lights are on, remove the  dry-run  and perform the mapping.", 
            "title": "paleomix, Next-Generation Sequencing wrapper"
        }, 
        {
            "location": "/mapping/#check-trimming", 
            "text": "First of all, check using  fastqc  that the trimming did remove the adapters that were contaminated the reads.  Again, with  parallel  specify the  max  number of jobs with the option  -j  to fit the #cpus booked  find . -name  reads.truncated.bz2  | parallel -j 12  fastqc {}     using the character   tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using  htop .  check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters?", 
            "title": "check trimming"
        }, 
        {
            "location": "/mapping/#filter-for-unique-reads", 
            "text": "Uniqueness  of reads refers to mappability. The fewer locations a read has in a genome, the higher is mappability will be.\nA common filter is to use  30  as a threshold for filtering reads. Filter them in parallel  parallel  samtools view -b -q 30 {}   {.}.q30.bam  ::: *.bam  Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?", 
            "title": "filter for unique reads"
        }, 
        {
            "location": "/mapping/#filter-for-duplicates", 
            "text": "Duplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage.\nDuplicates have an unclear definition in a mapped file. Usually, single-end reads that are mapped\nat the same 5' end are considered as duplicates. External coordinates are used for paired-end reads. \nFor regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are,\nby nature, clustered at one location this is not recommended. If duplication is observed at the reads level, \nsuch as in  fastqc  output, then filtering may be necessary. Marking duplicates allow keeping track of them without losing them.", 
            "title": "filter for duplicates?"
        }, 
        {
            "location": "/peak/", 
            "text": "Peak calling\n\n\nUsing \nMACS2\n\n\nFor both the day 0 and day 3 of differentiation into adipocytes, two files are available\n\n\n\n\ninput, as control\n\n\nhistone modification H3K4\n\n\n\n\nMACS2\n is going to use both files to normalize the read counts and perform the peak calling.\n\n\nRetrieve the BAM files with all chromosomes\n\n\ncd ~/chip-seq\nmkdir bams\ncd bams\nln -s /work/users/aginolhac/chip-seq/doctoral_school/data/*.bam .\n\n\n\n\n\nPerform peak calling\n\n\nmacs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 \n\nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3\n\n\n\n\n\nIn case \nmacs2\n gives \ncommand not found\n, your are certainly missing the module, please re-run \nthe \nmodule use\n and \nmodule load\n in the \nset-up\n\n\ncheck model inferred by MACS2\n\n\nexecute R script.\n\n\nRscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r\nRscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r\n\n\n\n\n\nfetch the pdf produced.\n\n\nsort per chromosomes and coordinates\n\n\nfind TC* -name '*.bdg' | parallel \nsort -k1,1 -k2,2n {} \n {.}.sort.bdg\n\n\n\n\n\n\nconvert to bigwig\n\n\nin order to get smaller files\n\n\nfind TC* -name '*sort.bdg' | parallel -j 2 \n/work/users/aginolhac/chip-seq/doctoral_school/bedGraphToBigWig {} \\\n  /work/users/aginolhac/chip-seq/doctoral_school/references/GRCm38.p3.chom.sizes {.}.bigwig\n\n\n\n\n\n\nFetch the files and display them in IGV\n\n\nIGV can be \ndownloaded\n from the broadinstitute. \n\n\nPerform peak calling with broad option\n\n\nmacs2 callpeak -t TC1-H3K27-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-ST2-H3K27-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K27-D0-broad \n\nmacs2 callpeak -t TC1-H3K27-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-A-H3K27-D3-broad -B -q 0.01 --outdir TC1-A-H3K27-D3-broad\n\n\n\n\n\nGet the \nbigwig\n files for \nH3K27\n.\nRedo those sort and conversion steps but only for the folders that end with 'broad'\n\n\nfind TC*broad -name '*.bdg' | parallel \nsort -k1,1 -k2,2n {} \n {.}.sort.bdg\n\nfind TC*broad -name '*sort.bdg' | parallel -j 2 \n/work/users/aginolhac/chip-seq/doctoral_school/bedGraphToBigWig {} \\\n  /work/users/aginolhac/chip-seq/doctoral_school/references/GRCm38.p3.chom.sizes {.}.bigwig\n\n\n\n\n\n\nGREAT analysis\n\n\nThe website \nGREAT\n allows pasting bed regions of enriched regions.\n\n\npredict functions of cis-regulatory regions\n\n\nUsing the \nTC1-A-H3K4_peaks.narrowPeak\n file produced by MACS2.\n\n\nThis file has the different fields:\n\n\n\n\nchromosome\n\n\nstart\n\n\nend\n\n\npeak name\n\n\ninteger score for display\n\n\nstrand\n\n\nfold-change\n\n\n-log\n10\n pvalue\n\n\n-log\n10\n qvalue\n\n\nrelative summit position to peak start\n\n\n\n\nLet's format the file as a 3 fields BED file and focus on more significant peaks filtering on \nq-values\n.\n\n\nawk '$9\n40'  TC1-A-H3K4/TC1-A-H3K4_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' \n  TC1-A-H3K4/TC1-A-H3K4_peaks.bed\ncat TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broadPeak | cut -f 1-3 | sed 's/^/chr/' \n TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broad.bed\n\n\n\n\n\nthen  \n\n\n\n\nload the BED in \nGREAT\n  \n\n\nfor the relevant genome, \nmm10\n  \n\n\nassociation rule:\n\n\nSingle nearest gene\n for \nH3K4\n \n\n\nTwo nearest genes\n for \nH3K27\n \n\n\n\n\n\n\n\n\nDifferential peak calling\n\n\nODIN\n allows comparing two conditions associated with their own controls.\n\n\nA command line looks like\n\n\nrgt-ODIN  --input-1=../TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n          --input-2=../TC1-I-A-D3.GRCm38.p3.q30.bam \\\n          -m -n TC1-I-A-D0vsD15 -v \\\n          TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n          ../references/GRCm38.p3.fasta ../references/GRCm38.p3.chom.sizes", 
            "title": "Peak calling"
        }, 
        {
            "location": "/peak/#peak-calling", 
            "text": "Using  MACS2  For both the day 0 and day 3 of differentiation into adipocytes, two files are available   input, as control  histone modification H3K4   MACS2  is going to use both files to normalize the read counts and perform the peak calling.  Retrieve the BAM files with all chromosomes  cd ~/chip-seq\nmkdir bams\ncd bams\nln -s /work/users/aginolhac/chip-seq/doctoral_school/data/*.bam .  Perform peak calling  macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0  \nmacs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3  In case  macs2  gives  command not found , your are certainly missing the module, please re-run \nthe  module use  and  module load  in the  set-up  check model inferred by MACS2  execute R script.  Rscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r\nRscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r  fetch the pdf produced.  sort per chromosomes and coordinates  find TC* -name '*.bdg' | parallel  sort -k1,1 -k2,2n {}   {.}.sort.bdg   convert to bigwig  in order to get smaller files  find TC* -name '*sort.bdg' | parallel -j 2  /work/users/aginolhac/chip-seq/doctoral_school/bedGraphToBigWig {} \\\n  /work/users/aginolhac/chip-seq/doctoral_school/references/GRCm38.p3.chom.sizes {.}.bigwig   Fetch the files and display them in IGV  IGV can be  downloaded  from the broadinstitute.   Perform peak calling with broad option  macs2 callpeak -t TC1-H3K27-ST2-D0.GRCm38.p3.q30.bam \\\n               -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-ST2-H3K27-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K27-D0-broad  \nmacs2 callpeak -t TC1-H3K27-A-D3.GRCm38.p3.q30.bam \\\n               -c TC1-I-A-D3.GRCm38.p3.q30.bam \\\n               -f BAM --broad -g mm -n TC1-A-H3K27-D3-broad -B -q 0.01 --outdir TC1-A-H3K27-D3-broad  Get the  bigwig  files for  H3K27 .\nRedo those sort and conversion steps but only for the folders that end with 'broad'  find TC*broad -name '*.bdg' | parallel  sort -k1,1 -k2,2n {}   {.}.sort.bdg \nfind TC*broad -name '*sort.bdg' | parallel -j 2  /work/users/aginolhac/chip-seq/doctoral_school/bedGraphToBigWig {} \\\n  /work/users/aginolhac/chip-seq/doctoral_school/references/GRCm38.p3.chom.sizes {.}.bigwig", 
            "title": "Peak calling"
        }, 
        {
            "location": "/peak/#great-analysis", 
            "text": "The website  GREAT  allows pasting bed regions of enriched regions.  predict functions of cis-regulatory regions  Using the  TC1-A-H3K4_peaks.narrowPeak  file produced by MACS2.  This file has the different fields:   chromosome  start  end  peak name  integer score for display  strand  fold-change  -log 10  pvalue  -log 10  qvalue  relative summit position to peak start   Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on  q-values .  awk '$9 40'  TC1-A-H3K4/TC1-A-H3K4_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/'    TC1-A-H3K4/TC1-A-H3K4_peaks.bed\ncat TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broadPeak | cut -f 1-3 | sed 's/^/chr/'   TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broad.bed  then     load the BED in  GREAT     for the relevant genome,  mm10     association rule:  Single nearest gene  for  H3K4    Two nearest genes  for  H3K27       Differential peak calling  ODIN  allows comparing two conditions associated with their own controls.  A command line looks like  rgt-ODIN  --input-1=../TC1-I-ST2-D0.GRCm38.p3.q30.bam \\\n          --input-2=../TC1-I-A-D3.GRCm38.p3.q30.bam \\\n          -m -n TC1-I-A-D0vsD15 -v \\\n          TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\\n          ../references/GRCm38.p3.fasta ../references/GRCm38.p3.chom.sizes", 
            "title": "GREAT analysis"
        }, 
        {
            "location": "/public/", 
            "text": "Public data, Mikkelsen et al.\n\n\nPublic data can be used in 2 ways:\n\n\n\n\ndownload processed data uploaded by the authors. See Sarah Diehl's tutorial\n\n\ndownload raw data\n\n\n\n\nUse the first one save time but rely on the authors's workflow\n\n\nFetch fastq\n\n\nGo on \nNCBI GEO\n\n\nThen select RunSelector.\n\n\nFrom a \nCell\n paper from 2010 and belong to this dataset:\n\n\nGEO GSE20752\n\n\nSpecifically we want to look at these samples:\n\n\n\n\n3T3L1_t2_H3K4me3\n\n\n3T3L1_t3_H3K4me3\n\n\n3T3L1_t2_H3K27ac\n \n\n\n3T3L1_t3_H3K27ac\n\n\n\n\nte input control they used for normalization:\n\n\ninput control\n\n\nDownload \nSRR_Acc_List.txt\n in \n/work/users/aginolhac/chip-seq/doctoral_school/Mikkelsen\n\n\nfastq-dump\n is a tool from NCBI, part of the \nsra-tools\n, available for free. Install this program.\n\n\nthen fetch and compress\n\n\nparallel -j 6 --progress \"fastq-dump --gzip {}\" :::: SRR_Acc_List.txt\n\n\nmapped with paleomix\n\n\nfile available \nhere\n\n\npaleomix bam_pipeline --bwa-max-threads=4 --max-threads=12 mikkelsen.yml\n\n\n\n\nlast for ~ 3 hours 30 minutes\n\n\npeak calling\n\n\nH3K4\n\n\nmacs2 callpeak -t Mikkelsen_3T3L1_t2_H3K4me3.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\\n               -f BAM -g mm -n 3T3L1_t2_H3K4 -B -q 0.01 --outdir 3T3L1_t2_H3K4 \n\nmacs2 callpeak -t Mikkelsen_3T3L1_t3_H3K4me3.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\\n               -f BAM -g mm -n3T3L1_t3_H3K4 -B -q 0.01 --outdir 3T3L1_t3_H3K4\n\n\n\n\nH3K27\n\n\nmacs2 callpeak -t Mikkelsen_3T3L1_t2_H3K27ac.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad  \\\n               -f BAM -g mm -n 3T3L1_t2_H3K27ac -B -q 0.01 --outdir 3T3L1_t2_H3K27ac \n\nmacs2 callpeak -t Mikkelsen_3T3L1_t3_H3K27ac.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad \\\n               -f BAM -g mm -n3T3L1_t3_H3K27ac -B -q 0.01 --outdir 3T3L1_t3_H3K27ac", 
            "title": "Public data"
        }, 
        {
            "location": "/public/#public-data-mikkelsen-et-al", 
            "text": "Public data can be used in 2 ways:   download processed data uploaded by the authors. See Sarah Diehl's tutorial  download raw data   Use the first one save time but rely on the authors's workflow  Fetch fastq  Go on  NCBI GEO  Then select RunSelector.  From a  Cell  paper from 2010 and belong to this dataset:  GEO GSE20752  Specifically we want to look at these samples:   3T3L1_t2_H3K4me3  3T3L1_t3_H3K4me3  3T3L1_t2_H3K27ac    3T3L1_t3_H3K27ac   te input control they used for normalization:  input control  Download  SRR_Acc_List.txt  in  /work/users/aginolhac/chip-seq/doctoral_school/Mikkelsen  fastq-dump  is a tool from NCBI, part of the  sra-tools , available for free. Install this program.  then fetch and compress  parallel -j 6 --progress \"fastq-dump --gzip {}\" :::: SRR_Acc_List.txt  mapped with paleomix  file available  here  paleomix bam_pipeline --bwa-max-threads=4 --max-threads=12 mikkelsen.yml  last for ~ 3 hours 30 minutes  peak calling  H3K4  macs2 callpeak -t Mikkelsen_3T3L1_t2_H3K4me3.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\\n               -f BAM -g mm -n 3T3L1_t2_H3K4 -B -q 0.01 --outdir 3T3L1_t2_H3K4  \nmacs2 callpeak -t Mikkelsen_3T3L1_t3_H3K4me3.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\\n               -f BAM -g mm -n3T3L1_t3_H3K4 -B -q 0.01 --outdir 3T3L1_t3_H3K4  H3K27  macs2 callpeak -t Mikkelsen_3T3L1_t2_H3K27ac.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad  \\\n               -f BAM -g mm -n 3T3L1_t2_H3K27ac -B -q 0.01 --outdir 3T3L1_t2_H3K27ac  \nmacs2 callpeak -t Mikkelsen_3T3L1_t3_H3K27ac.GRCm38.p3.bam \\\n               -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad \\\n               -f BAM -g mm -n3T3L1_t3_H3K27ac -B -q 0.01 --outdir 3T3L1_t3_H3K27ac", 
            "title": "Public data, Mikkelsen et al."
        }, 
        {
            "location": "/contact/", 
            "text": "Aurelien Ginolhac\n\n\naurelien.ginolhac@uni.lu\n\n\n--\n\nUniversity of Luxembourg\n\nFaculty of Science, Technology and Communication\n\nLife Science Research Unit\n\nCampus Belval, Biotech II - Office 4.22\n\n6, Avenue du Swing\n\nL-4367 Belvaux \n\nPhone: (+352) 46 66 44 6560\n\nFax: (+352) 46 66 44 36560", 
            "title": "Contact"
        }
    ]
}