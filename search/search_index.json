{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ChIP-seq practical session Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance clusters (HPC). Moreover, bioinformatic analyses involve many inter-dependent steps that need to be coherently run by a workflow manager such as snakemake Lecture slides are available as a pdf, click below Log in iris iris is one of the High Performance Computer (HPC) of the UNI . The HPC team has prepared a getting started page if you wish learn about this. You should use your account or one the student account prepared for you. Connect to the frontend To connect to it, you need an account and an authorized ssh key. Actually, a pair of keys, one public and one private. The public key is sent over when connecting to the remote and compared to the authorized private key. A match allows the sender to log in. No password required. For MacOS/Linux Instructions: follow step 1a of this tutorial After the setting up of your account, the following should work if you are using mac or GNU/Linux: ssh iris-cluster For Windows Instructions: follow step 1c of this tutorial Otherwise, on Windows, use xmobaterm . In the terminal, log as your username, such as student15 . You should see the following prompt of the iris frontend: ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) username@access1.iris-cluster.uni.lux(11:30:46): ~ $ Note that you are on the access frontend. The frontend is meant for browsing / transferring your files only and you MUST connect to a node for any computational work using the utility slurm described here . This program managed the queuing system and dispatch jobs among the resources according to the demands. Softwares are organized into modules that provide you with the binaries but also all the environment required for their running processes. However, we will use a container that will ease our analyses. The login procedure can be depicted as: TMUX log in to a remote computer is great, all computation, heat generation is happening elsewhere but this comes with a price: disconnection. This happens all the time. The way to get around it is to have a screen system that stores your terminal, commands, environment in which you can easily detach and re-attach. Two systems exist, screen and tmux . Both work well, but tmux has a nicer interface IMHO. a short tutorial is accessible here. Briefly, on the access frontend, start a tmux instance with tmux to detach (press CTRL and B together, release then use the next key): CTRL + B, then D to re-attach: tmux attach or the alias tmux at Useful commands once in an instance, create a new tab CTRL + B, then C move to the next tab CTRL + B, then N move to the previous tab CTRL + B, then P rename to the current tab CTRL + B, then , then type the new name Quit exit in all tabs kills the tmux session Of note, tmux instances live until the frontend is rebooted. Book resources on a computing node Connecting to a computing node is required to use resources. You need to book resources by specifying how many cores, optionally if they are on a same node, the memory required, and a wall time clock. A job can never get extended. Without entering into the details of submitting a job . The less you ask for, the more high up you are in the queue. Here is the explanation for the above command: srun is for interactive, sbatch for passive --time= following by hour:minute:second for wall time clock --mem= with 12GB for booking 12 gigabytes -c cores, A node is usually composed of 28 cores Once logged in, the prompt changes for: username@iris-001(11:17:55) where you see the node you are logged to (here iris-001 ). Monitoring the resources used On a shared cluster, you have to take care of three things: memory usage cores used disk space Memory Each node has On an interactive session, use the command htop to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled. For passive jobs, you can join a computing node by using the sjoin nodeid jobid command. Where nodeid and jobid can be autocomplemented by TAB . See page help Cores even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly. On an interactive session, use the command htop to see if a process is correctly using close to 100% of a core. Disk space Like on your local machine, you need to check how much data you used. Using a command line, you could use Disk usage du -sh ~ to display your disk usage ( du ) for your home folder ( ~ ). In a form readable by human ( -h ) Disk free df-ulphc disk free scans all disks mounted. Could takes time to display the global usage. Please worry if only few Mb are available on the disk you are planning to write to. check also your own quota with df-ulhpc on the frontend. Closing connection When you are done, you can kill yourself your job by either doing CTRL + D or typing exit . That will free your booked resources for others. Once done, you will still log on the frontend and normally inside a tmux . The best is to detach from the tmux instance and log off from the gaia frontend using CTRL + D or typing exit .","title":"Home"},{"location":"#chip-seq-practical-session","text":"Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance clusters (HPC). Moreover, bioinformatic analyses involve many inter-dependent steps that need to be coherently run by a workflow manager such as snakemake","title":"ChIP-seq practical session"},{"location":"#lecture","text":"slides are available as a pdf, click below","title":"Lecture"},{"location":"#log-in-iris","text":"iris is one of the High Performance Computer (HPC) of the UNI . The HPC team has prepared a getting started page if you wish learn about this. You should use your account or one the student account prepared for you.","title":"Log in iris"},{"location":"#connect-to-the-frontend","text":"To connect to it, you need an account and an authorized ssh key. Actually, a pair of keys, one public and one private. The public key is sent over when connecting to the remote and compared to the authorized private key. A match allows the sender to log in. No password required.","title":"Connect to the frontend"},{"location":"#for-macoslinux","text":"Instructions: follow step 1a of this tutorial After the setting up of your account, the following should work if you are using mac or GNU/Linux: ssh iris-cluster","title":"For MacOS/Linux"},{"location":"#for-windows","text":"Instructions: follow step 1c of this tutorial Otherwise, on Windows, use xmobaterm . In the terminal, log as your username, such as student15 . You should see the following prompt of the iris frontend: ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) username@access1.iris-cluster.uni.lux(11:30:46): ~ $ Note that you are on the access frontend. The frontend is meant for browsing / transferring your files only and you MUST connect to a node for any computational work using the utility slurm described here . This program managed the queuing system and dispatch jobs among the resources according to the demands. Softwares are organized into modules that provide you with the binaries but also all the environment required for their running processes. However, we will use a container that will ease our analyses. The login procedure can be depicted as:","title":"For Windows"},{"location":"#tmux","text":"log in to a remote computer is great, all computation, heat generation is happening elsewhere but this comes with a price: disconnection. This happens all the time. The way to get around it is to have a screen system that stores your terminal, commands, environment in which you can easily detach and re-attach. Two systems exist, screen and tmux . Both work well, but tmux has a nicer interface IMHO. a short tutorial is accessible here. Briefly, on the access frontend, start a tmux instance with tmux to detach (press CTRL and B together, release then use the next key): CTRL + B, then D to re-attach: tmux attach or the alias tmux at","title":"TMUX"},{"location":"#useful-commands","text":"once in an instance, create a new tab CTRL + B, then C move to the next tab CTRL + B, then N move to the previous tab CTRL + B, then P rename to the current tab CTRL + B, then , then type the new name","title":"Useful commands"},{"location":"#quit","text":"exit in all tabs kills the tmux session Of note, tmux instances live until the frontend is rebooted.","title":"Quit"},{"location":"#book-resources-on-a-computing-node","text":"Connecting to a computing node is required to use resources. You need to book resources by specifying how many cores, optionally if they are on a same node, the memory required, and a wall time clock. A job can never get extended. Without entering into the details of submitting a job . The less you ask for, the more high up you are in the queue. Here is the explanation for the above command: srun is for interactive, sbatch for passive --time= following by hour:minute:second for wall time clock --mem= with 12GB for booking 12 gigabytes -c cores, A node is usually composed of 28 cores Once logged in, the prompt changes for: username@iris-001(11:17:55) where you see the node you are logged to (here iris-001 ).","title":"Book resources on a computing node"},{"location":"#monitoring-the-resources-used","text":"On a shared cluster, you have to take care of three things: memory usage cores used disk space","title":"Monitoring the resources used"},{"location":"#memory","text":"Each node has On an interactive session, use the command htop to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled. For passive jobs, you can join a computing node by using the sjoin nodeid jobid command. Where nodeid and jobid can be autocomplemented by TAB . See page help","title":"Memory"},{"location":"#cores","text":"even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly. On an interactive session, use the command htop to see if a process is correctly using close to 100% of a core.","title":"Cores"},{"location":"#disk-space","text":"Like on your local machine, you need to check how much data you used. Using a command line, you could use","title":"Disk space"},{"location":"#disk-usage","text":"du -sh ~ to display your disk usage ( du ) for your home folder ( ~ ). In a form readable by human ( -h )","title":"Disk usage"},{"location":"#disk-free","text":"df-ulphc disk free scans all disks mounted. Could takes time to display the global usage. Please worry if only few Mb are available on the disk you are planning to write to. check also your own quota with df-ulhpc on the frontend.","title":"Disk free"},{"location":"#closing-connection","text":"When you are done, you can kill yourself your job by either doing CTRL + D or typing exit . That will free your booked resources for others. Once done, you will still log on the frontend and normally inside a tmux . The best is to detach from the tmux instance and log off from the gaia frontend using CTRL + D or typing exit .","title":"Closing connection"},{"location":"atac-seq/","text":"The workflow of all steps is summarised below: and the template is hosted on the LCSB Gitlab Book resources on iris si is a shortcut for booking a short interactive session (30 minutes, 1 core). But, we can alter the time for one hour with -t and ask for more cores with -c . Example below where we ask for 1 jour and 6 cores: si -t 1:00:00 -c 6 Interactive sessions are limited to maximum 2 hours. (Optional) Reservations Two nodes were reserved for this course. This will work if you have a student accounts or if you are part of the Department of Life Sciences. To access them you need to add the following parameter for Thursday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210902 and for Friday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210903 Install the snakemake template We will work in the scratch partition. the alias to go there is: cds Type pwd to ensure you are in /scratch/users/username create the directory and go inside: mkdir snakemake-atac-seq cd snakemake-atac-seq and run the following commands: VERSION=\"v0.0.2\" wget -qO- https://gitlab.lcsb.uni.lu/aurelien.ginolhac/snakemake-atac-seq/-/archive/${VERSION}/snakemake-atac-seq-${VERSION}.tar.gz | tar xfz - --strip-components=1 this command will download, extract (without the root folder) the following files: config/ Dockerfile README.md workflow/ you may want to delete the Dockerfile and README.md if you wish, they are not used by snakemake for runtime. Load necessary tools load singularity the command once on a node is: (base) user@access module load tools/Singularity load the snakemake environment (base) user@access $ conda activate snakemake Of note, the 2 above steps can be replaced by the alias smk if you added the alias in your .bashrc It should look like this from accessing the access machine to getting the resources and activating the environment: (base) aginolhac@access1.iris-cluster.uni.lux(14:05:02)-> 20:56): ~ $ si -c 6 -t 1:00:00 # salloc -p interactive --qos debug -C batch salloc: Pending job allocation 2424900 salloc: job 2424900 queued and waiting for resources salloc: job 2424900 has been allocated resources salloc: Granted job allocation 2424900 salloc: Waiting for resource configuration salloc: Nodes iris-139 are ready for job (base) aginolhac@iris-139(14:17:21)-> 29:51)(2424900 1N/T/1CN): ~ $ smk (snakemake) aginolhac@iris-139(14:17:23)-> 29:49)(2424900 1N/T/1CN): ~ $ Run on human data Disclaimer : those sequence files are of human origin. You must not copy them except for this teaching exercise. Fetch data Make sure you are in /scratch/users/username/snakemake-atac-seq/ , then copy the 12 fastq files: mkdir data rsync -v /scratch/users/aginolhac/snakemake-atac-seq/data/*gz data/ Set-up the cache To avoid spending too much time fetching the same reference genome and indexing it for mapping, we will share our work for these parts. export SNAKEMAKE_OUTPUT_CACHE=/scratch/users/aginolhac/snakecache Dry-run First a dry-run as we did before: snakemake -j 6 -n Run If all correct, run the workflow with cache activated. Of note, my scratch is shared for fetching the databases of fastq_screen . snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" --cache -j 6 Fetch data back on your computer The needed files are: report.html results/qc/multiqc/multiqc.html results/big_wig/mDAN_D30_1.bigWig results/big_wig/mDAN_D30_2.bigWig results/big_wig/mDAN_D30_3.bigWig results/big_wig/smNPC_1.bigWig results/big_wig/smNPC_2.bigWig results/big_wig/smNPC_3.bigWig","title":"Workflow ATAC-seq"},{"location":"atac-seq/#book-resources-on-iris","text":"si is a shortcut for booking a short interactive session (30 minutes, 1 core). But, we can alter the time for one hour with -t and ask for more cores with -c . Example below where we ask for 1 jour and 6 cores: si -t 1:00:00 -c 6 Interactive sessions are limited to maximum 2 hours.","title":"Book resources on iris"},{"location":"atac-seq/#optional-reservations","text":"Two nodes were reserved for this course. This will work if you have a student accounts or if you are part of the Department of Life Sciences. To access them you need to add the following parameter for Thursday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210902 and for Friday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210903","title":"(Optional) Reservations"},{"location":"atac-seq/#install-the-snakemake-template","text":"We will work in the scratch partition. the alias to go there is: cds Type pwd to ensure you are in /scratch/users/username create the directory and go inside: mkdir snakemake-atac-seq cd snakemake-atac-seq and run the following commands: VERSION=\"v0.0.2\" wget -qO- https://gitlab.lcsb.uni.lu/aurelien.ginolhac/snakemake-atac-seq/-/archive/${VERSION}/snakemake-atac-seq-${VERSION}.tar.gz | tar xfz - --strip-components=1 this command will download, extract (without the root folder) the following files: config/ Dockerfile README.md workflow/ you may want to delete the Dockerfile and README.md if you wish, they are not used by snakemake for runtime.","title":"Install the snakemake template"},{"location":"atac-seq/#load-necessary-tools","text":"load singularity the command once on a node is: (base) user@access module load tools/Singularity load the snakemake environment (base) user@access $ conda activate snakemake Of note, the 2 above steps can be replaced by the alias smk if you added the alias in your .bashrc It should look like this from accessing the access machine to getting the resources and activating the environment: (base) aginolhac@access1.iris-cluster.uni.lux(14:05:02)-> 20:56): ~ $ si -c 6 -t 1:00:00 # salloc -p interactive --qos debug -C batch salloc: Pending job allocation 2424900 salloc: job 2424900 queued and waiting for resources salloc: job 2424900 has been allocated resources salloc: Granted job allocation 2424900 salloc: Waiting for resource configuration salloc: Nodes iris-139 are ready for job (base) aginolhac@iris-139(14:17:21)-> 29:51)(2424900 1N/T/1CN): ~ $ smk (snakemake) aginolhac@iris-139(14:17:23)-> 29:49)(2424900 1N/T/1CN): ~ $","title":"Load necessary tools"},{"location":"atac-seq/#run-on-human-data","text":"Disclaimer : those sequence files are of human origin. You must not copy them except for this teaching exercise.","title":"Run on human data"},{"location":"atac-seq/#fetch-data","text":"Make sure you are in /scratch/users/username/snakemake-atac-seq/ , then copy the 12 fastq files: mkdir data rsync -v /scratch/users/aginolhac/snakemake-atac-seq/data/*gz data/","title":"Fetch data"},{"location":"atac-seq/#set-up-the-cache","text":"To avoid spending too much time fetching the same reference genome and indexing it for mapping, we will share our work for these parts. export SNAKEMAKE_OUTPUT_CACHE=/scratch/users/aginolhac/snakecache","title":"Set-up the cache"},{"location":"atac-seq/#dry-run","text":"First a dry-run as we did before: snakemake -j 6 -n","title":"Dry-run"},{"location":"atac-seq/#run","text":"If all correct, run the workflow with cache activated. Of note, my scratch is shared for fetching the databases of fastq_screen . snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" --cache -j 6","title":"Run"},{"location":"atac-seq/#fetch-data-back-on-your-computer","text":"The needed files are: report.html results/qc/multiqc/multiqc.html results/big_wig/mDAN_D30_1.bigWig results/big_wig/mDAN_D30_2.bigWig results/big_wig/mDAN_D30_3.bigWig results/big_wig/smNPC_1.bigWig results/big_wig/smNPC_2.bigWig results/big_wig/smNPC_3.bigWig","title":"Fetch data back on your computer"},{"location":"chip-seq/","text":"The workflow of all steps is summarised below: and the template is hosted on the LCSB Gitlab Singularity containers Singularity allows to use containers (from i.e Docker ) on High-Performance Computer. For more details see the lecture by HPC team Shortly, a container with all the necessary tools, softwares and all libraries are embeded. Hence, you need to book the HPC resources but not install those softwares. Snakemake will load the container for every action it runs. Book resources on iris si is a shortcut for booking a short interactive session (30 minutes, 1 core). But, we can alter the time for one hour with -t and ask for more cores with -c . Example below where we ask for 1 jour and 6 cores: si -t 1:00:00 -c 6 Interactive sessions are limited to maximum 2 hours. See at the bottom how to submit passive jobs (max walltime: 48 hours). (Optional) Reservations Two nodes were reserved for this course. This will work if you have a student accounts or if you are part of the Department of Life Sciences. To access them you need to add the following parameter for Thursday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210902 and for Friday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210903 Install the snakemake template We will work in the scratch partition. the alias to go there is: cds Type pwd to ensure you are in /scratch/users/username create the directory and go inside: mkdir snakemake-chip-seq cd snakemake-chip-seq and run the following commands: VERSION=\"v0.1.1\" wget -qO- https://gitlab.lcsb.uni.lu/aurelien.ginolhac/snakemake-chip-seq/-/archive/${VERSION}/snakemake-chip-seq-${VERSION}.tar.gz | tar xfz - --strip-components=1 this command will download, extract (without the root folder) the following files: CHANGELOG.md config/ Dockerfile LICENSE README.md resources/ workflow/ You can check the list of files using ll (alias of long list: ls -l ) you may want to delete the LICENSE , Dockerfile , CHANGELOG.md and README.md if you wish, they are not used by snakemake for runtime. Fetch test datasets Using the nextflow datasets , clone it using git : git clone -b chipseq --depth 1 https://github.com/nf-core/test-datasets.git Load necessary tools load singularity the command once on a node is: (base) user@access module load tools/Singularity load the snakemake environment (base) user@access $ conda activate snakemake Of note, the 2 above steps can be replaced by the alias smk if you added the alias in your .bashrc It should look like this from accessing the access machine to getting the resources and activating the environment: (base) aginolhac@access1.iris-cluster.uni.lux(14:05:02)-> 20:56): ~ $ si -c 6 -t 1:00:00 # salloc -p interactive --qos debug -C batch salloc: Pending job allocation 2424900 salloc: job 2424900 queued and waiting for resources salloc: job 2424900 has been allocated resources salloc: Granted job allocation 2424900 salloc: Waiting for resource configuration salloc: Nodes iris-139 are ready for job (base) aginolhac@iris-139(14:17:21)-> 29:51)(2424900 1N/T/1CN): ~ $ smk (snakemake) aginolhac@iris-139(14:17:23)-> 29:49)(2424900 1N/T/1CN): ~ $ Test the workflow Config files 3 tabulated separated values file ( .tsv ) defined where are the data and how to process them. They are all in the config folder. Everything in the workflow contains the snakemake machinery and can be ignored if you are not interested into this. units This file has the key column in the first column, that must match the first colummn of samples.tsv . unit are for technical replicates. fq1 is the path the first pair of FASTQ or for single-end. In this last case, fq2 needs to be empty. The 2 last columns, sra_accession and platform are not going to be used (and are empty for sra_accession ). Display the content of config/units.tsv (using less config/units.tsv for example) it should look like: sample unit fq1 fq2 sra_accession platform Spt5_IN 1 test-datasets/testdata/SRR5204809_Spt5-ChIP_Input1_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204809_Spt5-ChIP_Input1_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA Spt5_IN 2 test-datasets/testdata/SRR5204810_Spt5-ChIP_Input2_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204810_Spt5-ChIP_Input2_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA Spt5 1 test-datasets/testdata/SRR5204807_Spt5-ChIP_IP1_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204807_Spt5-ChIP_IP1_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA Spt5 2 test-datasets/testdata/SRR5204808_Spt5-ChIP_IP2_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204808_Spt5-ChIP_IP2_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA samples Now look at config/samples.tsv : sample group batch_effect control antibody peak-analysis Spt5_IN SptA batch1 Spt Spt5 SptA batch1 Spt5_IN Spt narrow sample are ID that must match the units.tsv sample column. group are for biological replicate, batch_effect is self-explanatory. control allows to specify which input has to be used. Leave it empty for input DNA sample. antibody is for antobody or histone mark used. peak-analysis either narrow (for K4) or broad (K27 or K36). This will be used when running macs2 . config The file config.yaml contains the generic parameters for the analysis. More specifically, it allows to specify the reference genome to use. Here the section is designed for the yeast genome, that fits the test-data resources: ref: # Ensembl species name species: saccharomyces_cerevisiae # Ensembl release release: 101 # Genome build build: R64-1-1 # for testing data a single chromosome can be selected (leave empty for a regular analysis) chromosome: # specify release version number of igenomes list to use (see https://github.com/nf-core/chipseq/releases), e.g. 1.2.2 igenomes_release: 1.2.2 # if igenomes.yaml cannot be used, a value for the mappable or effective genome size can be specified here, e.g. macs-gsize: 2.7e9 macs-gsize: # if igenomes.yaml cannot be used, a path to an own blacklist can be specified here blacklist: This section will need to be updated when we use the real human data Run the test-data workflow Check that your prompt is indicating that you are on a computing node, and with the snakemake environment loaded. Something like that: (snakemake) 0 [student20@iris-114 student20](2469485 1N/T/1CN)$ Dry-run snakemake -n [Wed Sep 1 10:53:50 2021] localrule all: input: <TBD> jobid: 0 resources: tmpdir=/tmp Job stats: job count min threads max threads ----------------- ------- ------------- ------------- all 1 1 1 generate_igenomes 1 1 1 get_gsize 1 1 1 total 3 1 1 This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. Produce the Direct Acyclic Graph With the alias, it is dag . You can then fetch and look at the dag.pdf . All jobs are with a plain line, meaning they are not done. Run the real workflow snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" -j 6 First it download the singularity image (and will cache it). It takes a few minutes (feels long however). Then you can see the workflow moving on. 107 jobs are scheduled. Messages like System has not been booted with systemd as init system (PID 1). Can't operate. Failed to create bus connection: Host is down can be ignored. The whole workflow takes ~ 15 minutes with 6 cores. Create the report snakemake --report You can now fetch both report.html and results/qc/multiqc/multiqc.html that summarise the run. DAG again if you wish, regenerate the dag.pdf , the lines are now dashed. Run on human data Disclaimer : those sequence files are of human origin. You must not copy them except for this teaching exercise. Cleanup test data To avoid mingling with the test data results, remove results entirely, it will be created again with the human results. rm -rf results resources/ref Fetch sequencing data Make sure you are in /scratch/users/username/snakemake-chip-seq/ , then copy the 4 fastq files: mkdir data rsync -v /scratch/users/aginolhac/tmp/K7/data/*gz data/ Adapt config files The machinery in workflow is agnostic of the data origin. However, we need to set-up that we use new files. config, the human reference Edit config/config.yaml The following lines (lines 9-18) must be changed for the following ones: resources: ref: # Ensembl species name species: homo_sapiens # Ensembl release release: 101 # Genome build build: GRCh38 # for testing data a single chromosome can be selected (leave empty for a regular analysis) chromosome: 1 Meaning that we work with the human genome genome, and only the chromosome 1 (this can be changed later). This, for the sake of computation time. To run the workflow with all genome, you remove the 1 and samples Edit config/samples.tsv sample group batch_effect control antibody peak-analysis K7_K27 K7 batch1 K7_IN K27Ac broad K7_K36 K7 batch1 K7_IN K36me3 broad K7_K4 K7 batch1 K7_IN K4me3 narrow K7_IN K7 batch1 IN All histone marks will use the INPUT sample as control. And K4me3 are narrow peaks and the rest are broad. units Edit config/units.tsv sample unit fq1 fq2 sra_accession platform K7_K27 1 data/C8MN9ACXX_K7-WT-H3K27Ac.fastq.gz ILLUMINA K7_K36 1 data/C8MN9ACXX_K7-WT-H3K36me3.fastq.gz ILLUMINA K7_K4 1 data/C8MN9ACXX_K7-WT-H3K4me3.fastq.gz ILLUMINA K7_IN 1 data/C8MN9ACXX_K7-WT-I.fastq.gz ILLUMINA fq2 is empty as this is a single-end run. And we don't have technical replicates so all unit as 1 . Run the workflow makes sure to have resources booked for 2 hours: si -c 6 -t 2:00:00 and loaded singularity and snakemake . Set-up the cache To avoid spending too much time fetching the same reference genome and indexing it for mapping, we will share our work for these parts. export SNAKEMAKE_OUTPUT_CACHE=/scratch/users/aginolhac/snakecache Dry-run First a dry-run as we did before: snakemake -j 6 -n Run If all correct, run the workflow with cache activated. Of note, my scratch is shared for fetching the databases of fastq_screen . snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" --cache -j 6 The whole run should fit within the 2 hours walltime. Report Generate it using: snakemake --report Fetch data back on your computer On MacOS and GNU/Linux, use scp or rsync . On Windows, with MobaXterm, your files are displayed on the left part, on SFTP. Type /scratch/users/username like below: The needed files are: report.html results/qc/multiqc/multiqc.html results/bigwig/K7_IN.bigWig results/bigwig/K7_K27.bigWig results/bigwig/K7_K36.bigWig results/bigwig/K7_K4.bigWig results/macs2_callpeak/K7_K27-K7_IN.broad_peaks.broadPeak results/macs2_callpeak/K7_K36-K7_IN.broad_peaks.broadPeak results/macs2_callpeak/K7_K4-K7_IN.broad_peaks.narrowPeak Submit passive jobs It consists in two steps: create a file launcher.sh at the root of your snakemake that contains the following lines: #!/bin/bash -l #SBATCH -N 1 #SBATCH -J JOB_NAME #SBATCH --mail-type=begin,end,fail #SBATCH --mail-user=yours@mail.lu #SBATCH --ntasks-per-node=1 #SBATCH --mem-per-cpu=4096 #SBATCH -c 12 #SBATCH --time=0-06:00:00 #SBATCH -p batch module load tools/Singularity conda activate snakemake srun snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" -j 12 You need to update the JOB_NAME and your email address. You can also delete those 3 lines (line 3 to 5) if you are interested into this monitoring. The duration (here 6 hours) and number of cores can be adapted to your needs. run the submission with sbatch launcher.sh your job will be queued and start whenever it's possible and according to the fairshare . You can monitor your scheduling with squeue -la -u $USER See also the HPC team tutorial .","title":"Workflow ChIP-seq"},{"location":"chip-seq/#singularity-containers","text":"Singularity allows to use containers (from i.e Docker ) on High-Performance Computer. For more details see the lecture by HPC team Shortly, a container with all the necessary tools, softwares and all libraries are embeded. Hence, you need to book the HPC resources but not install those softwares. Snakemake will load the container for every action it runs.","title":"Singularity containers"},{"location":"chip-seq/#book-resources-on-iris","text":"si is a shortcut for booking a short interactive session (30 minutes, 1 core). But, we can alter the time for one hour with -t and ask for more cores with -c . Example below where we ask for 1 jour and 6 cores: si -t 1:00:00 -c 6 Interactive sessions are limited to maximum 2 hours. See at the bottom how to submit passive jobs (max walltime: 48 hours).","title":"Book resources on iris"},{"location":"chip-seq/#optional-reservations","text":"Two nodes were reserved for this course. This will work if you have a student accounts or if you are part of the Department of Life Sciences. To access them you need to add the following parameter for Thursday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210902 and for Friday: si -t 1:0:0 -c 6 --reservation=aginolhac-teaching-20210903","title":"(Optional) Reservations"},{"location":"chip-seq/#install-the-snakemake-template","text":"We will work in the scratch partition. the alias to go there is: cds Type pwd to ensure you are in /scratch/users/username create the directory and go inside: mkdir snakemake-chip-seq cd snakemake-chip-seq and run the following commands: VERSION=\"v0.1.1\" wget -qO- https://gitlab.lcsb.uni.lu/aurelien.ginolhac/snakemake-chip-seq/-/archive/${VERSION}/snakemake-chip-seq-${VERSION}.tar.gz | tar xfz - --strip-components=1 this command will download, extract (without the root folder) the following files: CHANGELOG.md config/ Dockerfile LICENSE README.md resources/ workflow/ You can check the list of files using ll (alias of long list: ls -l ) you may want to delete the LICENSE , Dockerfile , CHANGELOG.md and README.md if you wish, they are not used by snakemake for runtime.","title":"Install the snakemake template"},{"location":"chip-seq/#fetch-test-datasets","text":"Using the nextflow datasets , clone it using git : git clone -b chipseq --depth 1 https://github.com/nf-core/test-datasets.git","title":"Fetch test datasets"},{"location":"chip-seq/#load-necessary-tools","text":"load singularity the command once on a node is: (base) user@access module load tools/Singularity load the snakemake environment (base) user@access $ conda activate snakemake Of note, the 2 above steps can be replaced by the alias smk if you added the alias in your .bashrc It should look like this from accessing the access machine to getting the resources and activating the environment: (base) aginolhac@access1.iris-cluster.uni.lux(14:05:02)-> 20:56): ~ $ si -c 6 -t 1:00:00 # salloc -p interactive --qos debug -C batch salloc: Pending job allocation 2424900 salloc: job 2424900 queued and waiting for resources salloc: job 2424900 has been allocated resources salloc: Granted job allocation 2424900 salloc: Waiting for resource configuration salloc: Nodes iris-139 are ready for job (base) aginolhac@iris-139(14:17:21)-> 29:51)(2424900 1N/T/1CN): ~ $ smk (snakemake) aginolhac@iris-139(14:17:23)-> 29:49)(2424900 1N/T/1CN): ~ $","title":"Load necessary tools"},{"location":"chip-seq/#test-the-workflow","text":"","title":"Test the workflow"},{"location":"chip-seq/#config-files","text":"3 tabulated separated values file ( .tsv ) defined where are the data and how to process them. They are all in the config folder. Everything in the workflow contains the snakemake machinery and can be ignored if you are not interested into this.","title":"Config files"},{"location":"chip-seq/#units","text":"This file has the key column in the first column, that must match the first colummn of samples.tsv . unit are for technical replicates. fq1 is the path the first pair of FASTQ or for single-end. In this last case, fq2 needs to be empty. The 2 last columns, sra_accession and platform are not going to be used (and are empty for sra_accession ). Display the content of config/units.tsv (using less config/units.tsv for example) it should look like: sample unit fq1 fq2 sra_accession platform Spt5_IN 1 test-datasets/testdata/SRR5204809_Spt5-ChIP_Input1_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204809_Spt5-ChIP_Input1_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA Spt5_IN 2 test-datasets/testdata/SRR5204810_Spt5-ChIP_Input2_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204810_Spt5-ChIP_Input2_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA Spt5 1 test-datasets/testdata/SRR5204807_Spt5-ChIP_IP1_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204807_Spt5-ChIP_IP1_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA Spt5 2 test-datasets/testdata/SRR5204808_Spt5-ChIP_IP2_SacCer_ChIP-Seq_ss100k_R1.fastq.gz test-datasets/testdata/SRR5204808_Spt5-ChIP_IP2_SacCer_ChIP-Seq_ss100k_R2.fastq.gz ILLUMINA","title":"units"},{"location":"chip-seq/#samples","text":"Now look at config/samples.tsv : sample group batch_effect control antibody peak-analysis Spt5_IN SptA batch1 Spt Spt5 SptA batch1 Spt5_IN Spt narrow sample are ID that must match the units.tsv sample column. group are for biological replicate, batch_effect is self-explanatory. control allows to specify which input has to be used. Leave it empty for input DNA sample. antibody is for antobody or histone mark used. peak-analysis either narrow (for K4) or broad (K27 or K36). This will be used when running macs2 .","title":"samples"},{"location":"chip-seq/#config","text":"The file config.yaml contains the generic parameters for the analysis. More specifically, it allows to specify the reference genome to use. Here the section is designed for the yeast genome, that fits the test-data resources: ref: # Ensembl species name species: saccharomyces_cerevisiae # Ensembl release release: 101 # Genome build build: R64-1-1 # for testing data a single chromosome can be selected (leave empty for a regular analysis) chromosome: # specify release version number of igenomes list to use (see https://github.com/nf-core/chipseq/releases), e.g. 1.2.2 igenomes_release: 1.2.2 # if igenomes.yaml cannot be used, a value for the mappable or effective genome size can be specified here, e.g. macs-gsize: 2.7e9 macs-gsize: # if igenomes.yaml cannot be used, a path to an own blacklist can be specified here blacklist: This section will need to be updated when we use the real human data","title":"config"},{"location":"chip-seq/#run-the-test-data-workflow","text":"Check that your prompt is indicating that you are on a computing node, and with the snakemake environment loaded. Something like that: (snakemake) 0 [student20@iris-114 student20](2469485 1N/T/1CN)$ Dry-run snakemake -n [Wed Sep 1 10:53:50 2021] localrule all: input: <TBD> jobid: 0 resources: tmpdir=/tmp Job stats: job count min threads max threads ----------------- ------- ------------- ------------- all 1 1 1 generate_igenomes 1 1 1 get_gsize 1 1 1 total 3 1 1 This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. Produce the Direct Acyclic Graph With the alias, it is dag . You can then fetch and look at the dag.pdf . All jobs are with a plain line, meaning they are not done. Run the real workflow snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" -j 6 First it download the singularity image (and will cache it). It takes a few minutes (feels long however). Then you can see the workflow moving on. 107 jobs are scheduled. Messages like System has not been booted with systemd as init system (PID 1). Can't operate. Failed to create bus connection: Host is down can be ignored. The whole workflow takes ~ 15 minutes with 6 cores. Create the report snakemake --report You can now fetch both report.html and results/qc/multiqc/multiqc.html that summarise the run. DAG again if you wish, regenerate the dag.pdf , the lines are now dashed.","title":"Run the test-data workflow"},{"location":"chip-seq/#run-on-human-data","text":"Disclaimer : those sequence files are of human origin. You must not copy them except for this teaching exercise.","title":"Run on human data"},{"location":"chip-seq/#cleanup-test-data","text":"To avoid mingling with the test data results, remove results entirely, it will be created again with the human results. rm -rf results resources/ref","title":"Cleanup test data"},{"location":"chip-seq/#fetch-sequencing-data","text":"Make sure you are in /scratch/users/username/snakemake-chip-seq/ , then copy the 4 fastq files: mkdir data rsync -v /scratch/users/aginolhac/tmp/K7/data/*gz data/","title":"Fetch sequencing data"},{"location":"chip-seq/#adapt-config-files","text":"The machinery in workflow is agnostic of the data origin. However, we need to set-up that we use new files.","title":"Adapt config files"},{"location":"chip-seq/#config-the-human-reference","text":"Edit config/config.yaml The following lines (lines 9-18) must be changed for the following ones: resources: ref: # Ensembl species name species: homo_sapiens # Ensembl release release: 101 # Genome build build: GRCh38 # for testing data a single chromosome can be selected (leave empty for a regular analysis) chromosome: 1 Meaning that we work with the human genome genome, and only the chromosome 1 (this can be changed later). This, for the sake of computation time. To run the workflow with all genome, you remove the 1 and","title":"config, the human reference"},{"location":"chip-seq/#samples_1","text":"Edit config/samples.tsv sample group batch_effect control antibody peak-analysis K7_K27 K7 batch1 K7_IN K27Ac broad K7_K36 K7 batch1 K7_IN K36me3 broad K7_K4 K7 batch1 K7_IN K4me3 narrow K7_IN K7 batch1 IN All histone marks will use the INPUT sample as control. And K4me3 are narrow peaks and the rest are broad.","title":"samples"},{"location":"chip-seq/#units_1","text":"Edit config/units.tsv sample unit fq1 fq2 sra_accession platform K7_K27 1 data/C8MN9ACXX_K7-WT-H3K27Ac.fastq.gz ILLUMINA K7_K36 1 data/C8MN9ACXX_K7-WT-H3K36me3.fastq.gz ILLUMINA K7_K4 1 data/C8MN9ACXX_K7-WT-H3K4me3.fastq.gz ILLUMINA K7_IN 1 data/C8MN9ACXX_K7-WT-I.fastq.gz ILLUMINA fq2 is empty as this is a single-end run. And we don't have technical replicates so all unit as 1 .","title":"units"},{"location":"chip-seq/#run-the-workflow","text":"makes sure to have resources booked for 2 hours: si -c 6 -t 2:00:00 and loaded singularity and snakemake .","title":"Run the workflow"},{"location":"chip-seq/#set-up-the-cache","text":"To avoid spending too much time fetching the same reference genome and indexing it for mapping, we will share our work for these parts. export SNAKEMAKE_OUTPUT_CACHE=/scratch/users/aginolhac/snakecache","title":"Set-up the cache"},{"location":"chip-seq/#dry-run","text":"First a dry-run as we did before: snakemake -j 6 -n","title":"Dry-run"},{"location":"chip-seq/#run","text":"If all correct, run the workflow with cache activated. Of note, my scratch is shared for fetching the databases of fastq_screen . snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" --cache -j 6 The whole run should fit within the 2 hours walltime.","title":"Run"},{"location":"chip-seq/#report","text":"Generate it using: snakemake --report","title":"Report"},{"location":"chip-seq/#fetch-data-back-on-your-computer","text":"On MacOS and GNU/Linux, use scp or rsync . On Windows, with MobaXterm, your files are displayed on the left part, on SFTP. Type /scratch/users/username like below: The needed files are: report.html results/qc/multiqc/multiqc.html results/bigwig/K7_IN.bigWig results/bigwig/K7_K27.bigWig results/bigwig/K7_K36.bigWig results/bigwig/K7_K4.bigWig results/macs2_callpeak/K7_K27-K7_IN.broad_peaks.broadPeak results/macs2_callpeak/K7_K36-K7_IN.broad_peaks.broadPeak results/macs2_callpeak/K7_K4-K7_IN.broad_peaks.narrowPeak","title":"Fetch data back on your computer"},{"location":"chip-seq/#submit-passive-jobs","text":"It consists in two steps: create a file launcher.sh at the root of your snakemake that contains the following lines: #!/bin/bash -l #SBATCH -N 1 #SBATCH -J JOB_NAME #SBATCH --mail-type=begin,end,fail #SBATCH --mail-user=yours@mail.lu #SBATCH --ntasks-per-node=1 #SBATCH --mem-per-cpu=4096 #SBATCH -c 12 #SBATCH --time=0-06:00:00 #SBATCH -p batch module load tools/Singularity conda activate snakemake srun snakemake --use-singularity --singularity-args \"-B /scratch/users/aginolhac:/scratch/users/aginolhac\" -j 12 You need to update the JOB_NAME and your email address. You can also delete those 3 lines (line 3 to 5) if you are interested into this monitoring. The duration (here 6 hours) and number of cores can be adapted to your needs. run the submission with sbatch launcher.sh your job will be queued and start whenever it's possible and according to the fairshare . You can monitor your scheduling with squeue -la -u $USER See also the HPC team tutorial .","title":"Submit passive jobs"},{"location":"cli/","text":"Command line The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac. You need to know these commands: pwd more less cp mv mkdir ls cd chmod rm find Two useful tips: use TAB on your keyboard for command and name completion. the up arrow allows to browse your history (also available with history ) Exercise 1 go to your home folder cd create a fake file by using echo \"hello world\" > filetest see if this file is present ls -l read it more fileTest Of note, less is an alternative to more rename it mv fileTest test check ls -l create a folder mkdir TEST Of note, all commands are case-sensitive ll ll is a classic alias for ls -l move the file in this folder mv test TEST check, and see if present in the folder ll TEST copy it in the current folder cp TEST/test . . is the current folder, .. is the folder one level close to the root / now we have the same file, with the same name, one in the TEST folder, one in the current. use the up arrow, you should see 'cp TEST/test .' and change it for cp TEST/test test2 the first and last field of ls -l should provide drwxr-xr-x TEST -rw-r--r-- test -rw-r--r-- test2 trash test rm test if this command doesn't ask for confirmation, let me know we may change this behavior. chmod allows changing permissions try to read the file test after chmod 222 test r stands for read, w for write and x for execution for files and browsing for folders. the first pattern is the owner the second pattern is for the group the third pattern is for everyone else TODO more details needed Text editor nano often installed, it is easy to use as all commands are written at the bottom: ^G Get Help ^O WriteOut ^R Read File ^Y Prev Page ^K Cut Text ^C Cur Pos ^X Exit ^J Justify ^W Where Is ^V Next Page ^U UnCut Text ^T To Spell you can write/modify text directly and use CTRL + O to save then CTRL + X to exit. VIM Let's have a look at a text editor, there is plenty of them, the one I use is vim , why? Because: it's commonly installed on servers extremely powerful enter the editor vim test2 you have two modes command insert By default you are in the command mode, let's enter in the editor mode with either i or insert on your keyboard. You should see --INSERT-- at the bottom. Now you can edit your file. When its finished, press ECHAP to return in the command mode. You must enter : for each command. The useful ones w save changes to the file :q! quit without saving changes :wq write and quit","title":"Command line, basics"},{"location":"cli/#command-line","text":"The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac. You need to know these commands: pwd more less cp mv mkdir ls cd chmod rm find","title":"Command line"},{"location":"cli/#two-useful-tips","text":"use TAB on your keyboard for command and name completion. the up arrow allows to browse your history (also available with history )","title":"Two useful tips:"},{"location":"cli/#exercise-1","text":"go to your home folder cd create a fake file by using echo \"hello world\" > filetest see if this file is present ls -l read it more fileTest Of note, less is an alternative to more rename it mv fileTest test check ls -l create a folder mkdir TEST Of note, all commands are case-sensitive ll ll is a classic alias for ls -l move the file in this folder mv test TEST check, and see if present in the folder ll TEST copy it in the current folder cp TEST/test . . is the current folder, .. is the folder one level close to the root / now we have the same file, with the same name, one in the TEST folder, one in the current. use the up arrow, you should see 'cp TEST/test .' and change it for cp TEST/test test2 the first and last field of ls -l should provide drwxr-xr-x TEST -rw-r--r-- test -rw-r--r-- test2 trash test rm test if this command doesn't ask for confirmation, let me know we may change this behavior. chmod allows changing permissions try to read the file test after chmod 222 test r stands for read, w for write and x for execution for files and browsing for folders. the first pattern is the owner the second pattern is for the group the third pattern is for everyone else TODO more details needed","title":"Exercise 1"},{"location":"cli/#text-editor","text":"","title":"Text editor"},{"location":"cli/#nano","text":"often installed, it is easy to use as all commands are written at the bottom: ^G Get Help ^O WriteOut ^R Read File ^Y Prev Page ^K Cut Text ^C Cur Pos ^X Exit ^J Justify ^W Where Is ^V Next Page ^U UnCut Text ^T To Spell you can write/modify text directly and use CTRL + O to save then CTRL + X to exit.","title":"nano"},{"location":"cli/#vim","text":"Let's have a look at a text editor, there is plenty of them, the one I use is vim , why? Because: it's commonly installed on servers extremely powerful enter the editor vim test2 you have two modes command insert By default you are in the command mode, let's enter in the editor mode with either i or insert on your keyboard. You should see --INSERT-- at the bottom. Now you can edit your file. When its finished, press ECHAP to return in the command mode. You must enter : for each command. The useful ones w save changes to the file :q! quit without saving changes :wq write and quit","title":"VIM"},{"location":"contact/","text":"Aurelien Ginolhac aurelien.ginolhac at uni dot lu I acknowledge that we are in an existential human-induced climate crisis caused by excessive CO2 emissions from a variety of human activities. While I recognize that our day-to-day transportation, energy use, materialistic consumption, animal-based diets and excessive flying impact the climate crisis, individual mitigation alone is no substitute for policy reform. I acknowledge that the path to resolving this crisis requires systemic societal and structural change supported by strong political intervention rather than individual actions, technological innovation or investment alone. I acknowledge that we have less than a decade to drastically reduce CO2 emissions to prevent irreparable damage to our quality of life, well-being and the diversity of nature. \u2014 Acknowledgment provided by acknowledge-the-climate-crisis.org Image of the \ud83c\udf0f \ud83c\udf21\ufe0f 1850-2020 from ShowYourStripes -- University of Luxembourg Faculty of Science, Technology and Communication Departement of Life Science and Medecine Campus Belval, Building KTT \u2013 Office 403 6, Avenue du Swing L-4367 Belvaux","title":"Contact"},{"location":"mapping/","text":"paleomix, Next-Generation Sequencing wrapper this framework is open-source and available on GitHub and wraps all steps from fastq to bam files. Actually, this tool can do much more but the rest is out of scope. Its major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default. Actually, not entirely true since independent tasks can be spawn on a separate machine. Full documentation available here check if paleomix is available paleomix In case it is not, your are certainly not using the singularity container , see instructions in the set-up test your install fetch the example, reference is the human mitochondrial genome mkdir -p ~/paleomix/example cp -r /scratch/users/aginolhac/chip-seq/bam_pipeline_example/000* ~/paleomix/example cd ~/paleomix/example run the example, start by a dry-run , adjust the number of threads accordingly. paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=8 --dry-run 000_makefile.yaml If all fine, re-rerun the command without the --dry-run option Of note, calling mapDamage and GATK were disabled to limit the computation time (~ 35 min when included). Anyway, this tool is not used for ChIP-seq analysis. Generate a makefile Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folders, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes through a YAML makefile. The syntax is pretty straight-forward. What matters is, that you use SPACES and not TABS. Create a generic makefile (extension, yml or yaml to get syntax highlights) cd ~/chip-seq paleomix bam_pipeline mkfile > mouse.yaml Edit the makefile using your favorite text editor (such as nano ), edit the mouse.yaml . For example vim mouse.yaml or kate or nano . Options for the compression, change the default behavior from bz2 to gz CompressionFormat: gz for duplicates, change the default behavior from filter to mark PCRDuplicates: mark Features Under the Features section, update the featurse that need to be performed. Change yes/no to match the following: Features: RawBAM: yes # Generate BAM from the raw libraries (no indel realignment) # Location: {Destination}/{Target}.{Genome}.bam RealignedBAM: no # Generate indel-realigned BAM using the GATK Indel realigner # Location: {Destination}/{Target}.{Genome}.realigned.bam mapDamage: no # Generate mapDamage plot for each (unrealigned) library # Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/ Coverage: yes # Generate coverage information for the raw BAM (wo/ indel realignment) # Location: {Destination}/{Target}.{Genome}.coverage Depths: no # Generate histogram of number of sites with a given read-depth # Location: {Destination}/{Target}.{Genome}.depths Summary: yes # Generate summary table for each target # Location: {Destination}/{Target}.summary DuplicateHist: no # Generate histogram of PCR duplicates, for use with PreSeq # Location: {Destination}/{Target}.{Genome}.duphist/{Library}/ In detail, the RealignedBAM are important for calling variants, we only need to RawBAM . Moreover, the Depths also help to define which upper limit could be used for variant calling. This is not in the scope of ChIP-seq analysis. Same for mapDamage , only relevant for ancient DNA. Prefixes These are the references to align read to. You could notice that we are going to use only one chromosome to save computational time. Prefixes: mouse_19: Path: /scratch/users/aginolhac/chip-seq/references/chr19.fasta Samples enter at the end of the makefile, the following lines. Again, do use spaces and not tabs for the indentation. For those who are lazy and use copy/paste in vim use the trick to :set paste to avoid extra spaces, comment hashes etc to be automatically added. The descriptions of the different hierachical names can be read here TC1-I-A-D3: TC1-I-A-D3: TC1-I-A-D3: \"14s006680-1-1\": fastq/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz TC1-H3K4-A-D3: TC1-H3K4-A-D3: TC1-H3K4-A-D3: \"14s006647-1-1\": fastq/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz TC1-I-ST2-D0: TC1-I-ST2-D0: TC1-I-ST2-D0: \"14s006677-1-1\": fastq/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: \"14s006644\": fastq/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz Perform the trimming / mapping First use the option --dry-run to spot mistakes. Please adapt the --max-threads option to the #cpus actually booked paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 --dry-run mouse.yaml when all green lights are on, remove the dry-run and perform the mapping. correct the makefile the trimming should go well, but an error arises because you cannot write to the reference folder. symbolic links in your own folder, removing the file that should be created if the ref is correct mkdir references ln -s /scratch/users/aginolhac/chip-seq/references/chr19.fasta references/ correct the makefile, so the Path is relative now Path: references/chr19.fasta enjoy paleomix by just re-running it, only necessary steps are done (validatating the ref, indexing it and mappings) paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 mouse.yaml the whole process should takes ~ 25 minutes with 8 cores check trimming First of all, check using fastqc that the trimming did remove the adapters that were contaminated the reads. Again, with parallel specify the max number of jobs with the option -j to fit the #cpus booked find . -name \"reads.truncated.gz\" | parallel -j 8 \"fastqc {}\" & using the character & tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using htop . check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters? truncated read files are all named the same. And they are located into a deeper folder structure. To rename them all with their ids and move them at the ~/chip-seq/ level, you can run the following: find -name \"reads.truncated.gz\" | xargs ls -1 | awk '{split($1, path, \"/\"); system(\"mv \"$0 \" \"path[3] \"_\" path[6])}' filter for unique reads Uniqueness of reads refers to mappability. The fewer locations a read has in a genome, the higher is mappability will be. A common filter is to use 30 as a threshold for filtering reads. Filter them in parallel parallel \"samtools view -b -q 30 {} > {.}.q30.bam\" ::: *.bam Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why? filter for duplicates? Duplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage. Duplicates have an unclear definition in a mapped file. Usually, single-end reads that are mapped at the same 5' end are considered as duplicates. External coordinates are used for paired-end reads. For regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are, by nature, clustered at one location this is not recommended. If duplication is observed at the reads level, such as in fastqc output, then filtering may be necessary. Marking duplicates allow keeping track of them without losing them.","title":"Mapping"},{"location":"mapping/#paleomix-next-generation-sequencing-wrapper","text":"this framework is open-source and available on GitHub and wraps all steps from fastq to bam files. Actually, this tool can do much more but the rest is out of scope. Its major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default. Actually, not entirely true since independent tasks can be spawn on a separate machine. Full documentation available here check if paleomix is available paleomix In case it is not, your are certainly not using the singularity container , see instructions in the set-up","title":"paleomix, Next-Generation Sequencing wrapper"},{"location":"mapping/#test-your-install","text":"fetch the example, reference is the human mitochondrial genome mkdir -p ~/paleomix/example cp -r /scratch/users/aginolhac/chip-seq/bam_pipeline_example/000* ~/paleomix/example cd ~/paleomix/example run the example, start by a dry-run , adjust the number of threads accordingly. paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=8 --dry-run 000_makefile.yaml If all fine, re-rerun the command without the --dry-run option Of note, calling mapDamage and GATK were disabled to limit the computation time (~ 35 min when included). Anyway, this tool is not used for ChIP-seq analysis.","title":"test your install"},{"location":"mapping/#generate-a-makefile","text":"Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folders, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes through a YAML makefile. The syntax is pretty straight-forward. What matters is, that you use SPACES and not TABS. Create a generic makefile (extension, yml or yaml to get syntax highlights) cd ~/chip-seq paleomix bam_pipeline mkfile > mouse.yaml","title":"Generate a makefile"},{"location":"mapping/#edit-the-makefile","text":"using your favorite text editor (such as nano ), edit the mouse.yaml . For example vim mouse.yaml or kate or nano .","title":"Edit the makefile"},{"location":"mapping/#options","text":"for the compression, change the default behavior from bz2 to gz CompressionFormat: gz for duplicates, change the default behavior from filter to mark PCRDuplicates: mark","title":"Options"},{"location":"mapping/#features","text":"Under the Features section, update the featurse that need to be performed. Change yes/no to match the following: Features: RawBAM: yes # Generate BAM from the raw libraries (no indel realignment) # Location: {Destination}/{Target}.{Genome}.bam RealignedBAM: no # Generate indel-realigned BAM using the GATK Indel realigner # Location: {Destination}/{Target}.{Genome}.realigned.bam mapDamage: no # Generate mapDamage plot for each (unrealigned) library # Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/ Coverage: yes # Generate coverage information for the raw BAM (wo/ indel realignment) # Location: {Destination}/{Target}.{Genome}.coverage Depths: no # Generate histogram of number of sites with a given read-depth # Location: {Destination}/{Target}.{Genome}.depths Summary: yes # Generate summary table for each target # Location: {Destination}/{Target}.summary DuplicateHist: no # Generate histogram of PCR duplicates, for use with PreSeq # Location: {Destination}/{Target}.{Genome}.duphist/{Library}/ In detail, the RealignedBAM are important for calling variants, we only need to RawBAM . Moreover, the Depths also help to define which upper limit could be used for variant calling. This is not in the scope of ChIP-seq analysis. Same for mapDamage , only relevant for ancient DNA.","title":"Features"},{"location":"mapping/#prefixes","text":"These are the references to align read to. You could notice that we are going to use only one chromosome to save computational time. Prefixes: mouse_19: Path: /scratch/users/aginolhac/chip-seq/references/chr19.fasta","title":"Prefixes"},{"location":"mapping/#samples","text":"enter at the end of the makefile, the following lines. Again, do use spaces and not tabs for the indentation. For those who are lazy and use copy/paste in vim use the trick to :set paste to avoid extra spaces, comment hashes etc to be automatically added. The descriptions of the different hierachical names can be read here TC1-I-A-D3: TC1-I-A-D3: TC1-I-A-D3: \"14s006680-1-1\": fastq/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz TC1-H3K4-A-D3: TC1-H3K4-A-D3: TC1-H3K4-A-D3: \"14s006647-1-1\": fastq/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz TC1-I-ST2-D0: TC1-I-ST2-D0: TC1-I-ST2-D0: \"14s006677-1-1\": fastq/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: \"14s006644\": fastq/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz","title":"Samples"},{"location":"mapping/#perform-the-trimming-mapping","text":"First use the option --dry-run to spot mistakes. Please adapt the --max-threads option to the #cpus actually booked paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 --dry-run mouse.yaml when all green lights are on, remove the dry-run and perform the mapping.","title":"Perform the trimming / mapping"},{"location":"mapping/#correct-the-makefile","text":"the trimming should go well, but an error arises because you cannot write to the reference folder. symbolic links in your own folder, removing the file that should be created if the ref is correct mkdir references ln -s /scratch/users/aginolhac/chip-seq/references/chr19.fasta references/ correct the makefile, so the Path is relative now Path: references/chr19.fasta enjoy paleomix by just re-running it, only necessary steps are done (validatating the ref, indexing it and mappings) paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 mouse.yaml the whole process should takes ~ 25 minutes with 8 cores","title":"correct the makefile"},{"location":"mapping/#check-trimming","text":"First of all, check using fastqc that the trimming did remove the adapters that were contaminated the reads. Again, with parallel specify the max number of jobs with the option -j to fit the #cpus booked find . -name \"reads.truncated.gz\" | parallel -j 8 \"fastqc {}\" & using the character & tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using htop . check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters? truncated read files are all named the same. And they are located into a deeper folder structure. To rename them all with their ids and move them at the ~/chip-seq/ level, you can run the following: find -name \"reads.truncated.gz\" | xargs ls -1 | awk '{split($1, path, \"/\"); system(\"mv \"$0 \" \"path[3] \"_\" path[6])}'","title":"check trimming"},{"location":"mapping/#filter-for-unique-reads","text":"Uniqueness of reads refers to mappability. The fewer locations a read has in a genome, the higher is mappability will be. A common filter is to use 30 as a threshold for filtering reads. Filter them in parallel parallel \"samtools view -b -q 30 {} > {.}.q30.bam\" ::: *.bam Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?","title":"filter for unique reads"},{"location":"mapping/#filter-for-duplicates","text":"Duplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage. Duplicates have an unclear definition in a mapped file. Usually, single-end reads that are mapped at the same 5' end are considered as duplicates. External coordinates are used for paired-end reads. For regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are, by nature, clustered at one location this is not recommended. If duplication is observed at the reads level, such as in fastqc output, then filtering may be necessary. Marking duplicates allow keeping track of them without losing them.","title":"filter for duplicates?"},{"location":"peak/","text":"Peak calling Using MACS2 For both the day 0 and day 3 of differentiation into adipocytes, two files are available input, as control histone modification H3K4 MACS2 is going to use both files to normalize the read counts and perform the peak calling. Retrieve the BAM files with all chromosomes cd ~/chip-seq mkdir bams cd bams ln -s /scratch/users/aginolhac/chip-seq/bams/*.bam . Perform peak calling macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 & macs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3 In case macs2 gives command not found , your are certainly missing the module, please see the set-up check model inferred by MACS2 execute R script. Rscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r Rscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r fetch the pdf produced. sort per chromosomes and coordinates find TC* -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\" convert to bigwig in order to get smaller files find TC* -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\" Fetch the files and display them in IGV IGV can be downloaded from the broadinstitute. Perform peak calling with broad option macs2 callpeak -t TC1-H3K27-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-ST2-H3K27-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K27-D0-broad & macs2 callpeak -t TC1-H3K27-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-A-H3K27-D3-broad -B -q 0.01 --outdir TC1-A-H3K27-D3-broad Get the bigwig files for H3K27 . Redo those sort and conversion steps but only for the folders that end with 'broad' find TC*broad -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\" find TC*broad -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\" GREAT analysis The website GREAT allows pasting bed regions of enriched regions. predict functions of cis-regulatory regions Using the TC1-A-H3K4-D3_peaks.narrowPeak file produced by MACS2. This file has the different fields: chromosome start end peak name integer score for display strand fold-change -log 10 pvalue -log 10 qvalue relative summit position to peak start Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on q-values . awk '$9>40' TC1-A-H3K4-D3/TC1-A-H3K4-D3_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K4-D3/TC1-A-H3K4_peaks.bed awk '$9>40' TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.bed For H3K27: cat TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broadPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broad.bed then load the BED in GREAT for the relevant genome, mm10 association rule: Single nearest gene for H3K4 Two nearest genes for H3K27 alternative with ngsplot Example of ngsplot where gene expression ranked the genes from top to bottom and ChIP-seq of H3K4 is mapped with the red density on top. Differential peak calling THOR allows comparing two conditions associated with their own controls and with replicates. first, index the bams parallel \"samtools index {}\" ::: *bam second, create a config file THOR.config that contains: #rep1 TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam #rep2 TC1-H3K4-A-D3.GRCm38.p3.q30.bam #chrom_sizes /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes #genome /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.fasta #inputs1 TC1-I-ST2-D0.GRCm38.p3.q30.bam #inputs2 TC1-I-A-D3.GRCm38.p3.q30.bam A command line looks like rgt-THOR -m -n TC1-I-A-D0vsD3 --output-dir=TC1-I-A-D0vsD3 THOR.config takes ~ 25 minutes visualization load the file TC1-I-A-D0vsD3-diffpeaks.bed and the bigwig files ( .bw extension) color bigwig for D0 in red color bigwig for D3 in green select both bigwig and right-click to Overlay tracks the BED track should display in red the regions with higher enrichments in the D0, green in the D3. meta-analysis using GREAT You can play with the BED file in R with this code to extract the fold-change from counts. It is encoded in the 11th field of the narrowPeak file as counts for the first condition (D0-ST2) and counts for the second condition (D3-A) # load the file using the tidyverse library(readr) library(dplyr) library(ggplot2) library(tidyr) diffpeaks <- read_tsv(\"TC1-I-A-D0vsD3-diffpeaks.bed\", col_names = FALSE, trim_ws = TRUE, col_types = cols(X1 = col_character())) # split the last field into three diffpeaks %>% separate(X11, into = c(\"count1\", \"count2\", \"third\"), sep = \";\", convert = TRUE) %>% mutate(FC = count2 / count1) -> thor_splitted # plot the histogram of the fold-change computed above, count second condition / count 1st condition thor_splitted %>% ggplot(aes(x = log2(FC))) + geom_histogram() + scale_x_continuous(breaks = seq(-5, 3, 1)) # create a bed file, append chr to chromosome names and write down the file thor_splitted %>% filter(log2(FC) > 0.5) %>% select(X1, X2, X3) %>% mutate(X1 = paste0(\"chr\", X1)) %>% write_tsv(\"THOR_logFC0.5.bed\", col_names = FALSE) you can now import the file THOR_logFC0.5.bed into GREAT and see again how the meta-analysis looks like.","title":"Peak"},{"location":"peak/#peak-calling","text":"Using MACS2 For both the day 0 and day 3 of differentiation into adipocytes, two files are available input, as control histone modification H3K4 MACS2 is going to use both files to normalize the read counts and perform the peak calling.","title":"Peak calling"},{"location":"peak/#retrieve-the-bam-files-with-all-chromosomes","text":"cd ~/chip-seq mkdir bams cd bams ln -s /scratch/users/aginolhac/chip-seq/bams/*.bam .","title":"Retrieve the BAM files with all chromosomes"},{"location":"peak/#perform-peak-calling","text":"macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 & macs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3 In case macs2 gives command not found , your are certainly missing the module, please see the set-up","title":"Perform peak calling"},{"location":"peak/#check-model-inferred-by-macs2","text":"execute R script. Rscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r Rscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r fetch the pdf produced.","title":"check model inferred by MACS2"},{"location":"peak/#sort-per-chromosomes-and-coordinates","text":"find TC* -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\"","title":"sort per chromosomes and coordinates"},{"location":"peak/#convert-to-bigwig","text":"in order to get smaller files find TC* -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\"","title":"convert to bigwig"},{"location":"peak/#fetch-the-files-and-display-them-in-igv","text":"IGV can be downloaded from the broadinstitute.","title":"Fetch the files and display them in IGV"},{"location":"peak/#perform-peak-calling-with-broad-option","text":"macs2 callpeak -t TC1-H3K27-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-ST2-H3K27-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K27-D0-broad & macs2 callpeak -t TC1-H3K27-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-A-H3K27-D3-broad -B -q 0.01 --outdir TC1-A-H3K27-D3-broad Get the bigwig files for H3K27 . Redo those sort and conversion steps but only for the folders that end with 'broad' find TC*broad -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\" find TC*broad -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\"","title":"Perform peak calling with broad option"},{"location":"peak/#great-analysis","text":"The website GREAT allows pasting bed regions of enriched regions.","title":"GREAT analysis"},{"location":"peak/#predict-functions-of-cis-regulatory-regions","text":"Using the TC1-A-H3K4-D3_peaks.narrowPeak file produced by MACS2. This file has the different fields: chromosome start end peak name integer score for display strand fold-change -log 10 pvalue -log 10 qvalue relative summit position to peak start Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on q-values . awk '$9>40' TC1-A-H3K4-D3/TC1-A-H3K4-D3_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K4-D3/TC1-A-H3K4_peaks.bed awk '$9>40' TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.bed For H3K27: cat TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broadPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broad.bed then load the BED in GREAT for the relevant genome, mm10 association rule: Single nearest gene for H3K4 Two nearest genes for H3K27","title":"predict functions of cis-regulatory regions"},{"location":"peak/#alternative-with-ngsplot","text":"Example of ngsplot where gene expression ranked the genes from top to bottom and ChIP-seq of H3K4 is mapped with the red density on top.","title":"alternative with ngsplot"},{"location":"peak/#differential-peak-calling","text":"THOR allows comparing two conditions associated with their own controls and with replicates. first, index the bams parallel \"samtools index {}\" ::: *bam second, create a config file THOR.config that contains: #rep1 TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam #rep2 TC1-H3K4-A-D3.GRCm38.p3.q30.bam #chrom_sizes /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes #genome /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.fasta #inputs1 TC1-I-ST2-D0.GRCm38.p3.q30.bam #inputs2 TC1-I-A-D3.GRCm38.p3.q30.bam A command line looks like rgt-THOR -m -n TC1-I-A-D0vsD3 --output-dir=TC1-I-A-D0vsD3 THOR.config takes ~ 25 minutes","title":"Differential peak calling"},{"location":"peak/#visualization","text":"load the file TC1-I-A-D0vsD3-diffpeaks.bed and the bigwig files ( .bw extension) color bigwig for D0 in red color bigwig for D3 in green select both bigwig and right-click to Overlay tracks the BED track should display in red the regions with higher enrichments in the D0, green in the D3.","title":"visualization"},{"location":"peak/#meta-analysis-using-great","text":"You can play with the BED file in R with this code to extract the fold-change from counts. It is encoded in the 11th field of the narrowPeak file as counts for the first condition (D0-ST2) and counts for the second condition (D3-A) # load the file using the tidyverse library(readr) library(dplyr) library(ggplot2) library(tidyr) diffpeaks <- read_tsv(\"TC1-I-A-D0vsD3-diffpeaks.bed\", col_names = FALSE, trim_ws = TRUE, col_types = cols(X1 = col_character())) # split the last field into three diffpeaks %>% separate(X11, into = c(\"count1\", \"count2\", \"third\"), sep = \";\", convert = TRUE) %>% mutate(FC = count2 / count1) -> thor_splitted # plot the histogram of the fold-change computed above, count second condition / count 1st condition thor_splitted %>% ggplot(aes(x = log2(FC))) + geom_histogram() + scale_x_continuous(breaks = seq(-5, 3, 1)) # create a bed file, append chr to chromosome names and write down the file thor_splitted %>% filter(log2(FC) > 0.5) %>% select(X1, X2, X3) %>% mutate(X1 = paste0(\"chr\", X1)) %>% write_tsv(\"THOR_logFC0.5.bed\", col_names = FALSE) you can now import the file THOR_logFC0.5.bed into GREAT and see again how the meta-analysis looks like.","title":"meta-analysis using GREAT"},{"location":"snakemake/","text":"Snakemake The installation is from Sarah Peter , bioinformatician at the LCSB . Her tutorial is summarised here. Main differences are we don't use a virtualbox VM, nor conda environments. Install Miniconda Miniconda will provide you with a base conda environment. If you are a regular use of the HPC, you may want to remove it at the end of this tutorial. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod u+x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh Follow the instructions prompted (use spacebar to scrool down the license), of note you need to specify your installation destination, e.g. /home/users/username/miniconda3 . You must use the full path and cannot use $HOME/miniconda3 . Answer yes to initialize Miniconda3. For permission denied issues chmod +x miniconda3/bin/python3.9 chmod +x miniconda3/bin/conda miniconda3/bin/conda init Activate conda by reloading your BASH configuration source ~/.bashrc Notice that from now, you have an extra (base) written at the beginning of your prompt Finalize installation Update the permissions (base) chmod +x $(which conda) (base) chmod +x $(which conda-env) Update conda (base) conda update conda Install mamba as recommended by Johannes K\u00f6ster ( snakemake author) (base) conda install -c conda-forge mamba Ensure enclosed environments conda may use python modules if already installed be default. To avoid this behaviour, you need to add this line in your .bashrc . You need to edit it like with vim ~/.bashrc . If you are not comfortable with editing files, see this page export PYTHONNOUSERSITE=True Install snakemake in a dedicated environment It is also recommended to leave the base environment as clean as possible, Create a new conda environment and activate it: (base) conda create -n snakemake (base) conda activate snakemake Now the prompt becomes (snakemake) and we can install snakemake inside it. This step takes 2-3 minutes and is the longest one of this chapter. (snakemake) mamba install -c conda-forge -c bioconda snakemake Check that the snakemake is now installed (snakemake) snakemake --version Should return 6.7.0 (Optional) Add useful aliases The following lines can be added to your .bashrc . The 3 first ones are handy shortcuts: alias dag='snakemake --dag | dot -Tpdf > dag.pdf' alias smk='conda activate snakemake && module load tools/Singularity' complete -o bashdefault -C snakemake-bash-completion snakemake dag is often run to see which steps are to be re-run or not smk to load the necessary tools on ULHPC in interactive sessions. complete command loads the auto-completion for snakemake Again, your need source the .bashrc to get those lines in the current session. source ~/.bashrc (Optional) Revert the changes to your environment From Sarah Peter , if you want to stop conda from always being active: (base) conda init --reverse In case you want to get rid of conda completely, you can now also delete the directory where you installed it (default is $HOME/miniconda3 ).","title":"Setup snakemake"},{"location":"snakemake/#snakemake","text":"The installation is from Sarah Peter , bioinformatician at the LCSB . Her tutorial is summarised here. Main differences are we don't use a virtualbox VM, nor conda environments.","title":"Snakemake"},{"location":"snakemake/#install-miniconda","text":"Miniconda will provide you with a base conda environment. If you are a regular use of the HPC, you may want to remove it at the end of this tutorial. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod u+x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh Follow the instructions prompted (use spacebar to scrool down the license), of note you need to specify your installation destination, e.g. /home/users/username/miniconda3 . You must use the full path and cannot use $HOME/miniconda3 . Answer yes to initialize Miniconda3. For permission denied issues chmod +x miniconda3/bin/python3.9 chmod +x miniconda3/bin/conda miniconda3/bin/conda init Activate conda by reloading your BASH configuration source ~/.bashrc Notice that from now, you have an extra (base) written at the beginning of your prompt","title":"Install Miniconda"},{"location":"snakemake/#finalize-installation","text":"Update the permissions (base) chmod +x $(which conda) (base) chmod +x $(which conda-env) Update conda (base) conda update conda Install mamba as recommended by Johannes K\u00f6ster ( snakemake author) (base) conda install -c conda-forge mamba Ensure enclosed environments conda may use python modules if already installed be default. To avoid this behaviour, you need to add this line in your .bashrc . You need to edit it like with vim ~/.bashrc . If you are not comfortable with editing files, see this page export PYTHONNOUSERSITE=True","title":"Finalize installation"},{"location":"snakemake/#install-snakemake-in-a-dedicated-environment","text":"It is also recommended to leave the base environment as clean as possible, Create a new conda environment and activate it: (base) conda create -n snakemake (base) conda activate snakemake Now the prompt becomes (snakemake) and we can install snakemake inside it. This step takes 2-3 minutes and is the longest one of this chapter. (snakemake) mamba install -c conda-forge -c bioconda snakemake Check that the snakemake is now installed (snakemake) snakemake --version Should return 6.7.0","title":"Install snakemake in a dedicated environment"},{"location":"snakemake/#optional-add-useful-aliases","text":"The following lines can be added to your .bashrc . The 3 first ones are handy shortcuts: alias dag='snakemake --dag | dot -Tpdf > dag.pdf' alias smk='conda activate snakemake && module load tools/Singularity' complete -o bashdefault -C snakemake-bash-completion snakemake dag is often run to see which steps are to be re-run or not smk to load the necessary tools on ULHPC in interactive sessions. complete command loads the auto-completion for snakemake Again, your need source the .bashrc to get those lines in the current session. source ~/.bashrc","title":"(Optional) Add useful aliases"},{"location":"snakemake/#optional-revert-the-changes-to-your-environment","text":"From Sarah Peter , if you want to stop conda from always being active: (base) conda init --reverse In case you want to get rid of conda completely, you can now also delete the directory where you installed it (default is $HOME/miniconda3 ).","title":"(Optional) Revert the changes to your environment"}]}