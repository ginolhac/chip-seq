{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ChIP-seq practical session Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance clusters (HPC). Lecture slides are available as a pdf, click below Log in iris iris is one of the High Performance Computer (HPC) of the UNI . You should use your account or one the student account prepared for you. Connect to the frontend To connect to it, you need an account and an authorized ssh key. Actually, a pair of keys, one public and one private. The public key is sent over when connecting to the remote and compared to the authorized private key. A match allows the sender to log in. No password required. After the setting up of your account, the following should work if you are using mac or GNU/Linux: ssh iris-cluster Otherwise, on Windows, use xmobaterm . In the terminal, log as your username, such as student15 . You should see the following prompt of the iris frontend: ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) username@access1.iris-cluster.uni.lux(11:30:46): ~ $ Note that you are on the access frontend. The frontend is meant for browsing / transferring your files only and you MUST connect to a node for any computational work using the utility slurm described here . This program managed the queuing system and dispatch jobs among the resources according to the demands. Softwares are organized into modules that provide you with the binaries but also all the environment required for their running processes. However, we will use a container that will ease our analyses. The login procedure can be depicted as: TMUX log in to a remote computer is great, all computation, heat generation is happening elsewhere but this comes with a price: disconnection. This happens all the time. The way to get around it is to have a screen system that stores your terminal, commands, environment in which you can easily detach and re-attach. Two systems exist, screen and tmux . Both work well, but tmux has a nicer interface IMHO. a short tutorial is accessible here. Briefly, on the access frontend, start a tmux instance with tmux to detach (press CTRL and B together, release then use the next key): CTRL + B, then D to re-attach: tmux attach or the alias tmux at Useful commands once in an instance, create a new tab CTRL + B, then C move to the next tab CTRL + B, then N move to the previous tab CTRL + B, then P rename to the current tab CTRL + B, then , then type the new name Quit exit in all tabs kills the tmux session Of note, tmux instances live until the frontend is rebooted. Book resources on a computing node Connecting to a computing node is required to use resources. You need to book resources by specifying how many cores, optionally if they are on a same node, the memory required, and a wall time clock. A job can never get extended. Without entering into the details of submitting a job . The less you ask for, the more high up you are in the queue. Here is the explanation for the above command: srun is for interactive, sbatch for passive --time= following by hour:minute:second for wall time clock --mem= with 12GB for booking 12 gigabytes -c cores, A node is usually composed of 28 cores Once logged in, the prompt changes for: username@iris-001(11:17:55) where you see the node you are logged to (here iris-001 ). Monitoring the resources used On a shared cluster, you have to take care of three things: memory usage cores used disk space Memory Each node has On an interactive session, use the command htop to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled. For passive jobs, you can join a computing node by using the sjoin nodeid jobid command. Where nodeid and jobid can be autocomplemented by TAB Cores even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly. On an interactive session, use the command htop to see if a process is correctly using close to 100% of a core. Disk space Like on your local machine, you need to check how much data you used. Using a command line, you could use Disk usage du -sh ~ to display your disk usage ( du ) for your home folder ( ~ ). In a form readable by human ( -h ) Disk free df-ulphc disk free scans all disks mounted. Could takes time to display the global usage. Please worry if only few Mb are available on the disk you are planning to write to. check also your own quota with df-ulhpc on the frontend. Closing connection When you are done, you can kill yourself your job by either doing CTRL + D or typing exit . That will free your booked resources for others. Once done, you will still log on the frontend and normally inside a tmux . The best is to detach from the tmux instance and log off from the gaia frontend using CTRL + D or typing exit .","title":"Home"},{"location":"#chip-seq-practical-session","text":"Running all analyses is computationally intensive and despite the power of the current laptops, jobs should be run on high-performance clusters (HPC).","title":"ChIP-seq practical session"},{"location":"#lecture","text":"slides are available as a pdf, click below","title":"Lecture"},{"location":"#log-in-iris","text":"iris is one of the High Performance Computer (HPC) of the UNI . You should use your account or one the student account prepared for you.","title":"Log in iris"},{"location":"#connect-to-the-frontend","text":"To connect to it, you need an account and an authorized ssh key. Actually, a pair of keys, one public and one private. The public key is sent over when connecting to the remote and compared to the authorized private key. A match allows the sender to log in. No password required. After the setting up of your account, the following should work if you are using mac or GNU/Linux: ssh iris-cluster Otherwise, on Windows, use xmobaterm . In the terminal, log as your username, such as student15 . You should see the following prompt of the iris frontend: ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) username@access1.iris-cluster.uni.lux(11:30:46): ~ $ Note that you are on the access frontend. The frontend is meant for browsing / transferring your files only and you MUST connect to a node for any computational work using the utility slurm described here . This program managed the queuing system and dispatch jobs among the resources according to the demands. Softwares are organized into modules that provide you with the binaries but also all the environment required for their running processes. However, we will use a container that will ease our analyses. The login procedure can be depicted as:","title":"Connect to the frontend"},{"location":"#tmux","text":"log in to a remote computer is great, all computation, heat generation is happening elsewhere but this comes with a price: disconnection. This happens all the time. The way to get around it is to have a screen system that stores your terminal, commands, environment in which you can easily detach and re-attach. Two systems exist, screen and tmux . Both work well, but tmux has a nicer interface IMHO. a short tutorial is accessible here. Briefly, on the access frontend, start a tmux instance with tmux to detach (press CTRL and B together, release then use the next key): CTRL + B, then D to re-attach: tmux attach or the alias tmux at","title":"TMUX"},{"location":"#useful-commands","text":"once in an instance, create a new tab CTRL + B, then C move to the next tab CTRL + B, then N move to the previous tab CTRL + B, then P rename to the current tab CTRL + B, then , then type the new name","title":"Useful commands"},{"location":"#quit","text":"exit in all tabs kills the tmux session Of note, tmux instances live until the frontend is rebooted.","title":"Quit"},{"location":"#book-resources-on-a-computing-node","text":"Connecting to a computing node is required to use resources. You need to book resources by specifying how many cores, optionally if they are on a same node, the memory required, and a wall time clock. A job can never get extended. Without entering into the details of submitting a job . The less you ask for, the more high up you are in the queue. Here is the explanation for the above command: srun is for interactive, sbatch for passive --time= following by hour:minute:second for wall time clock --mem= with 12GB for booking 12 gigabytes -c cores, A node is usually composed of 28 cores Once logged in, the prompt changes for: username@iris-001(11:17:55) where you see the node you are logged to (here iris-001 ).","title":"Book resources on a computing node"},{"location":"#monitoring-the-resources-used","text":"On a shared cluster, you have to take care of three things: memory usage cores used disk space","title":"Monitoring the resources used"},{"location":"#memory","text":"Each node has On an interactive session, use the command htop to see if the memory is not full. If the system is swapping (using hard drives for memory storage) it becomes super slow and eventually stalled. For passive jobs, you can join a computing node by using the sjoin nodeid jobid command. Where nodeid and jobid can be autocomplemented by TAB","title":"Memory"},{"location":"#cores","text":"even if you book 10 cores, nothing will prevent you from starting 100 jobs. They will run but then tasks are distributed on the available resources. In this example, each task will use 1/10th of a core, then runs very slowly. On an interactive session, use the command htop to see if a process is correctly using close to 100% of a core.","title":"Cores"},{"location":"#disk-space","text":"Like on your local machine, you need to check how much data you used. Using a command line, you could use","title":"Disk space"},{"location":"#disk-usage","text":"du -sh ~ to display your disk usage ( du ) for your home folder ( ~ ). In a form readable by human ( -h )","title":"Disk usage"},{"location":"#disk-free","text":"df-ulphc disk free scans all disks mounted. Could takes time to display the global usage. Please worry if only few Mb are available on the disk you are planning to write to. check also your own quota with df-ulhpc on the frontend.","title":"Disk free"},{"location":"#closing-connection","text":"When you are done, you can kill yourself your job by either doing CTRL + D or typing exit . That will free your booked resources for others. Once done, you will still log on the frontend and normally inside a tmux . The best is to detach from the tmux instance and log off from the gaia frontend using CTRL + D or typing exit .","title":"Closing connection"},{"location":"cli/","text":"Command line The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac. You need to know these commands: pwd more less cp mv mkdir ls cd chmod rm find Two useful tips: use TAB on your keyboard for command and name completion. the up arrow allows to browse your history (also available with history ) Exercise 1 go to your home folder cd create a fake file by using echo \"hello world\" > filetest see if this file is present ls -l read it more fileTest Of note, less is an alternative to more rename it mv fileTest test check ls -l create a folder mkdir TEST Of note, all commands are case-sensitive ll ll is a classic alias for ls -l move the file in this folder mv test TEST check, and see if present in the folder ll TEST copy it in the current folder cp TEST/test . . is the current folder, .. is the folder one level close to the root / now we have the same file, with the same name, one in the TEST folder, one in the current. use the up arrow, you should see 'cp TEST/test .' and change it for cp TEST/test test2 the first and last field of ls -l should provide drwxr-xr-x TEST -rw-r--r-- test -rw-r--r-- test2 trash test rm test if this command doesn't ask for confirmation, let me know we may change this behavior. chmod allows changing permissions try to read the file test after chmod 222 test r stands for read, w for write and x for execution for files and browsing for folders. the first pattern is the owner the second pattern is for the group the third pattern is for everyone else TODO more details needed Text editor nano often installed, it is easy to use as all commands are written at the bottom: ^G Get Help ^O WriteOut ^R Read File ^Y Prev Page ^K Cut Text ^C Cur Pos ^X Exit ^J Justify ^W Where Is ^V Next Page ^U UnCut Text ^T To Spell you can write/modify text directly and use CTRL + O to save then CTRL + X to exit. VIM Let's have a look at a text editor, there is plenty of them, the one I use is vim , why? Because: it's commonly installed on servers extremely powerful enter the editor vim test2 you have two modes command insert By default you are in the command mode, let's enter in the editor mode with either i or insert on your keyboard. You should see --INSERT-- at the bottom. Now you can edit your file. When its finished, press ECHAP to return in the command mode. You must enter : for each command. The useful ones w save changes to the file :q! quit without saving changes :wq write and quit","title":"Command line, basics"},{"location":"cli/#command-line","text":"The programs you call on a terminal are not so different from their graphical interface you are used to on windows/mac. You need to know these commands: pwd more less cp mv mkdir ls cd chmod rm find","title":"Command line"},{"location":"cli/#two-useful-tips","text":"use TAB on your keyboard for command and name completion. the up arrow allows to browse your history (also available with history )","title":"Two useful tips:"},{"location":"cli/#exercise-1","text":"go to your home folder cd create a fake file by using echo \"hello world\" > filetest see if this file is present ls -l read it more fileTest Of note, less is an alternative to more rename it mv fileTest test check ls -l create a folder mkdir TEST Of note, all commands are case-sensitive ll ll is a classic alias for ls -l move the file in this folder mv test TEST check, and see if present in the folder ll TEST copy it in the current folder cp TEST/test . . is the current folder, .. is the folder one level close to the root / now we have the same file, with the same name, one in the TEST folder, one in the current. use the up arrow, you should see 'cp TEST/test .' and change it for cp TEST/test test2 the first and last field of ls -l should provide drwxr-xr-x TEST -rw-r--r-- test -rw-r--r-- test2 trash test rm test if this command doesn't ask for confirmation, let me know we may change this behavior. chmod allows changing permissions try to read the file test after chmod 222 test r stands for read, w for write and x for execution for files and browsing for folders. the first pattern is the owner the second pattern is for the group the third pattern is for everyone else TODO more details needed","title":"Exercise 1"},{"location":"cli/#text-editor","text":"","title":"Text editor"},{"location":"cli/#nano","text":"often installed, it is easy to use as all commands are written at the bottom: ^G Get Help ^O WriteOut ^R Read File ^Y Prev Page ^K Cut Text ^C Cur Pos ^X Exit ^J Justify ^W Where Is ^V Next Page ^U UnCut Text ^T To Spell you can write/modify text directly and use CTRL + O to save then CTRL + X to exit.","title":"nano"},{"location":"cli/#vim","text":"Let's have a look at a text editor, there is plenty of them, the one I use is vim , why? Because: it's commonly installed on servers extremely powerful enter the editor vim test2 you have two modes command insert By default you are in the command mode, let's enter in the editor mode with either i or insert on your keyboard. You should see --INSERT-- at the bottom. Now you can edit your file. When its finished, press ECHAP to return in the command mode. You must enter : for each command. The useful ones w save changes to the file :q! quit without saving changes :wq write and quit","title":"VIM"},{"location":"contact/","text":"Aurelien Ginolhac aurelien.ginolhac@uni.lu -- University of Luxembourg Faculty of Science, Technology and Communication Departement of Life Science and Medecine Campus Belval, Building KTT \u2013 Office 403 6, Avenue du Swing L-4367 Belvaux","title":"Contact"},{"location":"install/","text":"Workflow The workflow of all steps is summarised below: Singularity container Singularity allows to use containers (from i.e Docker ) on High-Performance Computer. For more details see the lecture by HPC team Shortly, we built a container with all the necessary tools and softwares embeded. Hence, you need to book the HPC resources. Snakemake will load the container for every chip-seq sequences. book resources on iris 2 hours 8 cores interactive srun --cpu-bind=none -p interactive --time=2:0:0 -c 8 --pty bash -i load the container first we load the tools singularity second we load the container module load tools/Singularity singularity shell -s /bin/bash --bind /scratch/users:/scratch/users /scratch/users/aginolhac/ubuntu-chip-seq.simg prepare your working environment go to your home directory: cd create a new folder to work in: mkdir chip-seq go inside: cd chip-seq create and go in a sub-folder: mkdir fastq go inside: cd fastq symbolic link the FASTQ files: ln -s /scratch/users/aginolhac/chip-seq/fastq/C* . check your actions: ll (alias of ls -l ) check integrity of files Just as a side note, such large files are usually a pain to download. Since they are the very raw files after the sequencer (despite basecalling) checking their integrity is worth doing. Computing the md5sum ensure you have the same file as your sequence provider. Then paleomix will check the FASTQ are correct, i. e have 4 lines in a correct format. md5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5 you should observe an OK after few seconds of computing time.","title":"Install"},{"location":"install/#workflow","text":"The workflow of all steps is summarised below:","title":"Workflow"},{"location":"install/#singularity-container","text":"Singularity allows to use containers (from i.e Docker ) on High-Performance Computer. For more details see the lecture by HPC team Shortly, we built a container with all the necessary tools and softwares embeded. Hence, you need to book the HPC resources. Snakemake will load the container for every chip-seq sequences.","title":"Singularity container"},{"location":"install/#book-resources-on-iris","text":"2 hours 8 cores interactive srun --cpu-bind=none -p interactive --time=2:0:0 -c 8 --pty bash -i","title":"book resources on iris"},{"location":"install/#load-the-container","text":"first we load the tools singularity second we load the container module load tools/Singularity singularity shell -s /bin/bash --bind /scratch/users:/scratch/users /scratch/users/aginolhac/ubuntu-chip-seq.simg","title":"load the container"},{"location":"install/#prepare-your-working-environment","text":"go to your home directory: cd create a new folder to work in: mkdir chip-seq go inside: cd chip-seq create and go in a sub-folder: mkdir fastq go inside: cd fastq symbolic link the FASTQ files: ln -s /scratch/users/aginolhac/chip-seq/fastq/C* . check your actions: ll (alias of ls -l )","title":"prepare your working environment"},{"location":"install/#check-integrity-of-files","text":"Just as a side note, such large files are usually a pain to download. Since they are the very raw files after the sequencer (despite basecalling) checking their integrity is worth doing. Computing the md5sum ensure you have the same file as your sequence provider. Then paleomix will check the FASTQ are correct, i. e have 4 lines in a correct format. md5sum -c C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.md5 you should observe an OK after few seconds of computing time.","title":"check integrity of files"},{"location":"mapping/","text":"paleomix, Next-Generation Sequencing wrapper this framework is open-source and available on GitHub and wraps all steps from fastq to bam files. Actually, this tool can do much more but the rest is out of scope. Its major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default. Actually, not entirely true since independent tasks can be spawn on a separate machine. Full documentation available here check if paleomix is available paleomix In case it is not, your are certainly not using the singularity container , see instructions in the set-up test your install fetch the example, reference is the human mitochondrial genome mkdir -p ~/paleomix/example cp -r /scratch/users/aginolhac/chip-seq/bam_pipeline_example/000* ~/paleomix/example cd ~/paleomix/example run the example, start by a dry-run , adjust the number of threads accordingly. paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=8 --dry-run 000_makefile.yaml If all fine, re-rerun the command without the --dry-run option Of note, calling mapDamage and GATK were disabled to limit the computation time (~ 35 min when included). Anyway, this tool is not used for ChIP-seq analysis. Generate a makefile Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folders, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes through a YAML makefile. The syntax is pretty straight-forward. What matters is, that you use SPACES and not TABS. Create a generic makefile (extension, yml or yaml to get syntax highlights) cd ~/chip-seq paleomix bam_pipeline mkfile > mouse.yaml Edit the makefile using your favorite text editor (such as nano ), edit the mouse.yaml . For example vim mouse.yaml or kate or nano . Options for the compression, change the default behavior from bz2 to gz CompressionFormat: gz for duplicates, change the default behavior from filter to mark PCRDuplicates: mark Features Under the Features section, update the featurse that need to be performed. Change yes/no to match the following: Features: RawBAM: yes # Generate BAM from the raw libraries (no indel realignment) # Location: {Destination}/{Target}.{Genome}.bam RealignedBAM: no # Generate indel-realigned BAM using the GATK Indel realigner # Location: {Destination}/{Target}.{Genome}.realigned.bam mapDamage: no # Generate mapDamage plot for each (unrealigned) library # Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/ Coverage: yes # Generate coverage information for the raw BAM (wo/ indel realignment) # Location: {Destination}/{Target}.{Genome}.coverage Depths: no # Generate histogram of number of sites with a given read-depth # Location: {Destination}/{Target}.{Genome}.depths Summary: yes # Generate summary table for each target # Location: {Destination}/{Target}.summary DuplicateHist: no # Generate histogram of PCR duplicates, for use with PreSeq # Location: {Destination}/{Target}.{Genome}.duphist/{Library}/ In detail, the RealignedBAM are important for calling variants, we only need to RawBAM . Moreover, the Depths also help to define which upper limit could be used for variant calling. This is not in the scope of ChIP-seq analysis. Same for mapDamage , only relevant for ancient DNA. Prefixes These are the references to align read to. You could notice that we are going to use only one chromosome to save computational time. Prefixes: mouse_19: Path: /scratch/users/aginolhac/chip-seq/references/chr19.fasta Samples enter at the end of the makefile, the following lines. Again, do use spaces and not tabs for the indentation. For those who are lazy and use copy/paste in vim use the trick to :set paste to avoid extra spaces, comment hashes etc to be automatically added. The descriptions of the different hierachical names can be read here TC1-I-A-D3: TC1-I-A-D3: TC1-I-A-D3: \"14s006680-1-1\": fastq/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz TC1-H3K4-A-D3: TC1-H3K4-A-D3: TC1-H3K4-A-D3: \"14s006647-1-1\": fastq/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz TC1-I-ST2-D0: TC1-I-ST2-D0: TC1-I-ST2-D0: \"14s006677-1-1\": fastq/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: \"14s006644\": fastq/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz Perform the trimming / mapping First use the option --dry-run to spot mistakes. Please adapt the --max-threads option to the #cpus actually booked paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 --dry-run mouse.yaml when all green lights are on, remove the dry-run and perform the mapping. correct the makefile the trimming should go well, but an error arises because you cannot write to the reference folder. symbolic links in your own folder, removing the file that should be created if the ref is correct mkdir references ln -s /scratch/users/aginolhac/chip-seq/references/chr19.fasta references/ correct the makefile, so the Path is relative now Path: references/chr19.fasta enjoy paleomix by just re-running it, only necessary steps are done (validatating the ref, indexing it and mappings) paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 mouse.yaml the whole process should takes ~ 25 minutes with 8 cores check trimming First of all, check using fastqc that the trimming did remove the adapters that were contaminated the reads. Again, with parallel specify the max number of jobs with the option -j to fit the #cpus booked find . -name \"reads.truncated.gz\" | parallel -j 8 \"fastqc {}\" & using the character & tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using htop . check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters? truncated read files are all named the same. And they are located into a deeper folder structure. To rename them all with their ids and move them at the ~/chip-seq/ level, you can run the following: find -name \"reads.truncated.gz\" | xargs ls -1 | awk '{split($1, path, \"/\"); system(\"mv \"$0 \" \"path[3] \"_\" path[6])}' filter for unique reads Uniqueness of reads refers to mappability. The fewer locations a read has in a genome, the higher is mappability will be. A common filter is to use 30 as a threshold for filtering reads. Filter them in parallel parallel \"samtools view -b -q 30 {} > {.}.q30.bam\" ::: *.bam Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why? filter for duplicates? Duplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage. Duplicates have an unclear definition in a mapped file. Usually, single-end reads that are mapped at the same 5' end are considered as duplicates. External coordinates are used for paired-end reads. For regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are, by nature, clustered at one location this is not recommended. If duplication is observed at the reads level, such as in fastqc output, then filtering may be necessary. Marking duplicates allow keeping track of them without losing them.","title":"Mapping"},{"location":"mapping/#paleomix-next-generation-sequencing-wrapper","text":"this framework is open-source and available on GitHub and wraps all steps from fastq to bam files. Actually, this tool can do much more but the rest is out of scope. Its major drawback is that it is dedicated to one machine. For clusters, you are then limited to one node since memory are not shared by default. Actually, not entirely true since independent tasks can be spawn on a separate machine. Full documentation available here check if paleomix is available paleomix In case it is not, your are certainly not using the singularity container , see instructions in the set-up","title":"paleomix, Next-Generation Sequencing wrapper"},{"location":"mapping/#test-your-install","text":"fetch the example, reference is the human mitochondrial genome mkdir -p ~/paleomix/example cp -r /scratch/users/aginolhac/chip-seq/bam_pipeline_example/000* ~/paleomix/example cd ~/paleomix/example run the example, start by a dry-run , adjust the number of threads accordingly. paleomix bam_pipeline run --bwa-max-threads=1 --max-threads=8 --dry-run 000_makefile.yaml If all fine, re-rerun the command without the --dry-run option Of note, calling mapDamage and GATK were disabled to limit the computation time (~ 35 min when included). Anyway, this tool is not used for ChIP-seq analysis.","title":"test your install"},{"location":"mapping/#generate-a-makefile","text":"Trimming, mapping imply a lot of steps and it is hard to be sure that everything goes well. Paleomix works in temporary folders, check the data produced and then copy back files that are complete. Plus, you want to test different parameters, add a new reference without having to redo earlier steps while being sure that all files are up-to-date. This goes through a YAML makefile. The syntax is pretty straight-forward. What matters is, that you use SPACES and not TABS. Create a generic makefile (extension, yml or yaml to get syntax highlights) cd ~/chip-seq paleomix bam_pipeline mkfile > mouse.yaml","title":"Generate a makefile"},{"location":"mapping/#edit-the-makefile","text":"using your favorite text editor (such as nano ), edit the mouse.yaml . For example vim mouse.yaml or kate or nano .","title":"Edit the makefile"},{"location":"mapping/#options","text":"for the compression, change the default behavior from bz2 to gz CompressionFormat: gz for duplicates, change the default behavior from filter to mark PCRDuplicates: mark","title":"Options"},{"location":"mapping/#features","text":"Under the Features section, update the featurse that need to be performed. Change yes/no to match the following: Features: RawBAM: yes # Generate BAM from the raw libraries (no indel realignment) # Location: {Destination}/{Target}.{Genome}.bam RealignedBAM: no # Generate indel-realigned BAM using the GATK Indel realigner # Location: {Destination}/{Target}.{Genome}.realigned.bam mapDamage: no # Generate mapDamage plot for each (unrealigned) library # Location: {Destination}/{Target}.{Genome}.mapDamage/{Library}/ Coverage: yes # Generate coverage information for the raw BAM (wo/ indel realignment) # Location: {Destination}/{Target}.{Genome}.coverage Depths: no # Generate histogram of number of sites with a given read-depth # Location: {Destination}/{Target}.{Genome}.depths Summary: yes # Generate summary table for each target # Location: {Destination}/{Target}.summary DuplicateHist: no # Generate histogram of PCR duplicates, for use with PreSeq # Location: {Destination}/{Target}.{Genome}.duphist/{Library}/ In detail, the RealignedBAM are important for calling variants, we only need to RawBAM . Moreover, the Depths also help to define which upper limit could be used for variant calling. This is not in the scope of ChIP-seq analysis. Same for mapDamage , only relevant for ancient DNA.","title":"Features"},{"location":"mapping/#prefixes","text":"These are the references to align read to. You could notice that we are going to use only one chromosome to save computational time. Prefixes: mouse_19: Path: /scratch/users/aginolhac/chip-seq/references/chr19.fasta","title":"Prefixes"},{"location":"mapping/#samples","text":"enter at the end of the makefile, the following lines. Again, do use spaces and not tabs for the indentation. For those who are lazy and use copy/paste in vim use the trick to :set paste to avoid extra spaces, comment hashes etc to be automatically added. The descriptions of the different hierachical names can be read here TC1-I-A-D3: TC1-I-A-D3: TC1-I-A-D3: \"14s006680-1-1\": fastq/C53CYACXX_TC1-I-A-D3_14s006682-1-1_Sinkkonen_lane114s006682_sequence.txt.gz TC1-H3K4-A-D3: TC1-H3K4-A-D3: TC1-H3K4-A-D3: \"14s006647-1-1\": fastq/C51C3ACXX_TC1-H3K4-A-D3_14s006647-1-1_Sinkkonen_lane514s006647_sequence.txt.gz TC1-I-ST2-D0: TC1-I-ST2-D0: TC1-I-ST2-D0: \"14s006677-1-1\": fastq/C51C3ACXX_TC1-I-ST2-D0_14s006677-1-1_Sinkkonen_lane814s006677_sequence.txt.gz TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: TC1-H3K4-ST2-D0: \"14s006644\": fastq/C51C3ACXX_TC1-H3K4-ST2-D0_14s006644-1-1_Sinkkonen_lane514s006644_sequence.txt.gz","title":"Samples"},{"location":"mapping/#perform-the-trimming-mapping","text":"First use the option --dry-run to spot mistakes. Please adapt the --max-threads option to the #cpus actually booked paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 --dry-run mouse.yaml when all green lights are on, remove the dry-run and perform the mapping.","title":"Perform the trimming / mapping"},{"location":"mapping/#correct-the-makefile","text":"the trimming should go well, but an error arises because you cannot write to the reference folder. symbolic links in your own folder, removing the file that should be created if the ref is correct mkdir references ln -s /scratch/users/aginolhac/chip-seq/references/chr19.fasta references/ correct the makefile, so the Path is relative now Path: references/chr19.fasta enjoy paleomix by just re-running it, only necessary steps are done (validatating the ref, indexing it and mappings) paleomix bam_pipeline run --bwa-max-threads=2 --adapterremoval-max-threads=2 --max-threads=8 mouse.yaml the whole process should takes ~ 25 minutes with 8 cores","title":"correct the makefile"},{"location":"mapping/#check-trimming","text":"First of all, check using fastqc that the trimming did remove the adapters that were contaminated the reads. Again, with parallel specify the max number of jobs with the option -j to fit the #cpus booked find . -name \"reads.truncated.gz\" | parallel -j 8 \"fastqc {}\" & using the character & tells the shell that we want the processes to run in the background. Meaning that you can still run more things while the 4 tasks are running. Check them using htop . check especially, the input for ST2, day0 before and after trimming. Did it solve the issue with adapters? truncated read files are all named the same. And they are located into a deeper folder structure. To rename them all with their ids and move them at the ~/chip-seq/ level, you can run the following: find -name \"reads.truncated.gz\" | xargs ls -1 | awk '{split($1, path, \"/\"); system(\"mv \"$0 \" \"path[3] \"_\" path[6])}'","title":"check trimming"},{"location":"mapping/#filter-for-unique-reads","text":"Uniqueness of reads refers to mappability. The fewer locations a read has in a genome, the higher is mappability will be. A common filter is to use 30 as a threshold for filtering reads. Filter them in parallel parallel \"samtools view -b -q 30 {} > {.}.q30.bam\" ::: *.bam Since we are using only the chr19 for this tutorial, do you think the mappability score is correct? Why?","title":"filter for unique reads"},{"location":"mapping/#filter-for-duplicates","text":"Duplication is a bias that comes from PCR amplification. Reads then stack at the same location and create artificial high depth of coverage. Duplicates have an unclear definition in a mapped file. Usually, single-end reads that are mapped at the same 5' end are considered as duplicates. External coordinates are used for paired-end reads. For regular NGS, filtering for duplicates is mandatory. However, for ChIP-seq since the reads are, by nature, clustered at one location this is not recommended. If duplication is observed at the reads level, such as in fastqc output, then filtering may be necessary. Marking duplicates allow keeping track of them without losing them.","title":"filter for duplicates?"},{"location":"peak/","text":"Peak calling Using MACS2 For both the day 0 and day 3 of differentiation into adipocytes, two files are available input, as control histone modification H3K4 MACS2 is going to use both files to normalize the read counts and perform the peak calling. Retrieve the BAM files with all chromosomes cd ~/chip-seq mkdir bams cd bams ln -s /scratch/users/aginolhac/chip-seq/bams/*.bam . Perform peak calling macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 & macs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3 In case macs2 gives command not found , your are certainly missing the module, please see the set-up check model inferred by MACS2 execute R script. Rscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r Rscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r fetch the pdf produced. sort per chromosomes and coordinates find TC* -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\" convert to bigwig in order to get smaller files find TC* -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\" Fetch the files and display them in IGV IGV can be downloaded from the broadinstitute. Perform peak calling with broad option macs2 callpeak -t TC1-H3K27-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-ST2-H3K27-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K27-D0-broad & macs2 callpeak -t TC1-H3K27-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-A-H3K27-D3-broad -B -q 0.01 --outdir TC1-A-H3K27-D3-broad Get the bigwig files for H3K27 . Redo those sort and conversion steps but only for the folders that end with 'broad' find TC*broad -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\" find TC*broad -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\" GREAT analysis The website GREAT allows pasting bed regions of enriched regions. predict functions of cis-regulatory regions Using the TC1-A-H3K4-D3_peaks.narrowPeak file produced by MACS2. This file has the different fields: chromosome start end peak name integer score for display strand fold-change -log 10 pvalue -log 10 qvalue relative summit position to peak start Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on q-values . awk '$9>40' TC1-A-H3K4-D3/TC1-A-H3K4-D3_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K4-D3/TC1-A-H3K4_peaks.bed awk '$9>40' TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.bed For H3K27: cat TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broadPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broad.bed then load the BED in GREAT for the relevant genome, mm10 association rule: Single nearest gene for H3K4 Two nearest genes for H3K27 alternative with ngsplot Example of ngsplot where gene expression ranked the genes from top to bottom and ChIP-seq of H3K4 is mapped with the red density on top. Differential peak calling THOR allows comparing two conditions associated with their own controls and with replicates. first, index the bams parallel \"samtools index {}\" ::: *bam second, create a config file THOR.config that contains: #rep1 TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam #rep2 TC1-H3K4-A-D3.GRCm38.p3.q30.bam #chrom_sizes /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes #genome /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.fasta #inputs1 TC1-I-ST2-D0.GRCm38.p3.q30.bam #inputs2 TC1-I-A-D3.GRCm38.p3.q30.bam A command line looks like rgt-THOR -m -n TC1-I-A-D0vsD3 --output-dir=TC1-I-A-D0vsD3 THOR.config takes ~ 25 minutes visualization load the file TC1-I-A-D0vsD3-diffpeaks.bed and the bigwig files ( .bw extension) color bigwig for D0 in red color bigwig for D3 in green select both bigwig and right-click to Overlay tracks the BED track should display in red the regions with higher enrichments in the D0, green in the D3. meta-analysis using GREAT You can play with the BED file in R with this code to extract the fold-change from counts. It is encoded in the 11th field of the narrowPeak file as counts for the first condition (D0-ST2) and counts for the second condition (D3-A) # load the file using the tidyverse library(readr) library(dplyr) library(ggplot2) library(tidyr) diffpeaks <- read_tsv(\"TC1-I-A-D0vsD3-diffpeaks.bed\", col_names = FALSE, trim_ws = TRUE, col_types = cols(X1 = col_character())) # split the last field into three diffpeaks %>% separate(X11, into = c(\"count1\", \"count2\", \"third\"), sep = \";\", convert = TRUE) %>% mutate(FC = count2 / count1) -> thor_splitted # plot the histogram of the fold-change computed above, count second condition / count 1st condition thor_splitted %>% ggplot(aes(x = log2(FC))) + geom_histogram() + scale_x_continuous(breaks = seq(-5, 3, 1)) # create a bed file, append chr to chromosome names and write down the file thor_splitted %>% filter(log2(FC) > 0.5) %>% select(X1, X2, X3) %>% mutate(X1 = paste0(\"chr\", X1)) %>% write_tsv(\"THOR_logFC0.5.bed\", col_names = FALSE) you can now import the file THOR_logFC0.5.bed into GREAT and see again how the meta-analysis looks like.","title":"Peak calling"},{"location":"peak/#peak-calling","text":"Using MACS2 For both the day 0 and day 3 of differentiation into adipocytes, two files are available input, as control histone modification H3K4 MACS2 is going to use both files to normalize the read counts and perform the peak calling.","title":"Peak calling"},{"location":"peak/#retrieve-the-bam-files-with-all-chromosomes","text":"cd ~/chip-seq mkdir bams cd bams ln -s /scratch/users/aginolhac/chip-seq/bams/*.bam .","title":"Retrieve the BAM files with all chromosomes"},{"location":"peak/#perform-peak-calling","text":"macs2 callpeak -t TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-ST2-H3K4-D0 -B -q 0.01 --outdir TC1-ST2-H3K4-D0 & macs2 callpeak -t TC1-H3K4-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM -g mm -n TC1-A-H3K4-D3 -B -q 0.01 --outdir TC1-A-H3K4-D3 In case macs2 gives command not found , your are certainly missing the module, please see the set-up","title":"Perform peak calling"},{"location":"peak/#check-model-inferred-by-macs2","text":"execute R script. Rscript TC1-A-H3K4-D3/TC1-A-H3K4-D3_model.r Rscript TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_model.r fetch the pdf produced.","title":"check model inferred by MACS2"},{"location":"peak/#sort-per-chromosomes-and-coordinates","text":"find TC* -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\"","title":"sort per chromosomes and coordinates"},{"location":"peak/#convert-to-bigwig","text":"in order to get smaller files find TC* -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\"","title":"convert to bigwig"},{"location":"peak/#fetch-the-files-and-display-them-in-igv","text":"IGV can be downloaded from the broadinstitute.","title":"Fetch the files and display them in IGV"},{"location":"peak/#perform-peak-calling-with-broad-option","text":"macs2 callpeak -t TC1-H3K27-ST2-D0.GRCm38.p3.q30.bam \\ -c TC1-I-ST2-D0.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-ST2-H3K27-D0-broad -B -q 0.01 --outdir TC1-ST2-H3K27-D0-broad & macs2 callpeak -t TC1-H3K27-A-D3.GRCm38.p3.q30.bam \\ -c TC1-I-A-D3.GRCm38.p3.q30.bam \\ -f BAM --broad -g mm -n TC1-A-H3K27-D3-broad -B -q 0.01 --outdir TC1-A-H3K27-D3-broad Get the bigwig files for H3K27 . Redo those sort and conversion steps but only for the folders that end with 'broad' find TC*broad -name '*.bdg' | parallel \"sort -k1,1 -k2,2n {} > {.}.sort.bdg\" find TC*broad -name '*sort.bdg' | parallel -j 2 \"bedGraphToBigWig {} \\ /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes {.}.bigwig\"","title":"Perform peak calling with broad option"},{"location":"peak/#great-analysis","text":"The website GREAT allows pasting bed regions of enriched regions.","title":"GREAT analysis"},{"location":"peak/#predict-functions-of-cis-regulatory-regions","text":"Using the TC1-A-H3K4-D3_peaks.narrowPeak file produced by MACS2. This file has the different fields: chromosome start end peak name integer score for display strand fold-change -log 10 pvalue -log 10 qvalue relative summit position to peak start Let's format the file as a 3 fields BED file and focus on more significant peaks filtering on q-values . awk '$9>40' TC1-A-H3K4-D3/TC1-A-H3K4-D3_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K4-D3/TC1-A-H3K4_peaks.bed awk '$9>40' TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.narrowPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-ST2-H3K4-D0/TC1-ST2-H3K4-D0_peaks.bed For H3K27: cat TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broadPeak | cut -f 1-3 | sed 's/^/chr/' > TC1-A-H3K27-D3-broad/TC1-A-H3K27-D3-broad_peaks.broad.bed then load the BED in GREAT for the relevant genome, mm10 association rule: Single nearest gene for H3K4 Two nearest genes for H3K27","title":"predict functions of cis-regulatory regions"},{"location":"peak/#alternative-with-ngsplot","text":"Example of ngsplot where gene expression ranked the genes from top to bottom and ChIP-seq of H3K4 is mapped with the red density on top.","title":"alternative with ngsplot"},{"location":"peak/#differential-peak-calling","text":"THOR allows comparing two conditions associated with their own controls and with replicates. first, index the bams parallel \"samtools index {}\" ::: *bam second, create a config file THOR.config that contains: #rep1 TC1-H3K4-ST2-D0.GRCm38.p3.q30.bam #rep2 TC1-H3K4-A-D3.GRCm38.p3.q30.bam #chrom_sizes /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.chom.sizes #genome /scratch/users/aginolhac/chip-seq/references/GRCm38.p3.fasta #inputs1 TC1-I-ST2-D0.GRCm38.p3.q30.bam #inputs2 TC1-I-A-D3.GRCm38.p3.q30.bam A command line looks like rgt-THOR -m -n TC1-I-A-D0vsD3 --output-dir=TC1-I-A-D0vsD3 THOR.config takes ~ 25 minutes","title":"Differential peak calling"},{"location":"peak/#visualization","text":"load the file TC1-I-A-D0vsD3-diffpeaks.bed and the bigwig files ( .bw extension) color bigwig for D0 in red color bigwig for D3 in green select both bigwig and right-click to Overlay tracks the BED track should display in red the regions with higher enrichments in the D0, green in the D3.","title":"visualization"},{"location":"peak/#meta-analysis-using-great","text":"You can play with the BED file in R with this code to extract the fold-change from counts. It is encoded in the 11th field of the narrowPeak file as counts for the first condition (D0-ST2) and counts for the second condition (D3-A) # load the file using the tidyverse library(readr) library(dplyr) library(ggplot2) library(tidyr) diffpeaks <- read_tsv(\"TC1-I-A-D0vsD3-diffpeaks.bed\", col_names = FALSE, trim_ws = TRUE, col_types = cols(X1 = col_character())) # split the last field into three diffpeaks %>% separate(X11, into = c(\"count1\", \"count2\", \"third\"), sep = \";\", convert = TRUE) %>% mutate(FC = count2 / count1) -> thor_splitted # plot the histogram of the fold-change computed above, count second condition / count 1st condition thor_splitted %>% ggplot(aes(x = log2(FC))) + geom_histogram() + scale_x_continuous(breaks = seq(-5, 3, 1)) # create a bed file, append chr to chromosome names and write down the file thor_splitted %>% filter(log2(FC) > 0.5) %>% select(X1, X2, X3) %>% mutate(X1 = paste0(\"chr\", X1)) %>% write_tsv(\"THOR_logFC0.5.bed\", col_names = FALSE) you can now import the file THOR_logFC0.5.bed into GREAT and see again how the meta-analysis looks like.","title":"meta-analysis using GREAT"},{"location":"public/","text":"Public data, Mikkelsen et al. Public data can be used in 2 ways: download processed data uploaded by the authors download raw data Use the first one save time but rely on the authors's workflow Fetch fastq Go on NCBI GEO Then select RunSelector. From a Cell paper from 2010 and belong to this dataset: GEO GSE20752 Specifically we want to look at these samples: 3T3L1_t2_H3K4me3 3T3L1_t3_H3K4me3 3T3L1_t2_H3K27ac 3T3L1_t3_H3K27ac te input control they used for normalization: input control Download SRR_Acc_List.txt in /scratch/users/aginolhac/chip-seq/Mikkelsen fastq-dump is a tool from NCBI, part of the sra-tools , available for free. Install this program. then fetch and compress parallel -j 6 --progress \"fastq-dump --gzip {}\" :::: SRR_Acc_List.txt mapped with paleomix file available here paleomix bam_pipeline --bwa-max-threads=4 --max-threads=12 mikkelsen.yml last for ~ 3 hours 30 minutes peak calling H3K4 macs2 callpeak -t Mikkelsen_3T3L1_t2_H3K4me3.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\ -f BAM -g mm -n 3T3L1_t2_H3K4 -B -q 0.01 --outdir 3T3L1_t2_H3K4 & macs2 callpeak -t Mikkelsen_3T3L1_t3_H3K4me3.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\ -f BAM -g mm -n3T3L1_t3_H3K4 -B -q 0.01 --outdir 3T3L1_t3_H3K4 H3K27 macs2 callpeak -t Mikkelsen_3T3L1_t2_H3K27ac.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad \\ -f BAM -g mm -n 3T3L1_t2_H3K27ac -B -q 0.01 --outdir 3T3L1_t2_H3K27ac & macs2 callpeak -t Mikkelsen_3T3L1_t3_H3K27ac.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad \\ -f BAM -g mm -n3T3L1_t3_H3K27ac -B -q 0.01 --outdir 3T3L1_t3_H3K27ac","title":"Public"},{"location":"public/#public-data-mikkelsen-et-al","text":"Public data can be used in 2 ways: download processed data uploaded by the authors download raw data Use the first one save time but rely on the authors's workflow","title":"Public data, Mikkelsen et al."},{"location":"public/#fetch-fastq","text":"Go on NCBI GEO Then select RunSelector. From a Cell paper from 2010 and belong to this dataset: GEO GSE20752 Specifically we want to look at these samples: 3T3L1_t2_H3K4me3 3T3L1_t3_H3K4me3 3T3L1_t2_H3K27ac 3T3L1_t3_H3K27ac te input control they used for normalization: input control Download SRR_Acc_List.txt in /scratch/users/aginolhac/chip-seq/Mikkelsen fastq-dump is a tool from NCBI, part of the sra-tools , available for free. Install this program. then fetch and compress parallel -j 6 --progress \"fastq-dump --gzip {}\" :::: SRR_Acc_List.txt","title":"Fetch fastq"},{"location":"public/#mapped-with-paleomix","text":"file available here paleomix bam_pipeline --bwa-max-threads=4 --max-threads=12 mikkelsen.yml last for ~ 3 hours 30 minutes","title":"mapped with paleomix"},{"location":"public/#peak-calling","text":"","title":"peak calling"},{"location":"public/#h3k4","text":"macs2 callpeak -t Mikkelsen_3T3L1_t2_H3K4me3.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\ -f BAM -g mm -n 3T3L1_t2_H3K4 -B -q 0.01 --outdir 3T3L1_t2_H3K4 & macs2 callpeak -t Mikkelsen_3T3L1_t3_H3K4me3.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam \\ -f BAM -g mm -n3T3L1_t3_H3K4 -B -q 0.01 --outdir 3T3L1_t3_H3K4","title":"H3K4"},{"location":"public/#h3k27","text":"macs2 callpeak -t Mikkelsen_3T3L1_t2_H3K27ac.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad \\ -f BAM -g mm -n 3T3L1_t2_H3K27ac -B -q 0.01 --outdir 3T3L1_t2_H3K27ac & macs2 callpeak -t Mikkelsen_3T3L1_t3_H3K27ac.GRCm38.p3.bam \\ -c Mikkelsen_3T3L1_WCE.GRCm38.p3.bam --broad \\ -f BAM -g mm -n3T3L1_t3_H3K27ac -B -q 0.01 --outdir 3T3L1_t3_H3K27ac","title":"H3K27"},{"location":"snakemake/","text":"Snakemake The installation is from Sarah Peter , bioinformatician at the LCSB . Her tutorial is summarised here. Main differences are we don't use a virtualbox VM, nor conda environments. Install Miniconda Miniconda will provide you with a base conda environment. If you are a regular use of the HPC, you may want to remove it at the end of this tutorial. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod u+x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh Follow the instructions prompted (use spacebar to scrool down the license), of note you need to specify your installation destination, e.g. /home/users/username/miniconda3 . You must use the full path and cannot use $HOME/miniconda3 . Answer yes to initialize Miniconda3. Activate conda by reloading your BASH configuration source ~/.bashrc Notice that from now, you have an extra (base) written at the beginning of your prompt Finalize installation Update the permissions (base) chmod +x $(which conda) (base) chmod +x $(which conda-env) Update conda (base) conda update conda Install mamba as recommended by Johannes K\u00f6ster ( snakemake author) (base) conda install -c conda-forge mamba Ensure enclosed environments conda may use python modules if already installed be default. To avoid this behaviour, you need to add this line in your .bashrc . You need to edit it like with vim ~/.bashrc . If you are not comfortable with editing files, see this page export PYTHONNOUSERSITE=True Install snakemake in dedicated environment It is also recommended to leave the base environment as clean as possible, Create a new conda environment and activate it: (base) conda create -n snakemake (base) conda activate snakemake Now the prompt becomes (snakemake) and we can install snakemake inside it (snakemake) mamba install -c conda-forge -c bioconda snakemake Check that the snakemake is now installed (snakemake) snakemake --version Should return 6.7.0 (Optional) Revert the changes to your environment From Sarah Peter , if you want to stop conda from always being active: (base) conda init --reverse In case you want to get rid of conda completely, you can now also delete the directory where you installed it (default is $HOME/miniconda3 ).","title":"Setup"},{"location":"snakemake/#snakemake","text":"The installation is from Sarah Peter , bioinformatician at the LCSB . Her tutorial is summarised here. Main differences are we don't use a virtualbox VM, nor conda environments.","title":"Snakemake"},{"location":"snakemake/#install-miniconda","text":"Miniconda will provide you with a base conda environment. If you are a regular use of the HPC, you may want to remove it at the end of this tutorial. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod u+x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh Follow the instructions prompted (use spacebar to scrool down the license), of note you need to specify your installation destination, e.g. /home/users/username/miniconda3 . You must use the full path and cannot use $HOME/miniconda3 . Answer yes to initialize Miniconda3. Activate conda by reloading your BASH configuration source ~/.bashrc Notice that from now, you have an extra (base) written at the beginning of your prompt","title":"Install Miniconda"},{"location":"snakemake/#finalize-installation","text":"Update the permissions (base) chmod +x $(which conda) (base) chmod +x $(which conda-env) Update conda (base) conda update conda Install mamba as recommended by Johannes K\u00f6ster ( snakemake author) (base) conda install -c conda-forge mamba Ensure enclosed environments conda may use python modules if already installed be default. To avoid this behaviour, you need to add this line in your .bashrc . You need to edit it like with vim ~/.bashrc . If you are not comfortable with editing files, see this page export PYTHONNOUSERSITE=True","title":"Finalize installation"},{"location":"snakemake/#install-snakemake-in-dedicated-environment","text":"It is also recommended to leave the base environment as clean as possible, Create a new conda environment and activate it: (base) conda create -n snakemake (base) conda activate snakemake Now the prompt becomes (snakemake) and we can install snakemake inside it (snakemake) mamba install -c conda-forge -c bioconda snakemake Check that the snakemake is now installed (snakemake) snakemake --version Should return 6.7.0","title":"Install snakemake in dedicated environment"},{"location":"snakemake/#optional-revert-the-changes-to-your-environment","text":"From Sarah Peter , if you want to stop conda from always being active: (base) conda init --reverse In case you want to get rid of conda completely, you can now also delete the directory where you installed it (default is $HOME/miniconda3 ).","title":"(Optional) Revert the changes to your environment"}]}